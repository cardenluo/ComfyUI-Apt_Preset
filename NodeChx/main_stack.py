
#region-------------------------------import-----------------------


import folder_paths
from random import random, uniform
import torch
import numpy as np
from nodes import common_ksampler, CLIPTextEncode, ControlNetApplyAdvanced, VAEDecode, VAEEncode, InpaintModelConditioning, ControlNetLoader
from comfy.cldm.control_types import UNION_CONTROLNET_TYPES
import node_helpers
from PIL import Image, ImageFilter

from dataclasses import dataclass
#import torch.nn.functional as F
import numpy as np

import logging
from .IPAdapterPlus import ipadapter_execute, IPAdapterUnifiedLoader
from .AdvancedCN import *
from .main_nodes import Data_chx_Merge


from ..main_unit import *
from ..office_unit import *


#---------------------å®‰å…¨å¯¼å…¥------
try:
    import cv2
    REMOVER_AVAILABLE = True  # å¯¼å…¥æˆåŠŸæ—¶è®¾ç½®ä¸ºTrue
except ImportError:
    cv2 = None
    REMOVER_AVAILABLE = False  # å¯¼å…¥å¤±è´¥æ—¶è®¾ç½®ä¸ºFalse



WEIGHT_TYPES = ["linear", "ease in", "ease out", 'ease in-out', 'reverse in-out', 'weak input', 'weak output', 'weak middle', 'strong middle', 'style transfer', 'composition', 'strong style transfer', 'style and composition', 'style transfer precise', 'composition precise']


#endregion-----------------------------import----------------------------


#region---------------------æ”¶çº³----------------------------



#region--------latentæ€»æ§-----------------



class XXXStack_latent:
    ratio_sizes, ratio_dict = read_ratios()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},
            "optional": {
                "latent": ("LATENT",),
                "pixels": ("IMAGE",),
                "mask": ("MASK",),
                "noise_mask": ("BOOLEAN", {"default": True}),
                "diff_difusion": ("BOOLEAN", {"default": True}),  # æ–°å¢å‚æ•°
                "smoothness": ("INT", {"default": 0, "min": 0, "max": 500, "step": 1, "display": "slider"}),
                "ratio_selected": (['None','customer_WxH'] + cls.ratio_sizes, {"default": "None"}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 300}),
                "width": ("INT", {"default": 512, "min": 8, "max": 16384}),
                "height": ("INT", {"default": 512, "min": 8, "max": 16384}),               
            }
        }

    RETURN_TYPES = ("LATENT_STACK",)
    RETURN_NAMES = ("latent_stack",)
    FUNCTION = "stack_latent"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"

    def stack_latent(self, latent=None, pixels=None, mask=None, noise_mask=True, diff_difusion=True,  # æ–°å¢å‚æ•°
                    smoothness=1, ratio_selected="None", batch_size=1):
        # å°†diff_difusionåŠ å…¥å­˜å‚¨çš„ä¿¡æ¯ä¸­
        latent_info = (latent, pixels, mask, noise_mask, diff_difusion, smoothness, ratio_selected, batch_size)
        latent_stack = [latent_info]
        return (latent_stack,)



class XXXApply_latent:
    ratio_sizes, ratio_dict = read_ratios()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "positive": ("CONDITIONING", ),
                "negative": ("CONDITIONING", ),
                "vae": ("VAE",),
                "latent_stack": ("LATENT_STACK",),
            }
        }

    RETURN_TYPES = ("MODEL","CONDITIONING","CONDITIONING","LATENT",)
    RETURN_NAMES = ("model","positive","negative","latent",)
    FUNCTION = "apply_latent_stack"
    CATEGORY = "Apt_Preset/stack"

    def apply_latent_stack(self, model, positive, negative, vae, latent_stack):
        default_width = 512
        default_height = 512
        batch_size = 1

        for latent_info in latent_stack:
            # ä»å­˜å‚¨çš„ä¿¡æ¯ä¸­è§£åŒ…diff_difusionå‚æ•°
            latent, pixels, mask, noise_mask, diff_difusion, smoothness, ratio_selected, batch_size = latent_info

            if ratio_selected == "None" and latent is None and pixels is None:
                raise ValueError("pls input latent, or pixels, or ratio_selected.")

            if ratio_selected != "None":
                width = self.ratio_dict[ratio_selected]["width"]
                height = self.ratio_dict[ratio_selected]["height"]
                latent = {"samples": torch.zeros([batch_size, 4, height // 8, width // 8])}
                # åº”ç”¨diff_difusion
                if diff_difusion:
                    model = DifferentialDiffusion().apply(model)[0]
                return model, positive, negative, latent

            if latent is None :
                latent = {"samples": torch.zeros([batch_size, 4, default_height // 8, default_width // 8])}
                
            if pixels is not None:
                latent = VAEEncode().encode(vae, pixels)[0]

            if pixels is None and mask is None:
                # å¦‚æœ latent å·²ç»å­˜åœ¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨å®ƒ
                if latent is not None:
                    pass  # ç»§ç»­æ‰§è¡Œåç»­é€»è¾‘
                else:
                    raise TypeError("No input pixels")


            if mask is not None:
                mask = tensor2pil(mask)
                feathered_image = mask.filter(ImageFilter.GaussianBlur(smoothness))
                mask = pil2tensor(feathered_image)
                positive, negative, latent = InpaintModelConditioning().encode(positive, negative, pixels, vae, mask, noise_mask)
            
            # åº”ç”¨diff_difusion
            if diff_difusion:
                model = DifferentialDiffusion().apply(model)[0]

        latent = latentrepeat(latent, batch_size)[0]
        return model, positive, negative, latent








class Stack_latent:
    ratio_sizes, ratio_dict = read_ratios()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},
            "optional": {
                "latent": ("LATENT",),
                "pixels": ("IMAGE",),
                "mask": ("MASK",),
                "diff_difusion": ("BOOLEAN", {"default": True}),
                "smoothness": ("INT", {"default": 0, "min": 0, "max": 500, "step": 1, "display": "slider"}),
                "ratio_selected": (['None','customer_WxH'] + cls.ratio_sizes, {"default": "None"}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 300}),
                "width": ("INT", {"default": 512, "min": 8, "max": 16384}),
                "height": ("INT", {"default": 512, "min": 8, "max": 16384}),               
            }
        }

    RETURN_TYPES = ("LATENT_STACK",)
    RETURN_NAMES = ("latent_stack",)
    FUNCTION = "stack_latent"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"
    DESCRIPTION = """
    - latentã€pixelsã€ratio_selected ä¸‰è€…é€‰å…¶ä¸€
    - diff_difusion: æ˜¯å¦ä½¿ç”¨æ‰©æ•£å·®å¼‚ï¼Œé»˜è®¤å€¼ä¸ºTrueï¼Œä¸ºå¯é€‰é¡¹ã€‚
    - ratio_selectedæ¯”ä¾‹é€‰é¡¹ï¼š'None'ä¸æ“ä½œ
    - customer_WxH ç”¨å®½å’Œå®½å®šä¹‰latentå°ºå¯¸ 
    - å…¶å®ƒæ•°å­—åˆ™ç›´æ¥æŒ‰é¢„è®¾
    """





    def stack_latent(self, latent=None, pixels=None, mask=None, diff_difusion=True,
                    smoothness=1, ratio_selected="None", batch_size=1, width=512, height=512):
        noise_mask = True
        latent_info = (latent, pixels, mask, noise_mask, diff_difusion, smoothness, ratio_selected, batch_size, width, height)
        latent_stack = [latent_info]
        return (latent_stack,)



class Apply_latent:
    ratio_sizes, ratio_dict = read_ratios()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "positive": ("CONDITIONING", ),
                "negative": ("CONDITIONING", ),
                "vae": ("VAE",),
                "latent_stack": ("LATENT_STACK",),
            }
        }

    RETURN_TYPES = ("MODEL","CONDITIONING","CONDITIONING","LATENT",)
    RETURN_NAMES = ("model","positive","negative","latent",)
    FUNCTION = "apply_latent_stack"
    CATEGORY = "Apt_Preset/stack"

    def apply_latent_stack(self, model, positive, negative, vae, latent_stack):
        default_width = 512
        default_height = 512
        batch_size = 1

        for latent_info in latent_stack:
            latent, pixels, mask, noise_mask, diff_difusion, smoothness, ratio_selected, batch_size, width, height = latent_info

            if ratio_selected == "None" and latent is None and pixels is None:
                raise ValueError("pls input latent, or pixels, or ratio_selected.")

            if ratio_selected != "None":
                if ratio_selected == "customer_WxH":
                    width_val = width
                    height_val = height
                else:
                    width_val = self.ratio_dict[ratio_selected]["width"]
                    height_val = self.ratio_dict[ratio_selected]["height"]
                
                latent = {"samples": torch.zeros([batch_size, 4, height_val // 8, width_val // 8])}
                if diff_difusion:
                    model = DifferentialDiffusion().apply(model)[0]
                return model, positive, negative, latent

            if latent is None :
                latent = {"samples": torch.zeros([batch_size, 4, default_height // 8, default_width // 8])}
                
            if pixels is not None:
                latent = VAEEncode().encode(vae, pixels)[0]

            if pixels is None and mask is None:
                if latent is not None:
                    pass
                else:
                    raise TypeError("No input pixels")


            if mask is not None:
                mask = tensor2pil(mask)
                feathered_image = mask.filter(ImageFilter.GaussianBlur(smoothness))
                mask = pil2tensor(feathered_image)
                positive, negative, latent = InpaintModelConditioning().encode(positive, negative, pixels, vae, mask, noise_mask)
            
            if diff_difusion:
                model = DifferentialDiffusion().apply(model)[0]

        latent = latentrepeat(latent, batch_size)[0]
        return model, positive, negative, latent





#endregion--------latentæ€»æ§-----------------


#region--------moldä¸²è”-----------------


class Stack_LoRA:

    @classmethod
    def INPUT_TYPES(cls):
        loras = ["None"] + folder_paths.get_filename_list("loras")
        
        return {
            "required": {
                "lora_name_1": (loras,),
                "weight_1": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01}),
                "lora_name_2": (loras,),
                "weight_2": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01}),
                "lora_name_3": (loras,),
                "weight_3": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01}),
            },
            "optional": {
                "lora_stack": ("LORASTACK",)
            },
        }

    RETURN_TYPES = ("LORASTACK",)
    RETURN_NAMES = ("lora_stack",)
    FUNCTION = "lora_stacker"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"


    def lora_stacker(self, lora_name_1, weight_1, lora_name_2, weight_2, lora_name_3, weight_3, lora_stack=None):
        """
        å°†å¤šä¸ª LoRA é…ç½®æ·»åŠ åˆ°å †ä¸­ã€‚
        """
        lora_list = []

        # å¦‚æœä¼ å…¥äº†å·²æœ‰çš„ lora_stackï¼Œå°†å…¶å†…å®¹åˆå¹¶åˆ° lora_list ä¸­
        if lora_stack is not None:
            lora_list.extend([lora for lora in lora_stack if lora[0] != "None"])

        # å¦‚æœ LoRA é…ç½®æœ‰æ•ˆï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        if lora_name_1 != "None":
            lora_list.append((lora_name_1, weight_1))
        if lora_name_2 != "None":
            lora_list.append((lora_name_2, weight_2))
        if lora_name_3 != "None":
            lora_list.append((lora_name_3, weight_3))

        return (lora_list,)


class Apply_LoRAStack:

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "clip": ("CLIP",),
                "lora_stack": ("LORASTACK",),
            }
        }

    RETURN_TYPES = ("MODEL", "CLIP",)
    RETURN_NAMES = ("MODEL", "CLIP",)
    FUNCTION = "apply_lora_stack"
    CATEGORY = "Apt_Preset/stack"

    def apply_lora_stack(self, model, clip, lora_stack=None):
        if not lora_stack:
            return (model, clip,)

        model_lora = model
        clip_lora = clip

        for tup in lora_stack:
            lora_name, weight = tup  
            lora_path = folder_paths.get_full_path("loras", lora_name)
            lora = comfy.utils.load_torch_file(lora_path, safe_load=True)
            model_lora, clip_lora = comfy.sd.load_lora_for_models(model_lora, clip_lora, lora, weight, weight )

        return (model_lora, clip_lora,)




class Stack_IPA:

    def __init__(self):
        self.unfold_batch = False

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {

                "preset": ([ 'STANDARD (medium strength)','LIGHT - SD1.5 only (low strength)', 'VIT-G (medium strength)', 'PLUS (high strength)', 'PLUS FACE (portraits)', 'FULL FACE - SD1.5 only (portraits stronger)'], ),
                "weight": ("FLOAT", { "default": 1.0, "min": -1, "max": 5, "step": 0.05 }),
                "weight_type": (WEIGHT_TYPES, ),
                "combine_embeds": (["concat", "add", "subtract", "average", "norm average"],),
                "start_at": ("FLOAT", { "default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001 }),
                "end_at": ("FLOAT", { "default": 1.0, "min": 0.0, "max": 1.0, "step": 0.001 }),
                "embeds_scaling": (['V only', 'K+V', 'K+V w/ C penalty', 'K+mean(V) w/ C penalty'], ),
            },
            "optional": {
                "image": ("IMAGE",),
                "attn_mask": ("MASK",),
                "ipa_stack": ("IPA_STACK",),
                #"image_negative": ("IMAGE",),
            }
        }

    RETURN_TYPES = ("IPA_STACK",)
    RETURN_NAMES = ("ipa_stack",)
    FUNCTION = "ipa_stack"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"

    def ipa_stack(self,   preset, weight, weight_type, combine_embeds, start_at, end_at, embeds_scaling,image=None, attn_mask=None, image_negative=None, ipa_stack=None):
        
        if image is None:
            return (None,)
        
        # åˆå§‹åŒ–ipa_list
        ipa_list = []

        # å¦‚æœä¼ å…¥äº†ipa_stackï¼Œå°†å…¶ä¸­çš„å†…å®¹æ·»åŠ åˆ°ipa_listä¸­
        if ipa_stack is not None:
            ipa_list.extend([ipa for ipa in ipa_stack if ipa[0] != "None"])

        # å°†å½“å‰IPAçš„ç›¸å…³ä¿¡æ¯æ‰“åŒ…æˆä¸€ä¸ªå…ƒç»„ï¼Œå¹¶æ·»åŠ åˆ°ipa_listä¸­
        ipa_info = (
            image,
            preset,
            weight,
            weight_type,
            combine_embeds,
            start_at,
            end_at,
            embeds_scaling,
            attn_mask,
            image_negative,
        )
        ipa_list.append(ipa_info)

        return (ipa_list,)


class Apply_IPA:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "ipa_stack": ("IPA_STACK",),
            }
        }

    RETURN_TYPES = ("MODEL", )
    RETURN_NAMES = ("model", )
    FUNCTION = "apply_ipa_stack"
    CATEGORY = "Apt_Preset/stack"

    def apply_ipa_stack(self, model, ipa_stack):

        if not ipa_stack:
            raise ValueError("ipa_stack ä¸èƒ½ä¸ºç©º")

        # åˆå§‹åŒ–å˜é‡
        image0 = None
        mask0 = None
        work_model = model.clone()

        # éå† ipa_stack ä¸­çš„æ¯ä¸ª IPA é…ç½®
        for ipa_info in ipa_stack:
            (
                image,
                preset,
                weight,
                weight_type,
                combine_embeds,
                start_at,
                end_at,
                embeds_scaling,
                attn_mask,
                image_negative,
            ) = ipa_info

            # è®°å½•ç¬¬ä¸€ä¸ª image å’Œ mask
            if image0 is None:
                image0 = image
            if mask0 is None:
                mask0 = attn_mask

            # åŠ è½½ IPAdapter æ¨¡å‹
            model, ipadapter = IPAdapterUnifiedLoader().load_models(
                work_model, preset, lora_strength=0.0, provider="CPU", ipadapter=None
            )

            if 'ipadapter' in ipadapter:
                ipadapter_model = ipadapter['ipadapter']['model']
                clip_vision = ipadapter['clipvision']['model']
            else:
                ipadapter_model = ipadapter


            ipa_args = {
                "image": image,
                "image_negative": image_negative,
                "weight": weight,
                "weight_type": weight_type,
                "combine_embeds": combine_embeds,
                "start_at": start_at,
                "end_at": end_at,
                "attn_mask": attn_mask,
                "embeds_scaling": embeds_scaling,
                "insightface": None,  # å¦‚æœéœ€è¦ insightfaceï¼Œå¯ä»¥ä» ipa_stack ä¸­ä¼ é€’
                "layer_weights": None,  # å¦‚æœéœ€è¦ layer_weightsï¼Œå¯ä»¥ä» ipa_stack ä¸­ä¼ é€’
                "encode_batch_size": 0,  # é»˜è®¤å€¼
                "style_boost": None,  # å¦‚æœéœ€è¦ style_boostï¼Œå¯ä»¥ä» ipa_stack ä¸­ä¼ é€’
                "composition_boost": None,  # å¦‚æœéœ€è¦ composition_boostï¼Œå¯ä»¥ä» ipa_stack ä¸­ä¼ é€’
                "enhance_tiles": 1,  # é»˜è®¤å€¼
                "enhance_ratio": 1.0,  # é»˜è®¤å€¼
                "weight_kolors": 1.0,  # é»˜è®¤å€¼
            }

            # åº”ç”¨ IPA é…ç½®
            model, _ = ipadapter_execute(work_model, ipadapter_model, clip_vision, **ipa_args)

        return (model,)       #modelåœ¨ä¸‹é¢è¿è¡Œæ­£ç¡®ï¼Œä½†æ˜¯è¿™é‡Œä¼šæŠ¥é”™ï¼Œè¦ç»Ÿä¸€å…ƒç¥–æˆ–ç»Ÿä¸€æ¨¡å‹å¯¹è±¡


class AD_sch_IPA:

    def __init__(self):
        self.unfold_batch = False

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "preset": ([ 'STANDARD (medium strength)','LIGHT - SD1.5 only (low strength)', 'VIT-G (medium strength)', 'PLUS (high strength)', 'PLUS FACE (portraits)', 'FULL FACE - SD1.5 only (portraits stronger)'], ),
                "weight": ("FLOAT", { "default": 1.0, "min": -1, "max": 5, "step": 0.05 }),
                #"weight_type": (["none", "style", "content"], ),
                #"combine_embeds": (["concat", "add", "subtract", "average", "norm average"],),
                #"start_at": ("FLOAT", { "default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001 }),
                #"end_at": ("FLOAT", { "default": 1.0, "min": 0.0, "max": 1.0, "step": 0.001 }),
                "embeds_scaling": (['V only', 'K+V', 'K+V w/ C penalty', 'K+mean(V) w/ C penalty'], ),
                "points_string": ("STRING", {"default": "0:(0.0),\n7:(1.0),\n15:(0.0)\n", "multiline": True}),
                "invert": ("BOOLEAN", {"default": False}),
                "frames": ("INT", {"default": 16,"min": 2, "max": 255, "step": 1}),
                #"width": ("INT", {"default": 512,"min": 1, "max": 4096, "step": 1}),
                #"height": ("INT", {"default": 512,"min": 1, "max": 4096, "step": 1}),
                "interpolation": (["linear", "ease_in", "ease_out", "ease_in_out"],),
            },
            "optional": {
                "image": ("IMAGE",),
                "ipa_stack": ("IPA_STACK",),

            }
        }

    RETURN_TYPES = ("IPA_STACK",)
    RETURN_NAMES = ("ipa_stack",)
    FUNCTION = "ipa_stack"
    CATEGORY = "Apt_Preset/AD/ğŸ˜ºbackup"

    def createfademask(self, frames, width, height, invert, points_string, interpolation):
        
        def ease_in(t):
            return t * t
        
        def ease_out(t):
            return 1 - (1 - t) * (1 - t)

        def ease_in_out(t):
            return 3 * t * t - 2 * t * t * t
        
        # Parse the input string into a list of tuples
        points = []
        points_string = points_string.rstrip(',\n')
        for point_str in points_string.split(','):
            frame_str, color_str = point_str.split(':')
            frame = int(frame_str.strip())
            color = float(color_str.strip()[1:-1])  # Remove parentheses around color
            points.append((frame, color))

        # Check if the last frame is already in the points
        if len(points) == 0 or points[-1][0] != frames - 1:
            # If not, add it with the color of the last specified frame
            points.append((frames - 1, points[-1][1] if points else 0))

        # Sort the points by frame number
        points.sort(key=lambda x: x[0])

        batch_size = frames
        out = []
        image_batch = np.zeros((batch_size, height, width), dtype=np.float32)

        # Index of the next point to interpolate towards
        next_point = 1

        for i in range(batch_size):
            while next_point < len(points) and i > points[next_point][0]:
                next_point += 1

            # Interpolate between the previous point and the next point
            prev_point = next_point - 1
            t = (i - points[prev_point][0]) / (points[next_point][0] - points[prev_point][0])
            if interpolation == "ease_in":
                t = ease_in(t)
            elif interpolation == "ease_out":
                t = ease_out(t)
            elif interpolation == "ease_in_out":
                t = ease_in_out(t)
            elif interpolation == "linear":
                pass  # No need to modify `t` for linear interpolation

            color = points[prev_point][1] - t * (points[prev_point][1] - points[next_point][1])
            color = np.clip(color, 0, 255)
            image = np.full((height, width), color, dtype=np.float32)
            image_batch[i] = image

        output = torch.from_numpy(image_batch)
        mask = output
        out.append(mask)

        if invert:
            return 1.0 - torch.cat(out, dim=0)
        return torch.cat(out, dim=0)

    def ipa_stack(self, image, preset, weight, embeds_scaling, points_string, invert, frames, interpolation, ipa_stack=None):
        
        start_at=0
        end_at=1
        weight_type = "style"
        combine_embeds = "add"
        
        
        attn_mask = self.createfademask(frames, 512, 512, invert, points_string, interpolation)
        
        image_negative = None
        ipa_list = []
        if ipa_stack is not None:
            ipa_list.extend([ipa for ipa in ipa_stack if ipa[0] != "None"])

        ipa_info = (
            image,
            preset,
            weight,
            weight_type,
            combine_embeds,
            start_at,
            end_at,
            embeds_scaling,
            attn_mask,
            image_negative,
        )
        ipa_list.append(ipa_info)

        return (ipa_list,)


#endregion--------moldä¸²è”-----------------





#region--------æ¡ä»¶----åˆ†åŒºä¸²è”-----------------






class Stack_condition_mode:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "conditioning_1": ("CONDITIONING", ),
                "conditioning_2": ("CONDITIONING", ),
                "mode": (["combine", "average", "concat"], ),
            },
            "optional": {
                "conditioning_3": ("CONDITIONING",),
                "conditioning_4": ("CONDITIONING",),
                "strength1": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01}),
                "strength2": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01}),
                "strength3": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01}),
                "strength4": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01})  
            }
        }
    
    RETURN_TYPES = ("CONDITION_MODE_STACK",)
    RETURN_NAMES = ("condi_stack",)
    FUNCTION = "pack"
    CATEGORY = "Apt_Preset/chx_tool"


    def pack(self, conditioning_1, conditioning_2, mode, 
             conditioning_3=None, conditioning_4=None,
             strength1=1.0, strength2=1.0, strength3=1.0, strength4=1.0):

        condition_mode_pack = conditioning_1, conditioning_2, mode,conditioning_3, conditioning_4,strength1, strength2, strength3, strength4
        
        return condition_mode_pack



class pre_mul_Mulcondi:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "optional": {
                "context": ("RUN_CONTEXT",),
                "pos1": ("STRING", {"multiline": True, "dynamicPrompts": True,"default": "" }),
                "pos2": ("STRING", {"multiline": True, "dynamicPrompts": True,"default": "" }),
                "pos3": ("STRING", {"multiline": True, "dynamicPrompts": True,"default": "" }),
                "pos4": ("STRING", {"multiline": True, "dynamicPrompts": True,"default": "" }),
                "pos5": ("STRING", {"multiline": True, "dynamicPrompts": True,"default": "" }),
                "mask_1": ("MASK", ),
                "mask_2": ("MASK", ),
                "mask_3": ("MASK", ),
                "mask_4": ("MASK", ),
                "mask_5": ("MASK", ),
                "mask_1_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_2_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_3_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_4_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_5_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "background": ("STRING", {"multiline": False, "dynamicPrompts": True, "default": "background is sea"}),
                "neg": ("STRING", {"multiline": False, "dynamicPrompts": True,"default": "Poor quality" }),
            }
        }
        
    RETURN_TYPES = ("RUN_CONTEXT","CONDITIONING", "CONDITIONING", )
    RETURN_NAMES = ("context","positive", "negative",)

    FUNCTION = "Mutil_Clip"
    CATEGORY = "Apt_Preset/chx_tool/conditioning"

    def Mutil_Clip (self, pos1, pos2, pos3, pos4, pos5, background, neg,  mask_1_strength, mask_2_strength, mask_3_strength, mask_4_strength, mask_5_strength, mask_1=None, mask_2=None, mask_3=None, mask_4=None, mask_5=None, context=None):
        set_cond_area = "default"
        clip = context.get("clip")
        positive_1, = CLIPTextEncode().encode(clip, pos1)
        positive_2, = CLIPTextEncode().encode(clip, pos2)
        positive_3, = CLIPTextEncode().encode(clip, pos3)
        positive_4, = CLIPTextEncode().encode(clip, pos4)
        positive_5, = CLIPTextEncode().encode(clip, pos5)
        negative, = CLIPTextEncode().encode(clip, neg)

        c = []
        set_area_to_bounds = False
        if set_cond_area != "default":
            set_area_to_bounds = True
        valid_masks = []
        
        # å¤„ç†é®ç½©ç»´åº¦
        if mask_1 is not None and len(mask_1.shape) < 3:  
            mask_1 = mask_1.unsqueeze(0)
        if mask_2 is not None and len(mask_2.shape) < 3:  
            mask_2 = mask_2.unsqueeze(0)
        if mask_3 is not None and len(mask_3.shape) < 3:  
            mask_3 = mask_3.unsqueeze(0)
        if mask_4 is not None and len(mask_4.shape) < 3:  
            mask_4 = mask_4.unsqueeze(0)
        if mask_5 is not None and len(mask_5.shape) < 3:  
            mask_5 = mask_5.unsqueeze(0)

        # åº”ç”¨å„ä¸ªé®ç½©å¹¶æ”¶é›†æœ‰æ•ˆçš„é®ç½©
        if mask_1 is not None:
            for t in positive_1:
                append_helper(t, mask_1, c, set_area_to_bounds, mask_1_strength)
            valid_masks.append(mask_1)
        if mask_2 is not None:
            for t in positive_2:
                append_helper(t, mask_2, c, set_area_to_bounds, mask_2_strength)
            valid_masks.append(mask_2)
        if mask_3 is not None:
            for t in positive_3:
                append_helper(t, mask_3, c, set_area_to_bounds, mask_3_strength)
            valid_masks.append(mask_3)
        if mask_4 is not None:
            for t in positive_4:
                append_helper(t, mask_4, c, set_area_to_bounds, mask_4_strength)
            valid_masks.append(mask_4)
        if mask_5 is not None:
            for t in positive_5:
                append_helper(t, mask_5, c, set_area_to_bounds, mask_5_strength)
            valid_masks.append(mask_5)

        # è®¡ç®—èƒŒæ™¯é®ç½©
        if valid_masks:
            total_mask = sum(valid_masks)
            # ç¡®ä¿æ€»é®ç½©ä¸è¶…è¿‡1
            total_mask = torch.clamp(total_mask, 0, 1)
            mask_6 = 1 - total_mask
        else:
            mask_6 = torch.ones_like(mask_1) if mask_1 is not None else None

        # åº”ç”¨èƒŒæ™¯æ¡ä»¶
        if mask_6 is not None:
            background_cond, = CLIPTextEncode().encode(clip, background)
            for t in background_cond:
                append_helper(t, mask_6, c, set_area_to_bounds, 1)

        context = new_context(context, positive=c, negative=negative, clip=clip)

        return (context, c, negative)


class Stack_condi:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {},
            "optional": {
                "pos1": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}),
                "pos2": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}),
                "pos3": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}),
                "pos4": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}),
                "pos5": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": ""}),  # æ–°å¢pos5
                "mask_1": ("MASK", ),
                "mask_2": ("MASK", ),
                "mask_3": ("MASK", ),
                "mask_4": ("MASK", ),
                "mask_5": ("MASK", ),  # æ–°å¢mask_5è¾“å…¥
                "mask_1_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_2_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_3_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_4_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "mask_5_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),  # æ–°å¢mask_5å¼ºåº¦
                "background": ("STRING", {"multiline": True, "dynamicPrompts": True, "default": "background is sea"}),
                "background_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),  # æ–°å¢èƒŒæ™¯å¼ºåº¦
                "neg": ("STRING", {"multiline": False, "dynamicPrompts": True, "default": "Poor quality"}),
            }
        }
    
    RETURN_TYPES = ("STACK_CONDI",)
    RETURN_NAMES = ("condi_stack", )
    FUNCTION = "stack_condi"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"

    def stack_condi(self, pos1, pos2, pos3, pos4, pos5, background, background_strength, neg, 
                    mask_1_strength, mask_2_strength, mask_3_strength, mask_4_strength, mask_5_strength,
                    mask_1=None, mask_2=None, mask_3=None, mask_4=None, mask_5=None):
        condi_stack = list()
        set_cond_area ="default"
        
        # æ‰“åŒ…é€»è¾‘ï¼šæ¯ç»„ posã€mask å’Œ mask_strength æ˜¯é…å¥—çš„
        def pack_group(pos, mask, mask_strength):
            if mask is None or mask_strength <= 0:  # æ–°å¢mask_strengthæ£€æŸ¥
                return None
            return {
                "pos": pos,
                "mask": mask,
                "mask_strength": mask_strength,
            }
        
        valid_masks = []
        if mask_1 is not None:
            valid_masks.append(mask_1)
        if mask_2 is not None:
            valid_masks.append(mask_2)
        if mask_3 is not None:
            valid_masks.append(mask_3)
        if mask_4 is not None:
            valid_masks.append(mask_4)
        if mask_5 is not None:
            valid_masks.append(mask_5)

        # è®¡ç®—èƒŒæ™¯é®ç½©ï¼Œç¡®ä¿èŒƒå›´åœ¨0-1ä¹‹é—´
        if valid_masks:
            total_mask = sum(valid_masks)
            # ç¡®ä¿æ€»é®ç½©ä¸è¶…è¿‡1
            total_mask = torch.clamp(total_mask, 0, 1)
            mask_bg = 1 - total_mask
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé®ç½©ï¼ŒèƒŒæ™¯é®ç½©åº”è¯¥æ˜¯å…¨1
            mask_bg = None  # æ³¨æ„ï¼šè¿™é‡Œæ”¹ä¸ºNoneï¼Œåœ¨Apply_condiStackä¸­å¤„ç†å…¨1çš„æƒ…å†µ
        
        # æ‰“åŒ…æ¯ç»„ä¿¡æ¯
        group1 = pack_group(pos1, mask_1, mask_1_strength)
        group2 = pack_group(pos2, mask_2, mask_2_strength)
        group3 = pack_group(pos3, mask_3, mask_3_strength)
        group4 = pack_group(pos4, mask_4, mask_4_strength)
        group5 = pack_group(pos5, mask_5, mask_5_strength)
        group_bg = pack_group(background, mask_bg, background_strength)  # ä½¿ç”¨èƒŒæ™¯å¼ºåº¦å‚æ•°
        
        # å°†æ‰“åŒ…çš„ç»„æ·»åŠ åˆ° condi_stack
        if group1 is not None:
            condi_stack.append(group1)
        if group2 is not None:
            condi_stack.append(group2)
        if group3 is not None:
            condi_stack.append(group3)
        if group4 is not None:
            condi_stack.append(group4)
        if group5 is not None:
            condi_stack.append(group5)
        if group_bg is not None:  # æ·»åŠ èƒŒæ™¯ç»„
            condi_stack.append(group_bg)
        
        # æ‰“åŒ…è´Ÿé¢æç¤ºå’Œ set_cond_area
        condi_stack.append({
            "neg": neg,
            "set_cond_area": set_cond_area,
        })
        
        return (condi_stack,)


class Apply_condiStack:

    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {
            "clip": ("CLIP",),
            "stack_condi": ("STACK_CONDI",),
        }}

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING")
    RETURN_NAMES = ("positive", "negative")
    FUNCTION = "condiStack"
    CATEGORY = "Apt_Preset/stack"

    def condiStack(self, clip, stack_condi):
        neg_data = stack_condi[-1]
        neg = neg_data["neg"]
        set_cond_area = neg_data["set_cond_area"]

        negative, = CLIPTextEncode().encode(clip, neg)
        positive = []
        set_area_to_bounds = (set_cond_area != "default")

        for group in stack_condi[:-1]:
            pos = group["pos"]
            mask = group["mask"]
            mask_strength = group["mask_strength"]

            encoded_pos, = CLIPTextEncode().encode(clip, pos)

            if mask is None:
                # å¤„ç†èƒŒæ™¯é®ç½©ä¸ºNoneçš„æƒ…å†µï¼ˆå³å…¨1é®ç½©ï¼‰
                for t in encoded_pos:
                    # åˆ›å»ºä¸€ä¸ªå…¨1çš„é®ç½©
                    full_mask = torch.ones_like(t[0][0]) if t is not None and len(t) > 0 and len(t[0]) > 0 else None
                    if full_mask is not None:
                        append_helper(t, full_mask, positive, set_area_to_bounds, mask_strength)
            else:
                if len(mask.shape) < 3:
                    mask = mask.unsqueeze(0)
                for t in encoded_pos:
                    append_helper(t, mask, positive, set_area_to_bounds, mask_strength)
                    
        return (positive, negative)

#endregion--------æ¡ä»¶ä¸²è”-----------------


#region--------æ¡ä»¶---CNä¸²è”-----------------




class Stack_ControlNet:

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {

            },
            "optional": {
                "image": ("IMAGE",),
                "controlnet": (["None"] + folder_paths.get_filename_list("controlnet"),),
                "strength": ("FLOAT", {"default": 0.8, "min": -10.0, "max": 10.0, "step": 0.01}),
                "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "cn_stack": ("CN_STACK",),

            }
        }

    RETURN_TYPES = ("CN_STACK",)
    RETURN_NAMES = ("cn_stack",)
    FUNCTION = "controlnet_stacker"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"

    def controlnet_stacker(self, controlnet, strength, image=None, start_percent=0.0, end_percent=1.0, cn_stack=None):

        controlnet_list = []
        if cn_stack is not None:
            controlnet_list.extend([cn for cn in cn_stack if cn[0] != "None"])

        if controlnet != "None" and image is not None:
            controlnet_path = folder_paths.get_full_path("controlnet", controlnet)
            controlnet = comfy.controlnet.load_controlnet(controlnet_path)
            # å°†start_percentå’Œend_percentæ·»åŠ åˆ°å…ƒç»„ä¸­
            controlnet_list.append((controlnet, image, strength, start_percent, end_percent))

        return (controlnet_list,)


class Apply_ControlNetStack:

    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
                            "switch": (["Off","On"],),
                            "positive": ("CONDITIONING", ),
                            "negative": ("CONDITIONING",),
                            "controlnet_stack": ("CN_STACK", ),
                            },
            "optional": {
                "vae": ("VAE",),
            }
        }                    

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", )
    RETURN_NAMES = ("positive", "negative", )
    FUNCTION = "apply_controlnet_stack"
    CATEGORY = "Apt_Preset/stack"

    def apply_controlnet_stack(self, positive, negative, switch, vae=None, controlnet_stack=None):

        if switch == "Off":
            return (positive, negative, )
    
        if controlnet_stack is not None:
            for controlnet_tuple in controlnet_stack:
                # ä»å…ƒç»„ä¸­è·å–start_percentå’Œend_percent
                controlnet, image, strength, start_percent, end_percent = controlnet_tuple
                
                # ä½¿ç”¨è·å–åˆ°çš„start_percentå’Œend_percentå‚æ•°
                conditioning = ControlNetApplyAdvanced().apply_controlnet(
                    positive, negative, controlnet, image, strength, 
                    start_percent, end_percent, vae, extra_concat=[]
                )
                positive, negative = conditioning[0], conditioning[1]

        return (positive, negative, )





#endregion--------CNä¸²è”-----------------


#region--------æ¡ä»¶--æ–‡æœ¬ç”Ÿæˆ-----------------

class Stack_text:

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "optional": {
                "text_stack": ("TEXT_STACK",),
                "pos": ("STRING", {"default": "", "multiline": True}),
                "neg": ("STRING", {"default": "", "multiline": False}),
                "style": (["None"] + style_list()[0], {"default": "None"}),
            }
        }

    RETURN_TYPES = ("TEXT_STACK", )
    RETURN_NAMES = ("text_stack", )
    FUNCTION = "lora_stacker"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"
    def lora_stacker(self, style,  pos, neg, text_stack=None,):  # æ·»åŠ  text_stack å‚æ•°
        stack = list()
        if text_stack:
            stack.extend(text_stack)
            stack.append(',')  # æ·»åŠ é€—å·éš”å¼€
        pos, neg = add_style_to_subject(style, pos, neg) 
        
        stack.extend([(pos, neg)])
        return (stack,)


class Apply_textStack:

    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {
                "clip": ("CLIP",),
                "text_stack": ("TEXT_STACK",),
                },
        }

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING")
    RETURN_NAMES = ("positive", "negative")
    FUNCTION = "textStack"
    CATEGORY = "Apt_Preset/stack"
    def textStack(self, clip, text_stack):
        positive_list = []
        negative_list = []
        for item in text_stack:
            if isinstance(item, (tuple, list)) and len(item) == 2:
                pos, neg = item
                if pos is not None and pos != '':
                    (positive,) = CLIPTextEncode().encode(clip, pos)
                    (negative,) = CLIPTextEncode().encode(clip, neg)
                    positive_list.append(positive)
                    negative_list.append(negative)

        if positive_list and negative_list:
            positive = sum(positive_list, [])
            negative = sum(negative_list, [])
        else:
            positive = []
            negative = []

        return (positive, negative)



class AD_sch_prompt_stack:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompts": ("STRING", {"multiline": True, "default": DefaultPromp}),
                "easing_type": (list(easing_functions.keys()), {"default": "Linear"}),
            },
            "optional": {
                "max_length": ("INT", {"default": 120, "min": 0, "max": 100000}),
                "f_text": ("STRING", {"default": "", "multiline": False}),
                "b_text": ("STRING", {"default": "", "multiline": False}),
            }
        }

    RETURN_TYPES = ("PROMPT_SCHEDULE_STACK",)
    RETURN_NAMES = ("prompt_stack",)
    FUNCTION = "create_schedule"
    CATEGORY = "Apt_Preset/AD/ğŸ˜ºbackup"
    DESCRIPTION = """
    - æ’å…¥ç¼“åŠ¨å‡½æ•°ä¸¾ä¾‹Examples functionsï¼š
    - 0:0.5 @Sine_In@
    - 30:1 @Linear@
    - 60:0.5
    - 90:1
    - æ”¯æŒçš„ç¼“åŠ¨å‡½æ•°Supported easing functions:
    - Linear,
    - Sine_In,Sine_Out,Sine_InOut,Sin_Squared,
    - Quart_In,Quart_Out,Quart_InOut,
    - Cubic_In,Cubic_Out,Cubic_InOut,
    - Circ_In,Circ_Out,Circ_InOut,
    - Back_In,Back_Out,Back_InOut,
    - Elastic_In,Elastic_Out,Elastic_InOut,
    - Bounce_In,Bounce_Out,Bounce_InOut"
    """






    def create_schedule(self, prompts: str, max_length=0, easing_type="Linear", f_text="", b_text="",):

        PROMPT_STACK = (prompts, easing_type, max_length, f_text, b_text)
        return ( PROMPT_STACK,)


class AD_sch_prompt_apply:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip": ("CLIP",),
                "prompt_stack": ("PROMPT_SCHEDULE_STACK",),
            },
            "optional": {
            }
        }

    RETURN_TYPES = ("CONDITIONING","IMAGE")
    RETURN_NAMES = ("positive","graph")
    FUNCTION = "create_schedule"
    CATEGORY = "Apt_Preset/AD/ğŸ˜ºbackup"

    def create_schedule(self,clip, prompt_stack=None):
        (prompts, easing_type, max_length, f_text, b_text)= prompt_stack 
        frames = parse_prompt_schedule(prompts.strip(), easing_type)

        curve_img = generate_frame_weight_curve_image(frames, max_length)
        positive = build_conditioning(frames, clip, max_length, f_text=f_text, b_text=b_text)

        return ( positive, curve_img)


#endregion--------æ¡ä»¶ç”Ÿæˆ-é‡ç½®-----------------


#region--------æ¡ä»¶----Redux stack----------------------


class YC_LG_Redux:   #ä½œä¸ºå‡½æ•°è°ƒç”¨
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            
            "positive": ("CONDITIONING",),
            "style_model": (folder_paths.get_filename_list("style_models"), {"default": "flux1-redux-dev.safetensors"}),
            "clip_vision": (folder_paths.get_filename_list("clip_vision"), {"default": "sigclip_vision_patch14_384.safetensors"}),
            "image": ("IMAGE",),
            "crop": (["center", "mask_area", "none"], {
                "default": "none",
                "tooltip": "è£å‰ªæ¨¡å¼ï¼šcenter-ä¸­å¿ƒè£å‰ª, mask_area-é®ç½©åŒºåŸŸè£å‰ª, none-ä¸è£å‰ª"
            }),
            "sharpen": ("FLOAT", {
                "default": 0.0,
                "min": -5.0,
                "max": 5.0,
                "step": 0.1,
                "tooltip": "é”åŒ–å¼ºåº¦ï¼šè´Ÿå€¼ä¸ºæ¨¡ç³Šï¼Œæ­£å€¼ä¸ºé”åŒ–ï¼Œ0ä¸ºä¸å¤„ç†"
            }),
            "patch_res": ("INT", {
                "default": 16,
                "min": 1,
                "max": 64,
                "step": 1,
                "tooltip": "patchåˆ†è¾¨ç‡ï¼Œæ•°å€¼è¶Šå¤§åˆ†å—è¶Šç»†è‡´"
            }),
            "style_strength": ("FLOAT", {
                "default": 1.0,
                "min": 0.0,
                "max": 2.0,
                "step": 0.01,
                "tooltip": "é£æ ¼å¼ºåº¦ï¼Œè¶Šé«˜è¶Šåå‘å‚è€ƒå›¾ç‰‡"
            }),
            "prompt_strength": ("FLOAT", { 
                "default": 1.0,
                "min": 0.0,
                "max": 2.0,
                "step": 0.01,
                "tooltip": "æ–‡æœ¬æç¤ºè¯å¼ºåº¦ï¼Œè¶Šé«˜æ–‡æœ¬ç‰¹å¾è¶Šå¼º"
            }),
            "blend_mode": (["lerp", "feature_boost", "frequency"], {
                "default": "lerp",
                "tooltip": "é£æ ¼å¼ºåº¦çš„è®¡ç®—æ–¹å¼ï¼š\n" +
                        "lerp - çº¿æ€§æ··åˆ - é«˜åº¦å‚è€ƒåŸå›¾\n" +
                        "feature_boost - ç‰¹å¾å¢å¼º - å¢å¼ºçœŸå®æ„Ÿ\n" +
                        "frequency - é¢‘ç‡å¢å¼º - å¢å¼ºé«˜é¢‘ç»†èŠ‚"
            }),
            "noise_level": ("FLOAT", { 
                "default": 0.0,
                "min": 0.0,
                "max": 1.0,
                "step": 0.01,
                "tooltip": "æ·»åŠ éšæœºå™ªå£°çš„å¼ºåº¦ï¼Œå¯ç”¨äºä¿®å¤é”™è¯¯ç»†èŠ‚"
            }),
        },
        "optional": { 
            "mask": ("MASK", ), 
            "guidance": ("FLOAT", {"default": 30, "min": 0.0, "max": 100.0, "step": 0.1}),
        }}
        
    RETURN_TYPES = ("CONDITIONING",)
    RETURN_NAMES = ("positive",)
    
    FUNCTION = "apply_stylemodel"
    CATEGORY = "Apt_Preset/chx_tool"

    def crop_to_mask_area(self, image, mask):
        if len(image.shape) == 4:
            B, H, W, C = image.shape
            image = image.squeeze(0)
        else:
            H, W, C = image.shape
        
        if len(mask.shape) == 3:
            mask = mask.squeeze(0)
        
        nonzero_coords = torch.nonzero(mask)
        if len(nonzero_coords) == 0:
            return image, mask
        
        top = nonzero_coords[:, 0].min().item()
        bottom = nonzero_coords[:, 0].max().item()
        left = nonzero_coords[:, 1].min().item()
        right = nonzero_coords[:, 1].max().item()
        
        width = right - left
        height = bottom - top
        size = max(width, height)
        
        center_y = (top + bottom) // 2
        center_x = (left + right) // 2
        
        half_size = size // 2
        new_top = max(0, center_y - half_size)
        new_bottom = min(H, center_y + half_size)
        new_left = max(0, center_x - half_size)
        new_right = min(W, center_x + half_size)
        
        cropped_image = image[new_top:new_bottom, new_left:new_right]
        cropped_mask = mask[new_top:new_bottom, new_left:new_right]
        
        cropped_image = cropped_image.unsqueeze(0)
        cropped_mask = cropped_mask.unsqueeze(0)
        
        return cropped_image, cropped_mask
    
    def apply_image_preprocess(self, image, strength):
        original_shape = image.shape
        original_device = image.device
        
        if torch.is_tensor(image):
            if len(image.shape) == 4:
                image_np = (image[0].cpu().numpy() * 255).astype(np.uint8)
            else:
                image_np = (image.cpu().numpy() * 255).astype(np.uint8)
        
        if strength < 0:
            abs_strength = abs(strength)
            kernel_size = int(3 + abs_strength * 12) // 2 * 2 + 1
            sigma = 0.3 + abs_strength * 2.7
            processed = cv2.GaussianBlur(image_np, (kernel_size, kernel_size), sigma)
        elif strength > 0:
            kernel = np.array([[-1,-1,-1],
                             [-1, 9,-1],
                             [-1,-1,-1]]) * strength + np.array([[0,0,0],
                                                               [0,1,0],
                                                               [0,0,0]]) * (1 - strength)
            processed = cv2.filter2D(image_np, -1, kernel)
            processed = np.clip(processed, 0, 255)
        else:
            processed = image_np
        
        processed_tensor = torch.from_numpy(processed.astype(np.float32) / 255.0).to(original_device)
        if len(original_shape) == 4:
            processed_tensor = processed_tensor.unsqueeze(0)
        
        return processed_tensor
    
    def apply_style_strength(self, cond, txt, strength, mode="lerp"):
        if mode == "lerp":
            if txt.shape[1] != cond.shape[1]:
                txt_mean = txt.mean(dim=1, keepdim=True)
                txt_expanded = txt_mean.expand(-1, cond.shape[1], -1)
                return torch.lerp(txt_expanded, cond, strength)
            return torch.lerp(txt, cond, strength)
        
        elif mode == "feature_boost":
            mean = torch.mean(cond, dim=-1, keepdim=True)
            std = torch.std(cond, dim=-1, keepdim=True)
            normalized = (cond - mean) / (std + 1e-6)
            boost = torch.tanh(normalized * (strength * 2.0))
            return cond * (1 + boost * 2.0)
    
        elif mode == "frequency":
            try:
                B, N, C = cond.shape
                x = cond.float()
                fft = torch.fft.rfft(x, dim=-1)
                magnitudes = torch.abs(fft)
                phases = torch.angle(fft)
                freq_dim = fft.shape[-1]
                freq_range = torch.linspace(0, 1, freq_dim, device=cond.device)
                alpha = 2.0 * strength
                beta = 0.5
                filter_response = 1.0 + alpha * torch.pow(freq_range, beta)
                filter_response = filter_response.view(1, 1, -1)
                enhanced_magnitudes = magnitudes * filter_response
                enhanced_fft = enhanced_magnitudes * torch.exp(1j * phases)
                enhanced = torch.fft.irfft(enhanced_fft, n=C, dim=-1)
                mean = enhanced.mean(dim=-1, keepdim=True)
                std = enhanced.std(dim=-1, keepdim=True)
                enhanced_norm = (enhanced - mean) / (std + 1e-6)
                mix_ratio = torch.sigmoid(torch.tensor(strength * 2 - 1))
                result = torch.lerp(cond, enhanced_norm.to(cond.dtype), mix_ratio)
                residual = (result - cond) * strength
                final = cond + residual
                return final
            except Exception as e:
                print(f"é¢‘ç‡å¤„ç†å‡ºé”™: {e}")
                print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {cond.shape}")
                return cond
                
        return cond
    
    def apply_stylemodel(self, style_model, clip_vision, image, positive, 
                        patch_res=16, style_strength=1.0, prompt_strength=1.0, 
                        noise_level=0.0, crop="none", sharpen=0.0, guidance=30,
                        blend_mode="lerp", mask=None, ):
        
        
        conditioning = positive

        style_model_path = folder_paths.get_full_path_or_raise("style_models", style_model)
        style_model = comfy.sd.load_style_model(style_model_path)

        clip_path = folder_paths.get_full_path_or_raise("clip_vision", clip_vision)
        clip_vision = comfy.clip_vision.load(clip_path)


        
        processed_image = image.clone()
        if sharpen != 0:
            processed_image = self.apply_image_preprocess(processed_image, sharpen)
        if crop == "mask_area" and mask is not None:
            processed_image, mask = self.crop_to_mask_area(processed_image, mask)
            clip_vision_output = clip_vision.encode_image(processed_image, crop=False)
        else:
            crop_image = True if crop == "center" else False
            clip_vision_output = clip_vision.encode_image(processed_image, crop=crop_image)
        
        cond = style_model.get_cond(clip_vision_output)
        
        B = cond.shape[0]
        H = W = int(math.sqrt(cond.shape[1]))
        C = cond.shape[2]
        cond = cond.reshape(B, H, W, C)
        
        new_H = H * patch_res // 16
        new_W = W * patch_res // 16
        
        cond = torch.nn.functional.interpolate(
            cond.permute(0, 3, 1, 2),
            size=(new_H, new_W),
            mode='bilinear',
            align_corners=False
        )
        
        cond = cond.permute(0, 2, 3, 1)
        cond = cond.reshape(B, -1, C)
        cond = cond.flatten(start_dim=0, end_dim=1).unsqueeze(dim=0)
        
        c_out = []
        for t in conditioning:
            txt, keys = t
            keys = keys.copy()
            
            if prompt_strength != 1.0:
                txt_enhanced = txt * (prompt_strength ** 3)
                txt_repeated = txt_enhanced.repeat(1, 2, 1)
                txt = txt_repeated
            
            if style_strength != 1.0:
                processed_cond = self.apply_style_strength(
                    cond, txt, style_strength, blend_mode
                )
            else:
                processed_cond = cond
    
            if mask is not None:
                feature_size = int(math.sqrt(processed_cond.shape[1]))
                processed_mask = torch.nn.functional.interpolate(
                    mask.unsqueeze(1) if mask.dim() == 3 else mask,
                    size=(feature_size, feature_size),
                    mode='bilinear',
                    align_corners=False
                ).flatten(1).unsqueeze(-1)
                
                if txt.shape[1] != processed_cond.shape[1]:
                    txt_mean = txt.mean(dim=1, keepdim=True)
                    txt_expanded = txt_mean.expand(-1, processed_cond.shape[1], -1)
                else:
                    txt_expanded = txt
                
                processed_cond = processed_cond * processed_mask + \
                               txt_expanded * (1 - processed_mask)
    
            if noise_level > 0:
                noise = torch.randn_like(processed_cond)
                noise = (noise - noise.mean()) / (noise.std() + 1e-8)
                processed_cond = torch.lerp(processed_cond, noise, noise_level)
                processed_cond = processed_cond * (1.0 + noise_level)
                
            c_out.append([torch.cat((txt, processed_cond), dim=1), keys])
        
        
        
        positive = node_helpers.conditioning_set_values(c_out, {"guidance": guidance})

        
        return (positive,)


class Stack_Redux:
    def __init__(self):
        self.unfold_batch = False

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {

                "style_model": (folder_paths.get_filename_list("style_models"), {"default": "flux1-redux-dev.safetensors"}),
                "clip_vision": (folder_paths.get_filename_list("clip_vision"), {"default": "sigclip_vision_patch14_384.safetensors"}),

                "crop": (["center", "mask_area", "none"], {
                    "default": "none",
                    "tooltip": "è£å‰ªæ¨¡å¼ï¼šcenter-ä¸­å¿ƒè£å‰ª, mask_area-é®ç½©åŒºåŸŸè£å‰ª, none-ä¸è£å‰ª"
                }),
                "sharpen": ("FLOAT", {
                    "default": 0.0,
                    "min": -5.0,
                    "max": 5.0,
                    "step": 0.1,
                    "tooltip": "é”åŒ–å¼ºåº¦ï¼šè´Ÿå€¼ä¸ºæ¨¡ç³Šï¼Œæ­£å€¼ä¸ºé”åŒ–ï¼Œ0ä¸ºä¸å¤„ç†"
                }),
                "patch_res": ("INT", {
                    "default": 16,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "tooltip": "patchåˆ†è¾¨ç‡ï¼Œæ•°å€¼è¶Šå¤§åˆ†å—è¶Šç»†è‡´"
                }),
                "style_strength": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.01,
                    "tooltip": "é£æ ¼å¼ºåº¦ï¼Œè¶Šé«˜è¶Šåå‘å‚è€ƒå›¾ç‰‡"
                }),
                "prompt_strength": ("FLOAT", { 
                    "default": 1.0,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.01,
                    "tooltip": "æ–‡æœ¬æç¤ºè¯å¼ºåº¦ï¼Œè¶Šé«˜æ–‡æœ¬ç‰¹å¾è¶Šå¼º"
                }),
                "blend_mode": (["lerp", "feature_boost", "frequency"], {
                    "default": "lerp",
                    "tooltip": "é£æ ¼å¼ºåº¦çš„è®¡ç®—æ–¹å¼ï¼š\n" +
                            "lerp - çº¿æ€§æ··åˆ - é«˜åº¦å‚è€ƒåŸå›¾\n" +
                            "feature_boost - ç‰¹å¾å¢å¼º - å¢å¼ºçœŸå®æ„Ÿ\n" +
                            "frequency - é¢‘ç‡å¢å¼º - å¢å¼ºé«˜é¢‘ç»†èŠ‚"
                }),
                "noise_level": ("FLOAT", { 
                    "default": 0.0,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "tooltip": "æ·»åŠ éšæœºå™ªå£°çš„å¼ºåº¦ï¼Œå¯ç”¨äºä¿®å¤é”™è¯¯ç»†èŠ‚"
                }),
            },
            "optional": { 
                "image": ("IMAGE",),
                "mask": ("MASK", ), 
                "guidance": ("FLOAT", {"default": 30, "min": 0.0, "max": 100.0, "step": 0.1}),
                "redux_stack": ("REDUX_STACK",),  # æ–°å¢è¾“å…¥
            }
        }

    RETURN_TYPES = ("REDUX_STACK",)
    RETURN_NAMES = ("redux_stack",)
    FUNCTION = "redux_stack"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"

    def redux_stack(self,style_model, clip_vision,  crop, sharpen, patch_res, style_strength, prompt_strength, blend_mode, noise_level, image=None,mask=None, guidance=30, redux_stack=None):

        if image is None:
            return (None,)
        

        # åˆå§‹åŒ–redux_list
        redux_list = []

        # å¦‚æœä¼ å…¥äº†redux_stackï¼Œå°†å…¶ä¸­çš„å†…å®¹æ·»åŠ åˆ°redux_listä¸­
        if redux_stack is not None:
            redux_list.extend([redux for redux in redux_stack if redux[0] != "None"])

        # å°†å½“å‰Reduxçš„ç›¸å…³ä¿¡æ¯æ‰“åŒ…æˆä¸€ä¸ªå…ƒç»„ï¼Œå¹¶æ·»åŠ åˆ°redux_listä¸­
        redux_info = (
            style_model,
            clip_vision,
            image,
            crop,
            sharpen,
            patch_res,
            style_strength,
            prompt_strength,
            blend_mode,
            noise_level,
            mask,
            guidance
        )
        redux_list.append(redux_info)

        return (redux_list,)


class Apply_Redux:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
            "positive": ("CONDITIONING",),
            "redux_stack": ("REDUX_STACK",),
            }
        }

    RETURN_TYPES = ("CONDITIONING",)
    RETURN_NAMES = ("positive",)
    FUNCTION = "apply_redux_stack"
    CATEGORY = "Apt_Preset/stack"

    def apply_redux_stack(self, positive, redux_stack):
        if not redux_stack:
            raise ValueError("redux_stack ä¸èƒ½ä¸ºç©º")

        chx_yc_lg_redux = YC_LG_Redux()

        # éå† redux_stack ä¸­çš„æ¯ä¸ª Redux é…ç½®
        for redux_info in redux_stack:
            (
                style_model,
                clip_vision,
                image,
                crop,
                sharpen,
                patch_res,
                style_strength,
                prompt_strength,
                blend_mode,
                noise_level,
                mask,
                guidance
            ) = redux_info

            # ç›´æ¥è°ƒç”¨ chx_YC_LG_Redux ç±»ä¸­çš„ apply_stylemodel æ–¹æ³•
            positive = chx_yc_lg_redux.apply_stylemodel(
                style_model, clip_vision, image, positive, 
                patch_res=patch_res, style_strength=style_strength, prompt_strength=prompt_strength, 
                noise_level=noise_level, crop=crop, sharpen=sharpen, guidance=guidance,
                blend_mode=blend_mode, mask=mask
            )[0]

        return (positive,)




#endregion------------------Redux stack----------------------





class Stack_pre_Mark2:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mask_mode": (["original", "fill", "fill_block", "outline", "outline_block", "circle", "outline_circle"], {"default": "fill"}),
                "smoothness": ("INT", {"default": 0, "min": 0, "max": 150, "step": 1}),
                "mask_expand": ("INT", {"default": 0, "min": -500, "max": 1000, "step": 1}),
                "mask_min": ("FLOAT", {"default": 0.0, "min": -10.0, "max": 1.0, "step": 0.01}),
                "mask_max": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
              
            }
        }

    RETURN_TYPES = ("MASK_STACK2", )
    RETURN_NAMES = ("mask_stack", )
    FUNCTION = "visualize"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"

    def visualize(self, mask_mode,smoothness, mask_expand, mask_min, mask_max):  
        
        mask_stack2 = (
            mask_mode,
            smoothness, 
            mask_expand,            
            mask_min, 
            mask_max, 

        )
        return (mask_stack2,)




#endregion---------------------æ”¶çº³----------------------------



class sum_stack_AD:

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "context": ("RUN_CONTEXT",),
            },
            "optional": {
                "model":("MODEL", ),
                "chx_Merge": ("RUN_CONTEXT",),
                "lora_stack": ("LORASTACK",),
                "ipa_stack": ("IPA_STACK",),
                "pos_sch_stack": ("PROMPT_SCHEDULE_STACK",),
                "cn_stack": ("CN_STACK",),
                "latent_stack": ("LATENT_STACK",),
            },
            "hidden": {},
        }
        
    RETURN_TYPES = ("RUN_CONTEXT", "MODEL", "CONDITIONING", "CONDITIONING", "LATENT", "IMAGE" )
    RETURN_NAMES = ("context", "model", "positive", "negative", "latent", "graph")
    FUNCTION = "merge"
    CATEGORY = "Apt_Preset/chx_tool"

    def merge(self, model=None,chx_Merge=None, ipa_stack=None, lora_stack=None, pos_sch_stack=None, cn_stack=None, context=None,latent_stack=None,):

        if chx_Merge is not None :
            context = Data_chx_Merge().merge(context, chx_Merge, chx_Merge)[0] 

        clip = context.get("clip", None)
        vae = context.get("vae", None)
        latent = context.get("latent",None)
        positive = context.get("positive", None)
        negative = context.get("negative", None)

        if model is None:
            model = context.get("model")
        if lora_stack is not None:
            model, clip = Apply_LoRAStack().apply_lora_stack(model, clip, lora_stack)
            pos = context.get("pos","a girl")
            neg = context.get("neg","bad quality")  
            positive, = CLIPTextEncode().encode(clip, pos)
            negative, = CLIPTextEncode().encode(clip, neg)

        if ipa_stack is not None:
            model, = Apply_IPA().apply_ipa_stack(model, ipa_stack)


        if pos_sch_stack is None:
            graph = None
        if pos_sch_stack is not None:
            positive, graph = AD_sch_prompt_apply().create_schedule(clip, pos_sch_stack)
        
        if cn_stack is not None:
            positive, = Apply_adv_CN().apply_controlnet(positive, cn_stack)

#-------------------------------------------------------------------- 
        if latent_stack is not None:
            model, positive, negative, latent = Apply_latent().apply_latent_stack(model, positive, negative, vae, latent_stack)

        context = new_context(context, clip=clip, positive=positive, latent=latent, negative=negative, model=model)
        return (context, model, positive, negative, latent,graph) 



#endregion-------stack_pack------------------------------------------------------------------------------#

  


class pre_controlnet_union:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {},
            "optional": {
                "context": ("RUN_CONTEXT",),
                "image1": ("IMAGE",),
                "image2": ("IMAGE",),
                "dual_image": ("IMAGE",),

                "controlNet": (['None'] + folder_paths.get_filename_list("controlnet"),),
                "type1": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength1": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent1": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent1": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                
                "type2": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength2": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent2": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent2": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                
                "dual_type_1": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "dual_type_2": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength3": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent3": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent3": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
            }
        }

    RETURN_TYPES = ("RUN_CONTEXT","CONDITIONING","CONDITIONING", )
    RETURN_NAMES = ("context", "positive", "negative",  )
    CATEGORY = "Apt_Preset/chx_tool/controlnet"
    FUNCTION = "load_controlnet"
    DESCRIPTION = """
    - dual_image: åŒæ§å›¾ï¼Œå¸¸è§çš„æ˜¯çº¿ç¨¿+æ·±åº¦, pose+æ·±åº¦
    - dual_type_1: åªé€‰ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥å½“å•æ§å›¾ï¼Œ
    - dual_type_2: åªé€‰ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥å½“å•æ§å›¾
    """



    def load_controlnet(self, strength1, strength2, strength3, 
                       start_percent1=0.0, end_percent1=1.0,
                       start_percent2=0.0, end_percent2=1.0,
                       start_percent3=0.0, end_percent3=1.0,
                       context=None, image1=None, image2=None, dual_image=None,
                       controlNet=None, type1=None, type2=None, dual_type_1=None,dual_type_2=None,
                       extra_concat=[]):

        positive = context.get("positive", [])
        negative = context.get("negative", [])
        vae = context.get("vae", None)
        
        if controlNet == "None":
            return (context, positive, negative)
            
        control_net = ControlNetLoader().load_controlnet(controlNet)[0]

        # å¤„ç†ç¬¬ä¸€ä¸ªControlNet
        if type1 != "None" and strength1 != 0 and image1 is not None:
            control_net = SetUnionControlNetType().set_controlnet_type(control_net, type1)[0]
            out =  ControlNetApplyAdvanced().apply_controlnet(positive, negative, control_net, image1, 
                                  strength1, start_percent1, end_percent1, 
                                  vae, extra_concat)
            positive, negative = out[0], out[1]

        # å¤„ç†ç¬¬äºŒä¸ªControlNet
        if type2 != "None" and strength2 != 0 and image2 is not None:
            control_net = SetUnionControlNetType().set_controlnet_type(control_net, type2)[0]
            out =  ControlNetApplyAdvanced().apply_controlnet(positive, negative, control_net, image2, 
                                  strength2, start_percent2, end_percent2, 
                                  vae, extra_concat)
            positive, negative = out[0], out[1]

        # å¤„ç†ç¬¬ä¸‰ä¸ªControlNet
        if strength3 != 0 and dual_image is not None:
           if dual_type_1 == "None":
               control_net = SetUnionControlNetType().set_controlnet_type(control_net, dual_type_1)[0]
           if dual_type_2 != "None":
               control_net = SetUnionControlNetType().set_controlnet_type(control_net, dual_type_2)[0]
           
           out =  ControlNetApplyAdvanced().apply_controlnet(positive, negative, control_net, dual_image, 
                                    strength3, start_percent3, end_percent3, 
                                    vae, extra_concat)
           
        positive, negative = out[0], out[1]

        context = new_context(context, positive=positive, negative=negative)
        return (context, positive, negative)




class Apply_CN_union:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
            },
            "optional": {
                "vae": ("VAE",),
                "union_stack": ("UNION_STACK",),
            }
        }

    RETURN_TYPES = ("CONDITIONING","CONDITIONING",)
    RETURN_NAMES = ("positive", "negative",)
    CATEGORY = "Apt_Preset/stack"
    FUNCTION = "apply_union_stack"

    def apply_union_stack(self, positive, negative, vae=None, union_stack=None, extra_concat=[]):
        # æ£€æŸ¥union_stackæ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if union_stack is not None and len(union_stack) > 0:
            # éå†æ ˆä¸­çš„æ¯ä¸ªæ§åˆ¶ç½‘ç»œé…ç½®
            for item in union_stack:
                # æ ¹æ®å…ƒç»„é•¿åº¦å’Œæ ‡è¯†ç¬¦åˆ¤æ–­é…ç½®ç±»å‹
                if len(item) >= 6:
                    control_net, image, strength, start_percent, end_percent, mode = item[:6]
                    
                    # æ ¹æ®æ¨¡å¼å¤„ç†æ§åˆ¶ç½‘ç»œç±»å‹
                    if mode == "dual" and len(item) >= 8:
                        # åŒæ§æ¨¡å¼
                        type1, type2 = item[6], item[7]
                        # å¯¹äºåŒæ§ï¼Œæˆ‘ä»¬éœ€è¦åˆ†åˆ«åº”ç”¨ä¸¤ä¸ªç±»å‹
                        # å…ˆåº”ç”¨ç¬¬ä¸€ä¸ªç±»å‹
                        if type1 != "None":
                            control_net1 = control_net.copy()
                            control_net1 = SetUnionControlNetType().set_controlnet_type(control_net1, type1)[0]
                            out = ControlNetApplyAdvanced().apply_controlnet(
                                positive, negative, control_net1, image, 
                                strength, start_percent, end_percent, 
                                vae, extra_concat
                            )
                            positive, negative = out[0], out[1]
                        
                        # å†åº”ç”¨ç¬¬äºŒä¸ªç±»å‹
                        if type2 != "None":
                            control_net2 = control_net.copy()
                            control_net2 = SetUnionControlNetType().set_controlnet_type(control_net2, type2)[0]
                            out = ControlNetApplyAdvanced().apply_controlnet(
                                positive, negative, control_net2, image, 
                                strength, start_percent, end_percent, 
                                vae, extra_concat
                            )
                            positive, negative = out[0], out[1]
                    
                    elif mode == "single" and len(item) >= 7:
                        # å•æ§æ¨¡å¼
                        control_type = item[6]
                        if control_type != "None":
                            control_net = SetUnionControlNetType().set_controlnet_type(control_net, control_type)[0]
                            out = ControlNetApplyAdvanced().apply_controlnet(
                                positive, negative, control_net, image, 
                                strength, start_percent, end_percent, 
                                vae, extra_concat
                            )
                            positive, negative = out[0], out[1]
                    
                    elif mode == "none":
                        # æ— ç±»å‹æŒ‡å®šï¼Œç›´æ¥åº”ç”¨
                        out = ControlNetApplyAdvanced().apply_controlnet(
                            positive, negative, control_net, image, 
                            strength, start_percent, end_percent, 
                            vae, extra_concat
                        )
                        positive, negative = out[0], out[1]
                        
        return (positive, negative,)



#--------------------------------------


class Stack_CN_union:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
            },
            "optional": {
                "image": ("IMAGE",),
                "controlNet": (["None"] + folder_paths.get_filename_list("controlnet"),),
                "type1": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "type2": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "union_stack": ("UNION_STACK",),
            }
        }

    RETURN_TYPES = ("UNION_STACK", )
    RETURN_NAMES = ("union_stack", )
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"
    FUNCTION = "load_controlnet"
    DESCRIPTION = """
    - åŒæ—¶é€‰type1å’Œtype2: ç”¨åŒæ§å›¾ï¼Œå¸¸è§çš„æ˜¯çº¿ç¨¿+æ·±åº¦, pose+æ·±åº¦
    - dual_type_1: åªé€‰ä¸€ä¸ªï¼Œå•æ§å›¾
    - dual_type_2: åªé€‰ä¸€ä¸ªï¼Œå•æ§å›¾
    """

    def load_controlnet(self,  
                        image=None,
                        controlNet="None",
                        type1="None",
                        type2="None",
                        strength=0.8,
                        start_percent=0.0,
                        end_percent=1.0,
                        union_stack=None,
                        extra_concat=[]):
        
        stack_list = []
        if union_stack is not None:
            stack_list.extend([item for item in union_stack if item[0] is not None])

        if controlNet != "None" and strength != 0 and image is not None:
            # å…ˆåŠ è½½control_net
            control_net = ControlNetLoader().load_controlnet(controlNet)[0]
            
            # æ ¹æ®type1å’Œtype2è®¾ç½®æ§åˆ¶ç½‘ç»œç±»å‹
            if type1 != "None" and type2 != "None":
                # åŒæ§æ¨¡å¼ï¼šåŒæ—¶è®¾ç½®ä¸¤ä¸ªç±»å‹
                stack_item = (control_net, image, strength, start_percent, end_percent, "dual", type1, type2)
            elif type1 != "None":
                # å•æ§æ¨¡å¼ï¼šåªè®¾ç½®type1
                stack_item = (control_net, image, strength, start_percent, end_percent, "single", type1)
            elif type2 != "None":
                # å•æ§æ¨¡å¼ï¼šåªè®¾ç½®type2
                stack_item = (control_net, image, strength, start_percent, end_percent, "single", type2)
            else:
                # æ— ç±»å‹æŒ‡å®š
                stack_item = (control_net, image, strength, start_percent, end_percent, "none")
            
            stack_list.append(stack_item)

        return (tuple(stack_list), )




class Stack_CN_union3:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {},
            "optional": {
                "controlNet": (["None"] + folder_paths.get_filename_list("controlnet"),),
                
                "image1": ("IMAGE",),
                "type1_a": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "type1_b": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength1": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent1": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent1": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                
                "image2": ("IMAGE",),
                "type2_a": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "type2_b": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength2": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent2": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent2": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                
                "image3": ("IMAGE",),
                "type3_a": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "type3_b": (["None"] + list(UNION_CONTROLNET_TYPES.keys()),),
                "strength3": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent3": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "end_percent3": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
            }
        }

    RETURN_TYPES = ("UNION_STACK",)
    RETURN_NAMES = ("union_stack3",)
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"
    FUNCTION = "load_controlnet3"

    def load_controlnet3(self,  
                        controlNet="None",
                        image1=None, type1_a="None", type1_b="None",
                        strength1=0.8, start_percent1=0.0, end_percent1=1.0,
                        image2=None, type2_a="None", type2_b="None",
                        strength2=0.8, start_percent2=0.0, end_percent2=1.0,
                        image3=None, type3_a="None", type3_b="None",
                        strength3=0.8, start_percent3=0.0, end_percent3=1.0,
                        extra_concat=[]):
        
        stack_list = []

        if controlNet == "None":
            return (tuple(stack_list),)
        
        control_net = ControlNetLoader().load_controlnet(controlNet)[0]

        def add_cn_to_stack(image, type_a, type_b, strength, start_p, end_p):
            if strength != 0 and image is not None:
                # åªè¦æœ‰ä¸€ä¸ªç±»å‹ä¸ä¸º"None"å°±æœ‰æ•ˆ
                if type_a != "None" and type_b != "None":
                    # åŒæ§æ¨¡å¼
                    stack_item = (control_net.copy(), image, strength, start_p, end_p, "dual", type_a, type_b)
                    stack_list.append(stack_item)
                elif type_a != "None" or type_b != "None":
                    # å•æ§æ¨¡å¼
                    control_type = type_a if type_a != "None" else type_b
                    stack_item = (control_net.copy(), image, strength, start_p, end_p, "single", control_type)
                    stack_list.append(stack_item)

        # å¤„ç†æ¯ä¸ªå›¾åƒè¾“å…¥
        add_cn_to_stack(image1, type1_a, type1_b, strength1, start_percent1, end_percent1)
        add_cn_to_stack(image2, type2_a, type2_b, strength2, start_percent2, end_percent2)
        add_cn_to_stack(image3, type3_a, type3_b, strength3, start_percent3, end_percent3)

        return (tuple(stack_list),)








#region--------- inpaint-------------


class Stack_inpaint:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {

            },
            "optional": {
                "control_net": (["None"] + folder_paths.get_filename_list("controlnet"), {"default": "None"}),
                "mask_mode": (["Ailmama", "mask_black", "mask_white", "mask_gray"], {"default": "Ailmama"}),
                "latent_image": ("IMAGE", ),  
                "latent_mask": ("MASK", ),    
                "smoothness": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001}),
                "end_percent": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.001})
            }
        }

    RETURN_TYPES = ("INPAINT_STACK",)
    RETURN_NAMES = ("inpaint",)
    FUNCTION = "pack_inpaint_params"
    CATEGORY = "Apt_Preset/stack/ğŸ˜ºbackup"
    DESCRIPTION = """
    - mask_modeï¼šé’ˆå¯¹é®ç½©åŒºå›¾ç‰‡çš„é¢„å¤„ç†æ–¹å¼ï¼Œmask_blackå¤„ç†æˆé»‘è‰²å—ï¼Œmask_whiteå¤„ç†æˆç™½è‰²å—ï¼Œmask_grayå¤„ç†æˆç°è‰²å—

    - æ¨¡å¼ä¸€ï¼šæ²¡æœ‰é€‰æ‹©controlnetï¼Œé‡‡æ ·å†…éƒ¨é‡ç»˜æ¨¡å¼InpaintModelConditioning
    - ï¼ˆ1ï¼‰ä½å™ªé‡ç»˜ï¼šé€‚åˆæ‰€æœ‰æ¨¡å‹ï¼Œé€‚åˆä½å™ªé‡ç»˜ï¼Œç»†èŠ‚ä¿®å¤ï¼Œå¾®æ”¹å˜
    - ï¼ˆ2ï¼‰é«˜å™ªé‡ç»˜ï¼šé€‚åˆå›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œå¦‚qwenEdit,kontext

    - æ¨¡å¼äºŒï¼šé€‰æ‹©controlnetï¼Œé‡‡ç”¨CNæ§åˆ¶æ¨¡å¼ï¼Œcontrolnet inpaintingçš„æ¨¡å‹
    - SDé‡ç»˜ï¼Œç”¨mask_black
    - XLé‡ç»˜ï¼Œç”¨mask_white
    - fluxé‡ç»˜ï¼Œç”¨Alimama
    - qwen_imageé‡ç»˜ï¼Œç”¨Alimama
    """


    def pack_inpaint_params(self, control_net, mask_mode="Ailmama", latent_image=None, latent_mask=None, smoothness=0.0, strength=1.0, start_percent=0.0, end_percent=1.0):
        inpaint_sum_pack = ( control_net, mask_mode, latent_image, latent_mask,smoothness, strength, start_percent, end_percent)
        return (inpaint_sum_pack,)



class pack_inpaint: #éšè—
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "vae": ("VAE",),
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "control_net": (["None"] + folder_paths.get_filename_list("controlnet"), {"default": "None"}),
                "mask_mode": (["Ailmama", "mask_black", "mask_white", "mask_gray"], {"default": "Ailmama"}),
                "latent_image": ("IMAGE", ),  # å·²æ”¹ä¸º latent_imageï¼ˆIMAGE ç±»å‹ï¼‰
                "latent_mask": ("MASK", ),    # å…³é”®ä¿®æ”¹ï¼šmask â†’ latent_maskï¼ˆç»Ÿä¸€å‚æ•°åï¼‰
                "smoothness": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001}),
                "end_percent": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.001})
            },
            "optional": {}
        }

    RETURN_TYPES = ("CONDITIONING","CONDITIONING","LATENT",)
    RETURN_NAMES = ("positive", "negative","latent")
    FUNCTION = "apply_controlnet"
    CATEGORY = "Apt_Preset/chx_tool/controlnet"

    def apply_controlnet(self, vae, positive, negative, control_net, latent_image, strength, smoothness, start_percent, end_percent, mask_mode="mask_black", latent_mask=None):
        # 1. ç¼–ç å›¾åƒä¸º latentï¼ˆlatent_image æ˜¯ IMAGE ç±»å‹ï¼Œä¿æŒåŸé€»è¾‘ï¼‰
        latent = encode(vae, latent_image)[0] if latent_image is not None else None

        # 2. æœ‰ latent_mask æ—¶ï¼šå¤„ç†é®ç½©å¹¶ç»‘å®šåˆ° latentï¼›æ— åˆ™è¾“å‡ºæ™®é€š latent
        if latent_mask is not None:
            # é®ç½©å¹³æ»‘ï¼ˆsmoothness > 0 ç”Ÿæ•ˆï¼‰
            if smoothness > 0:
                latent_mask = smoothness_mask(latent_mask, smoothness)  # ç»Ÿä¸€ç”¨ latent_mask
            # ç»™ latent ç»‘å®šé®ç½©
            if latent is not None:
                latent = set_mask(latent, latent_mask)  # ä¼ å…¥ latent_mask

        #---------------------------------------------------æ²¡æœ‰cnå°±æ˜¯inpaint--------------------------------------------#
        if control_net == "None":
            # ä¼ å…¥ latent_image å’Œ latent_mask æ‰§è¡Œ inpaintï¼ˆå‚æ•°åç»Ÿä¸€ï¼‰
            positive, negative, latent = InpaintModelConditioning().encode(positive, negative, latent_image, vae, latent_mask, True)
            return (positive, negative, latent)

        #---------------------------------------------------æœ‰cnåˆ†ä¸¤æƒ…å†µ,å‚è€ƒå›¾å¤„ç†--------------------------------------------#
        else:
            control_net = ControlNetLoader().load_controlnet(control_net)[0]

            if mask_mode == "Ailmama":
                out = InpaintingAliMamaApply().apply_inpaint_controlnet(
                    positive, negative, control_net, vae, latent_image, latent_mask,  # ä¼  latent_mask
                    strength, start_percent, end_percent
                )
            else: 
                # å¤„ç†å‚è€ƒå›¾ï¼ˆä¼ å…¥ latent_image å’Œ latent_maskï¼‰
                processed_image = self.InpaintPreprocessor(latent_image, latent_mask, mask_mode)[0]
                out = ControlNetApplyAdvanced().apply_controlnet(
                    positive, negative, control_net, processed_image, strength, 
                    start_percent, end_percent, vae=None, extra_concat=[]
                )
            
            # è¾“å‡ºï¼šæœ‰ latent_mask åˆ™å¸¦é®ç½©ï¼Œæ— åˆ™æ— é®ç½©
            return (out[0], out[1], latent)

    def InpaintPreprocessor(self, image, latent_mask, mask_color="mask_black"):
        # é®ç½©å°ºå¯¸é€‚é…å›¾åƒå°ºå¯¸ï¼ˆå‚æ•°åæ”¹ä¸º latent_maskï¼‰
        latent_mask = torch.nn.functional.interpolate(
            latent_mask.reshape((-1, 1, latent_mask.shape[-2], latent_mask.shape[-1])),
            size=(image.shape[1], image.shape[2]),
            mode="bilinear"
        )
        latent_mask = latent_mask.movedim(1, -1).expand((-1, -1, -1, 3))
        image = image.clone()

        # æ ¹æ®æ¨¡å¼è®¾ç½®é®ç½©åŒºåŸŸåƒç´ å€¼
        if mask_color == "mask_black":
            masked_pixel = -1.0  
        elif mask_color == "mask_white":
            masked_pixel = 1.0  
        elif mask_color == "mask_gray":
            masked_pixel = 0.0  
        else:
            masked_pixel = -1.0  # é»˜è®¤é»‘è‰²
        
        image[latent_mask > 0.5] = masked_pixel  # ä½¿ç”¨ latent_mask åˆ¤æ–­
        return (image,)








class InpaintingAliMamaApply():
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {"positive": ("CONDITIONING", ),
                             "negative": ("CONDITIONING", ),
                             "control_net": ("CONTROL_NET", ),
                             "vae": ("VAE", ),
                             "image": ("IMAGE", ),
                             "mask": ("MASK", ),
                             "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                             "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001}),
                             "end_percent": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.001})
                             }}

    FUNCTION = "apply_inpaint_controlnet"
    CATEGORY = "conditioning/controlnet"

    def apply_inpaint_controlnet(self, positive, negative, control_net, vae, image, mask, strength, start_percent, end_percent):
        extra_concat = []
        if control_net.concat_mask:
            mask = 1.0 - mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1]))
            mask_apply = comfy.utils.common_upscale(mask, image.shape[2], image.shape[1], "bilinear", "center").round()
            image = image * mask_apply.movedim(1, -1).repeat(1, 1, 1, image.shape[3])
            extra_concat = [mask]

        out= self.apply_controlnet(positive, negative, control_net, image, strength, start_percent, end_percent, vae=vae, extra_concat=extra_concat)
        
        return  (out[0], out[1])

    def apply_controlnet(self, positive, negative, control_net, image, strength, start_percent, end_percent, vae=None, extra_concat=[]):
        if strength == 0:
            return (positive, negative)

        control_hint = image.movedim(-1,1)
        cnets = {}

        out = []
        for conditioning in [positive, negative]:
            c = []
            for t in conditioning:
                d = t[1].copy()

                prev_cnet = d.get('control', None)
                if prev_cnet in cnets:
                    c_net = cnets[prev_cnet]
                else:
                    c_net = control_net.copy().set_cond_hint(control_hint, strength, (start_percent, end_percent), vae=vae, extra_concat=extra_concat)
                    c_net.set_previous_controlnet(prev_cnet)
                    cnets[prev_cnet] = c_net

                d['control'] = c_net
                d['control_apply_to_uncond'] = False
                n = [t[0], d]
                c.append(n)
            out.append(c)
        return (out[0], out[1])



class pre_inpaint_sum:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "context": ("RUN_CONTEXT",),
                "control_net": (["None"] + folder_paths.get_filename_list("controlnet"), {"default": "None"}),
                "mask_mode": (["Ailmama", "mask_black", "mask_white", "mask_gray"], {"default": "Ailmama"}),
                "latent_image": ("IMAGE", ),  
                "latent_mask": ("MASK", ),    
                "smoothness": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 20.0, "step": 0.1}),
                "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.01}),
                "start_percent": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.001}),
                "end_percent": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.001})
            },
            "optional": {}
        }

    RETURN_TYPES = ("RUN_CONTEXT","CONDITIONING","CONDITIONING","LATENT",)
    RETURN_NAMES = ("context","positive", "negative","latent")
    FUNCTION = "apply_controlnet"
    CATEGORY = "Apt_Preset/chx_tool/controlnet"
    DESCRIPTION = """- mask_modeï¼šé’ˆå¯¹é®ç½©åŒºå›¾ç‰‡çš„é¢„å¤„ç†æ–¹å¼ï¼Œmask_blackå¤„ç†æˆé»‘è‰²å—ï¼Œmask_whiteå¤„ç†æˆç™½è‰²å—ï¼Œmask_grayå¤„ç†æˆç°è‰²å—
- æ¨¡å¼ä¸€ï¼šæ²¡æœ‰é€‰æ‹©controlnetï¼Œé‡‡æ ·å†…éƒ¨é‡ç»˜æ¨¡å¼InpaintModelConditioning
- ï¼ˆ1ï¼‰ä½å™ªé‡ç»˜ï¼šé€‚åˆæ‰€æœ‰æ¨¡å‹ï¼Œé€‚åˆä½å™ªé‡ç»˜ï¼Œç»†èŠ‚ä¿®å¤ï¼Œå¾®æ”¹å˜
- ï¼ˆ2ï¼‰é«˜å™ªé‡ç»˜ï¼šé€‚åˆå›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œå¦‚qwenEdit,kontext
- æ¨¡å¼äºŒï¼šé€‰æ‹©controlnetï¼Œé‡‡ç”¨CNæ§åˆ¶æ¨¡å¼ï¼Œcontrolnet inpaintingçš„æ¨¡å‹
- SDé‡ç»˜ï¼Œç”¨mask_black
- XLé‡ç»˜ï¼Œç”¨mask_white
- fluxé‡ç»˜ï¼Œç”¨Alimama
- qwen_imageé‡ç»˜ï¼Œç”¨Alimama"""

    def apply_controlnet(self, context, control_net, latent_image, strength, smoothness, start_percent, end_percent, mask_mode="mask_black", latent_mask=None):
        vae = context.get("vae", None)
        positive = context.get("positive", None)
        negative = context.get("negative", None)
          
        out = pack_inpaint().apply_controlnet(
            vae, positive, negative, control_net, latent_image, strength,
            smoothness, start_percent, end_percent, mask_mode, latent_mask
        )

        latent = out[2]
        context = new_context(context, positive=out[0], negative=out[1], latent=latent)
        return (context, out[0], out[1], out[2])




class sum_stack_image:

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "context": ("RUN_CONTEXT",),
            },
            "optional": {
                "model":("MODEL", ),
                "lora_stack": ("LORASTACK",),
                "ipa_stack": ("IPA_STACK",),
                "redux_stack": ("REDUX_STACK",),
                "condi_stack": ("STACK_CONDI", ),
                "union_stack": ("UNION_STACK",),
                "cn_stack": ("CN_STACK",),
                "inpaint": ("INPAINT_STACK",),
                "latent_stack": ("LATENT_STACK",),
            },
            "hidden": {},
        }
        
    RETURN_TYPES = ("RUN_CONTEXT","MODEL", "CONDITIONING","CONDITIONING","LATENT","VAE","CLIP","IMAGE")
    RETURN_NAMES = ("context", "model","positive","negative","latent","vae","clip","image")
    FUNCTION = "merge"
    CATEGORY = "Apt_Preset/chx_tool"

    def merge(self, context=None, model=None, ipa_stack=None, 
              redux_stack=None, lora_stack=None,condi_stack=None,
              union_stack=None, cn_stack=None,inpaint=None, latent_stack=None):
        

        clip = context.get("clip")
        latent = context.get("latent", None)
        vae = context.get("vae", None)
        image = context.get("images", None)
        positive = context.get("positive", None)
        negative = context.get("negative", None)

        if model is None:
            model = context.get("model", None)

        if lora_stack is not None:
            model, clip = Apply_LoRAStack().apply_lora_stack(model, clip, lora_stack)
            pos = context.get("pos","a girl")
            neg = context.get("neg","bad quality")  
            positive, = CLIPTextEncode().encode(clip, pos)
            negative, = CLIPTextEncode().encode(clip, neg)
#-------------------------------------------

        if ipa_stack is not None:
            model, = Apply_IPA().apply_ipa_stack(model, ipa_stack)


        if redux_stack is not None:
            positive, =  Apply_Redux().apply_redux_stack(positive, redux_stack,)

        if condi_stack is not None:
            positive, negative = Apply_condiStack().condiStack(clip, condi_stack)



        if union_stack is not None:
            positive, negative = Apply_CN_union().apply_union_stack(positive, negative, vae, union_stack, extra_concat=[])


        if cn_stack is not None and len(cn_stack) > 0:
            first_element = cn_stack[0]
            
            if len(first_element) == 5:
                positive, negative = Apply_ControlNetStack().apply_controlnet_stack(
                    positive=positive, 
                    negative=negative, 
                    switch="On", 
                    vae=vae,
                    controlnet_stack=cn_stack
                )
            elif len(first_element) == 8:
                positive, = Apply_adv_CN().apply_controlnet(positive, cn_stack)
            else:
                print(f"è­¦å‘Š: æœªçŸ¥çš„æ§åˆ¶ç½‘ç»œå †æ ˆç±»å‹ï¼Œå…ƒç´ é•¿åº¦ä¸º {len(first_element)}")


        if inpaint is not None:
            control_net, mask_mode, latent_image, latent_mask,smoothness, strength, start_percent, end_percent = inpaint

            positive, negative, latent = pack_inpaint().apply_controlnet(
                vae, positive, negative, control_net, latent_image, 
                strength, smoothness, start_percent, end_percent, 
                mask_mode, latent_mask
            )



        if latent_stack is not None:
            model, positive, negative, latent = Apply_latent().apply_latent_stack(model, positive, negative, vae, latent_stack)

        context = new_context(context, clip=clip, positive=positive, negative=negative, model=model, latent=latent, vae=vae)
        return (context, model, positive, negative, latent,vae,clip, image )








#endregion--------- inpaint-------------










