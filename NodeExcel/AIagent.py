



#region-----------GLM4.5V-------------------------------------

import os
import json
import base64
import random
from PIL import Image
import numpy as np
import io
import requests

try:
    from zhipuai import ZhipuAI
    ZHIPUAI_AVAILABLE = True
except ImportError:
    ZhipuAI = None
    ZHIPUAI_AVAILABLE = False

current_dir = os.path.dirname(os.path.abspath(__file__))
json_path = os.path.join(current_dir, "AiPromptPreset.json")


def load_text_prompts():
    if not os.path.exists(json_path):
        # JSON æ–‡ä»¶ä¸å­˜åœ¨æ—¶è¿”å›é»˜è®¤å­—å…¸ï¼Œé¿å…æŠ¥é”™
        return {"None": ""}
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # è¿”å› TEXT_PROMPTSï¼Œè‹¥ä¸å­˜åœ¨åˆ™è¿”å›é»˜è®¤å­—å…¸
        return data.get("TEXT_PROMPTS", {"None": ""})
    except Exception:
        # JSON æ ¼å¼é”™è¯¯æ—¶è¿”å›é»˜è®¤å­—å…¸
        return {"None": ""}

TEXT_PROMPTS = load_text_prompts()

def _log_info(message):
    print(f"[GLM_Nodes] ä¿¡æ¯ï¼š{message}")

def _log_warning(message):
    print(f"[GLM_Nodes] è­¦å‘Šï¼š{message}")

def _log_error(message):
    print(f"[GLM_Nodes] é”™è¯¯ï¼š{message}")

def get_zhipuai_api_key():
    env_api_key = os.getenv("ZHIPUAI_API_KEY")
    if env_api_key:
        _log_info("ä½¿ç”¨ç¯å¢ƒå˜é‡ API Keyã€‚")
        return env_api_key
    _log_warning("æœªè®¾ç½®ç¯å¢ƒå˜é‡ ZHIPUAI_API_KEYã€‚")
    return ""

# åŸæœ‰æ¨¡å‹åˆ—è¡¨ä¿ç•™
ZHIPU_MODELS = [
    "GLM-4.5-Flash",
    "glm-4v-flash",
    "XX----ä¸‹é¢çš„è¦å¼€é€šæ”¯ä»˜-----XX",
    "glm-4.5-air",
    "glm-4.5",
    "glm-4.5-x",
    "glm-4.5-airx",
    "glm-4.5-flash",
    "glm-4-plus",
    "glm-z1-air",
    "glm-4v-plus-0111",
    "glm-4.1v-thinking-flash"
]

class AI_GLM4:
    @classmethod
    def INPUT_TYPES(cls):
        # ä» JSON åŠ è½½çš„ TEXT_PROMPTS ä¸­è·å–é”®ï¼Œä½œä¸ºé¢„è®¾é€‰é¡¹
        prompt_keys = list(TEXT_PROMPTS.keys())
        default_selection = prompt_keys[0] if prompt_keys else ""

        return {
            "required": {
                "text_input": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "placeholder": "æ­¤å¤„æ˜¯è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬è¾“å…¥ï¼Œå›¾ç‰‡åˆ†æç”¨ç³»ç»Ÿæç¤ºè¯è¾“å…¥"
                }),
                "prompt_preset": (prompt_keys, {"default": default_selection}),
                "prompt_override": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": " è¾“å…¥ç³»ç»Ÿæç¤ºè¯ï¼Œç•™ç©ºåˆ™ç”¨é¢„è®¾"
                }),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "å¯é€‰ï¼šæ™ºè°±AI API Key (ç•™ç©ºåˆ™å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–config.jsonè¯»å–)"
                }),
                "model_name": (ZHIPU_MODELS, {
                    "default": "glm-4v-flash",
                    "placeholder": "è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ glm-4v-flash "
                }),
            },
            "optional": {
                "image_input": ("IMAGE", {"optional": True}),  # ç§»é™¤ tooltip å¤‡æ³¨
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response",)
    FUNCTION = "execute"
    CATEGORY = "Apt_Preset/prompt"

    def execute(self, prompt_preset, prompt_override, api_key, model_name,
                max_tokens, seed, text_input, image_input=None):
        
        image_input_provided = image_input is not None
        
        if image_input_provided:
            return self._process_image(api_key, prompt_preset, prompt_override, model_name, seed,
                                     image_input, max_tokens)
        else:
            return self._process_text(text_input, api_key, prompt_preset, prompt_override, model_name,
                                    max_tokens, seed)

    def _process_text(self, text_input, api_key, prompt_preset, prompt_override, model_name,
                      max_tokens, seed):
        final_api_key = api_key.strip() or get_zhipuai_api_key()
        if not final_api_key:
            _log_error("API Key æœªæä¾›ã€‚")
            return ("API Key æœªæä¾›ã€‚",)

        _log_info("åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯ã€‚")

        try:
            client = ZhipuAI(api_key=final_api_key)
        except Exception as e:
            _log_error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return (f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}",)

        # ä» JSON åŠ è½½çš„ TEXT_PROMPTS ä¸­è·å–æç¤ºè¯ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
        final_system_prompt = ""
        if prompt_override and prompt_override.strip():
            final_system_prompt = prompt_override.strip()
            _log_info("ä½¿ç”¨ 'prompt_override'ã€‚")
        elif prompt_preset in TEXT_PROMPTS:
            final_system_prompt = TEXT_PROMPTS[prompt_preset]
            _log_info(f"ä½¿ç”¨é¢„è®¾æç¤ºè¯: '{prompt_preset}'ã€‚")
        else:
            # è‹¥é¢„è®¾ä¸å­˜åœ¨ï¼Œä½¿ç”¨å­—å…¸ç¬¬ä¸€ä¸ªå€¼ï¼ˆå…¼å®¹åŸæœ‰é€»è¾‘ï¼‰
            final_system_prompt = next(iter(TEXT_PROMPTS.values()), "") if TEXT_PROMPTS else ""
            _log_warning("é¢„è®¾æç¤ºè¯æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯ã€‚")

        if not final_system_prompt:
            _log_error("ç³»ç»Ÿæç¤ºè¯ä¸èƒ½ä¸ºç©ºã€‚")
            return ("ç³»ç»Ÿæç¤ºè¯ä¸èƒ½ä¸ºç©ºã€‚",)

        if not isinstance(final_system_prompt, str):
            final_system_prompt = str(final_system_prompt)

        messages = [
            {"role": "system", "content": final_system_prompt},
            {"role": "user", "content": text_input}
        ]

        effective_seed = seed if seed != 0 else random.randint(0, 0xffffffffffffffff)
        _log_info(f"å†…éƒ¨ç§å­: {effective_seed}ã€‚")
        random.seed(effective_seed)

        _log_info(f"è°ƒç”¨ GLM-4 ({model_name})...")

        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.9,
                top_p=0.7,
                max_tokens=max_tokens,
            )
            response_text = response.choices[0].message.content
            _log_info("GLM-4 å“åº”æˆåŠŸã€‚")
            return (response_text,)
        except Exception as e:
            error_message = f"GLM-4 API è°ƒç”¨å¤±è´¥: {e}"
            return (error_message,)

    def _process_image(self, api_key, prompt_preset, prompt_override, model_name, seed,
                    image_input=None, max_tokens=1024):
        final_api_key = api_key.strip() or get_zhipuai_api_key()
        if not final_api_key:
            _log_error("API Key æœªæä¾›ã€‚")
            return ("API Key æœªæä¾›ã€‚",)
        _log_info("åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯ã€‚")

        try:
            client = ZhipuAI(api_key=final_api_key)
        except Exception as e:
            _log_error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return (f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}",)

        if image_input is None:
            _log_error("å¿…é¡»æä¾›æœ‰æ•ˆçš„IMAGEå¯¹è±¡ã€‚")
            return ("å¿…é¡»æä¾›æœ‰æ•ˆçš„IMAGEå¯¹è±¡ã€‚",)

        try:
            i = 255. * image_input.cpu().numpy()
            img_array = np.clip(i, 0, 255).astype(np.uint8)[0]
            img = Image.fromarray(img_array)
            buffered = io.BytesIO()
            img.save(buffered, format="PNG")
            image_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            final_image_data = f"data:image/png;base64,{image_base64}"
            _log_info("IMAGEå¯¹è±¡æˆåŠŸè½¬æ¢ä¸ºBase64æ ¼å¼")
        except Exception as e:
            _log_error(f"å›¾ç‰‡æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
            return (f"å›¾ç‰‡æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}",)

        # ä» JSON åŠ è½½çš„ TEXT_PROMPTS ä¸­è·å–æç¤ºè¯ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
        final_prompt_text = ""
        if prompt_override and prompt_override.strip():
            final_prompt_text = prompt_override.strip()
            _log_info("ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯")
        elif prompt_preset in TEXT_PROMPTS:
            final_prompt_text = TEXT_PROMPTS[prompt_preset]
            _log_info(f"ä½¿ç”¨é¢„è®¾æç¤ºè¯: {prompt_preset}")
        else:
            final_prompt_text = next(iter(TEXT_PROMPTS.values()), "") if TEXT_PROMPTS else ""
            _log_warning("ä½¿ç”¨é»˜è®¤æç¤ºè¯")

        if not final_prompt_text:
            _log_error("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
            return ("æç¤ºè¯ä¸èƒ½ä¸ºç©º",)

        content_parts = [
            {"type": "text", "text": final_prompt_text},
            {"type": "image_url", "image_url": {"url": final_image_data}}
        ]

        effective_seed = seed if seed != 0 else random.randint(0, 0xffffffffffffffff)
        _log_info(f"ä½¿ç”¨å†…éƒ¨ç§å­: {effective_seed}")
        random.seed(effective_seed)

        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": content_parts}],
                temperature=0.9,
                top_p=0.7,
                max_tokens=max_tokens,
            )
            response_content = str(response.choices[0].message.content)
            _log_info("GLM-4Vå›¾ç‰‡è¯†åˆ«æˆåŠŸ")
            return (response_content,)
        except Exception as e:
            error_message = f"GLM-4V APIè°ƒç”¨å¤±è´¥: {str(e)}"
            _log_error(error_message)
            return (error_message,)



#endregion--------------------------------------------------------------------------



#region-----------qwen-image edit---

import requests
from io import BytesIO
import os
import io
import base64
import json
import folder_paths
import numpy as np
from PIL import Image


try:
    import dashscope
    REMOVER_AVAILABLE = True  
except ImportError:
    dashscope = None
    REMOVER_AVAILABLE = False  


current_dir = os.path.dirname(os.path.abspath(__file__))
json_path = os.path.join(current_dir, "AiPromptPreset.json")

def load_Image_Analysis():
    if not os.path.exists(json_path):
        return {"None": ""}
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("IMAGE_PROMPTS", {"None": ""})
    except Exception:
        return {"None": ""}

IMAGE_PROMPTS = load_Image_Analysis()

custom_nodes_paths = folder_paths.get_folder_paths("custom_nodes")
comfy_root = os.path.dirname(custom_nodes_paths[0]) if custom_nodes_paths else os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../.."))

key_path = os.path.join(comfy_root, "custom_nodes", "ComfyUI-Apt_Preset", "NodeExcel", "ApiKey_AI_Qwen.txt")

def get_aliyun_api_key():
    env_api_key = os.getenv("aliyun_API_KEY")
    if env_api_key and env_api_key.strip():
        return env_api_key.strip()
    if os.path.exists(key_path):
        try:
            with open(key_path, "r", encoding="utf-8") as f:
                api_key = f.read().strip()
                if api_key:
                    return api_key
        except:
            pass
    return ""

def encode_image(pil_image, save_tokens=True):
    buffered = io.BytesIO()
    if save_tokens:
        image = resize_to_limit(pil_image)
        image.save(buffered, format="JPEG", optimize=True, quality=75)
    else:
        pil_image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")

def resize_to_limit(img, max_pixels=262144):
    width, height = img.size
    total_pixels = width * height
    if total_pixels <= max_pixels:
        return img
    scale = (max_pixels / total_pixels) ** 0.5
    return img.resize((int(width * scale), int(height * scale)), Image.LANCZOS)

def tensor2pil(image):
    batch_count = image.size(0) if len(image.shape) > 3 else 1
    if batch_count > 1:
        return [tensor2pil(image[i])[0] for i in range(batch_count)]
    return [Image.fromarray(np.clip(255.0 * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))]

def analyze_images(images, model, api_key, final_prompt, max_tokens, seed=None, max_retries=3):
    if not api_key:
        raise ValueError("APIå¯†é’¥æœªé…ç½®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®ï¼š\n1. è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ aliyun_API_KEY\n2. åœ¨custom_nodes/ComfyUI-Apt_Preset/NodeExcelç›®å½•ä¸‹åˆ›å»ºApiKey_AI_Qwen.txtå¹¶å†™å…¥å¯†é’¥")
    
    img_count = len(images)
    user_text = f"è¯·æŒ‰è¦æ±‚åˆ†æä»¥ä¸‹{img_count}å¼ å›¾ç‰‡ï¼ˆæŒ‰è¾“å…¥é¡ºåºï¼‰ï¼š{final_prompt}"
    if seed is not None and seed != 0:
        user_text += f"\nã€å›ºå®šç§å­ï¼š{seed}ã€‘"
    
    user_content = []
    for img in images:
        user_content.append({"image": f"data:image/png;base64,{encode_image(img)}"})
    user_content.append({"text": user_text})
    
    messages = [{"role": "user", "content": user_content}]
    
    for retry in range(max_retries):
        try:
            call_params = {
                "api_key": api_key, 
                "model": model, 
                "messages": messages, 
                "result_format": "message",
                "max_tokens": max_tokens
            }
            if seed is not None and seed != 0:
                call_params["seed"] = seed
            
            response = dashscope.MultiModalConversation.call(**call_params)
            if response.status_code == 200:
                return response.output.choices[0].message.content[0]["text"].strip()
            else:
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.message}")
        except Exception as e:
            if "seed" in str(e).lower() and retry < max_retries - 1:
                call_params.pop("seed", None)
                response = dashscope.MultiModalConversation.call(**call_params)
                if response.status_code == 200:
                    return response.output.choices[0].message.content[0]["text"].strip()
            if retry == max_retries - 1:
                raise Exception(f"å¤šæ¬¡å°è¯•åå¤±è´¥: {str(e)}")
            

class AI_Qwen:
    def __init__(self):
        self.api_key = get_aliyun_api_key()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": (["qwen-vl-max", "qwen-vl-max-latest"], {"default": "qwen-vl-max-latest"}),
                "preset": (list(IMAGE_PROMPTS.keys()), {"default": "None"}),
                "text": ("STRING", {"default": "", "multiline": True}),
                "max_tokens": ("INT", {
                    "default": 1024,
                    "min": 10,
                    "max": 8192,
                    "step": 10,
                }),
            },
            "optional": {

                "image_1": ("IMAGE",),
                "image_2": ("IMAGE",),
                "image_3": ("IMAGE",),
                "seed": ("INT", {"default": 0, "min": 0, "max": 999999999, "step": 1}),
                "api_key_input": ("STRING", {"default": "", "multiline": False, }),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("result", "system_prompt")
    FUNCTION = "analyze"
    CATEGORY = "Apt_Preset/prompt"

    def analyze(self, model, preset, text, max_tokens, api_key_input="", image_1=None, image_2=None, image_3=None, seed=0):
        # è·å–presetå¯¹åº”çš„æ–‡æœ¬å†…å®¹ï¼ˆé”®åæ˜ å°„åˆ°å€¼ï¼‰
        preset_text = IMAGE_PROMPTS.get(preset, "") if preset != "None" else ""
        text_content = text.strip()
        
        # ç¡®å®šæœ€ç»ˆæç¤ºè¯
        if text_content:
            final_prompt = text_content
        else:
            final_prompt = preset_text if preset_text else "ç”Ÿæˆè¾“å…¥å›¾ç‰‡çš„è¯¦ç»†ä¸­æ–‡æè¿°"

        input_images = []
        if image_1 is not None:
            pil_img = tensor2pil(image_1)[0]
            if pil_img:
                input_images.append(pil_img)
        if image_2 is not None:
            pil_img = tensor2pil(image_2)[0]
            if pil_img:
                input_images.append(pil_img)
        if image_3 is not None:
            pil_img = tensor2pil(image_3)[0]
            if pil_img:
                input_images.append(pil_img)
        
        if not input_images:
            return ("é”™è¯¯ï¼šè¯·è‡³å°‘è¾“å…¥1å¼ æœ‰æ•ˆå›¾ç‰‡", preset_text)
        
        # ä½¿ç”¨ä¼ å…¥çš„API keyï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ–¹å¼è·å–
        api_key = api_key_input.strip() if api_key_input.strip() else self.api_key
        if not api_key:
            return ("APIå¯†é’¥æœªé…ç½®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®ï¼š\n1. åœ¨èŠ‚ç‚¹è¾“å…¥ä¸­ç›´æ¥å¡«å†™APIå¯†é’¥\n2. è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ aliyun_API_KEY\n3. æˆ–åœ¨custom_nodes/ComfyUI-Apt_Preset/NodeExcelç›®å½•ä¸‹åˆ›å»ºApiKey_AI_Qwen.txtå¹¶å†™å…¥å¯†é’¥", preset_text)
        
        try:
            result = analyze_images(
                input_images, 
                model, 
                api_key, 
                final_prompt, 
                max_tokens,
                seed if seed != 0 else None
            )
            return (result, preset_text)
        except Exception as e:
            return (f"åˆ†æå¤±è´¥: {str(e)}", preset_text)

#endregion--------------------------------------------------------------------------



#region-----------qwen-prompt-----------------

try:
    import dashscope
    from dashscope import Conversation
    REMOVER_AVAILABLE = True
except ImportError:
    dashscope = None
    Conversation = None
    REMOVER_AVAILABLE = False

def analyze_text(model, api_key, final_prompt, max_tokens, seed=None, max_retries=3):
    if not api_key:
        raise ValueError("APIå¯†é’¥æœªé…ç½®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®ï¼š\n1. è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ aliyun_API_KEY\n2. åœ¨custom_nodes/ComfyUI-Apt_Preset/NodeExcelç›®å½•ä¸‹åˆ›å»ºApiKey_AI_Qwen.txtå¹¶å†™å…¥å¯†é’¥")
    
    if not Conversation:
        raise ImportError("dashscope.Conversation å¯¼å…¥å¤±è´¥ï¼Œè¯·ç¡®ä¿dashscopeåº“ç‰ˆæœ¬æ­£ç¡®")
    
    user_text = final_prompt
    if seed is not None and seed != 0:
        user_text += f"\nã€å›ºå®šç§å­ï¼š{seed}ã€‘"
    
    messages = [{"role": "user", "content": user_text}]
    
    for retry in range(max_retries):
        try:
            conv = Conversation()
            call_params = {
                "api_key": api_key, 
                "model": model, 
                "messages": messages, 
                "result_format": "message",
                "max_tokens": max_tokens,
                "temperature": 0.7,
                "top_p": 0.8
            }
            if seed is not None and seed != 0:
                call_params["seed"] = seed
            
            response = conv.call(**call_params)
            if response.status_code == 200:
                return response.output.choices[0].message.content.strip()
            else:
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.message} (çŠ¶æ€ç ï¼š{response.status_code})")
        except Exception as e:
            if "seed" in str(e).lower() and retry < max_retries - 1:
                call_params.pop("seed", None)
                conv_retry = Conversation()
                response = conv_retry.call(**call_params)
                if response.status_code == 200:
                    return response.output.choices[0].message.content.strip()
            if retry == max_retries - 1:
                raise Exception(f"å¤šæ¬¡å°è¯•åå¤±è´¥: {str(e)}")

# ä¿®å¤ç³»ç»Ÿæç¤ºè¯åŠ è½½é€»è¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
json_path = os.path.join(current_dir, "AiPromptPreset.json")

def load_text_prompts():
    if not os.path.exists(json_path):
        return {"None": ""}
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("TEXT_PROMPTS", {"None": ""})
    except Exception:
        return {"None": ""}

TEXT_PROMPTS = load_text_prompts()



class AI_Qwen_text:
    def __init__(self):
        self.api_key = get_aliyun_api_key()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_model": (["qwen3-coder-plus", "qwen3-coder-plus-2025-09-23", "qwen3-coder-flash", 
                              "qwen3-coder-480b-a35b-instruct", "qwen3-coder-30b-a3b-instruct"], {
                    "default": "qwen3-coder-plus", }),

                "preset": (list(TEXT_PROMPTS.keys()), {"default": "None"}),

            },
            "optional": {
                "custom_system_prompt": ("STRING", {"default": "", "multiline": False, "description": "è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå¡«å†™åå°†æ›¿ä»£é¢„è®¾çš„preset"}),
                "text": ("STRING", {"default": "", "multiline": True}),
                "max_tokens": ("INT", {"default": 1024, "min": 10, "max": 8192,"step": 10, }),
                "seed": ("INT", {"default": 0, "min": 0, "max": 999999999, "step": 1}),
                "api_key_input": ("STRING", {"default": "", "multiline": False, }),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("result", "system_prompt")
    FUNCTION = "analyze"
    CATEGORY = "Apt_Preset/prompt"

    def analyze(self, llm_model, preset, text, max_tokens, custom_system_prompt="", api_key_input="", seed=0):
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œæ— å†…å®¹æ—¶ä½¿ç”¨é¢„è®¾
        if custom_system_prompt.strip():
            system_prompt = custom_system_prompt.strip()
        else:
            system_prompt = TEXT_PROMPTS.get(preset, "") if preset != "None" else ""
        
        text_content = text.strip()
        
        # æ„å»ºæœ€ç»ˆæç¤ºè¯
        if text_content and system_prompt:
            final_prompt = f"{system_prompt}\n\n{text_content}"
        elif text_content:
            final_prompt = text_content
        else:
            final_prompt = system_prompt if system_prompt else "è¯·æ ¹æ®éœ€æ±‚å®Œæˆç›¸å…³ä»£ç æˆ–æ–‡æœ¬ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€ä»£ç è§£é‡Šã€ç¼–ç¨‹é—®é¢˜è§£ç­”ç­‰ï¼‰"

        # æ ¡éªŒæœ€ç»ˆæç¤ºè¯
        if not final_prompt.strip():
            return ("é”™è¯¯ï¼šè¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬æˆ–é€‰æ‹©åˆé€‚çš„é¢„è®¾/å¡«å†™è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯", system_prompt)
        
        # æ£€æŸ¥ä¾èµ–
        if not REMOVER_AVAILABLE or not Conversation:
            return ("é”™è¯¯ï¼šdashscopeåº“å¯¼å…¥å¤±è´¥ï¼Œè¯·å®‰è£…æœ€æ–°ç‰ˆæœ¬dashscopeï¼ˆpip install -U dashscopeï¼‰", system_prompt)
        
        # å¤„ç†APIå¯†é’¥
        api_key = api_key_input.strip() if api_key_input.strip() else self.api_key
        if not api_key:
            return ("APIå¯†é’¥æœªé…ç½®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®ï¼š\n1. åœ¨èŠ‚ç‚¹è¾“å…¥ä¸­ç›´æ¥å¡«å†™APIå¯†é’¥\n2. è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡ aliyun_API_KEY\n3. æˆ–åœ¨custom_nodes/ComfyUI-Apt_Preset/NodeExcelç›®å½•ä¸‹åˆ›å»ºApiKey_AI_Qwen.txtå¹¶å†™å…¥å¯†é’¥", system_prompt)
        
        # è°ƒç”¨åˆ†æå‡½æ•°
        try:
            result = analyze_text(
                llm_model, 
                api_key, 
                final_prompt, 
                max_tokens,
                seed if seed != 0 else None
            )
            return (result, system_prompt)
        except Exception as e:
            return (f"å¤„ç†å¤±è´¥: {str(e)}", system_prompt)








#endregion--------------------------------------------------------------------------









#region-----------------ä¿å­˜æç¤ºè¯------------

import os
import json
import folder_paths

current_dir = os.path.dirname(os.path.abspath(__file__))
json_path = os.path.join(current_dir, "AiPromptPreset.json")

# ç¡®ä¿JSONæ–‡ä»¶å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤æ–‡ä»¶
def init_json_file():
    if not os.path.exists(json_path):
        default_data = {
            "TEXT_PROMPTS": {"None": ""},
            "IMAGE_PROMPTS": {"None": ""}
        }
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(default_data, f, ensure_ascii=False, indent=4)

init_json_file()

def load_prompt_dicts():
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)

def save_prompt_dicts(data):
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)




class AI_PresetSave:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "target_dict": (["TEXT_PROMPTS", "IMAGE_PROMPTS"], {"default": "TEXT_PROMPTS"}),
                "prompt_title": ("STRING", {"default": "", "placeholder": "è¾“å…¥æç¤ºè¯æ ‡é¢˜ï¼ˆä½œä¸ºå­—å…¸çš„é”®ï¼‰"}),
                "prompt_content": ("STRING", {"default": "", "multiline": True, "placeholder": "è¾“å…¥æç¤ºè¯å†…å®¹ï¼ˆä½œä¸ºå­—å…¸çš„å€¼ï¼‰"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("result",)
    FUNCTION = "append_prompt"
    CATEGORY = "Apt_Preset/prompt"
 
    DESCRIPTION = """
    TEXT_PROMPTS: æ–‡æœ¬æç¤ºè¯ï¼Œå¯¹åº”AiPromptPreset.jsonä¸­çš„TEXT_PROMPTSå­—å…¸
    IMAGE_PROMPTS: å›¾åƒæç¤ºè¯ï¼Œå¯¹åº”AiPromptPreset.jsonä¸­çš„IMAGE_PROMPTSå­—å…¸
   """ 


    def append_prompt(self, target_dict, prompt_title, prompt_content):
        prompt_title = prompt_title.strip()
        prompt_content = prompt_content.strip()
        if not prompt_title:
            return ("é”™è¯¯ï¼šæç¤ºè¯æ ‡é¢˜ä¸èƒ½ä¸ºç©º",)
        if not prompt_content:
            return ("é”™è¯¯ï¼šæç¤ºè¯å†…å®¹ä¸èƒ½ä¸ºç©º",)
        try:
            data = load_prompt_dicts()
        except Exception as e:
            return (f"é”™è¯¯ï¼šåŠ è½½JSONå¤±è´¥ - {str(e)}",)
        if target_dict not in data:
            return (f"é”™è¯¯ï¼šJSONä¸­ä¸å­˜åœ¨{target_dict}å­—å…¸",)
        if prompt_title in data[target_dict]:
            return (f"é”™è¯¯ï¼š{target_dict}ä¸­å·²å­˜åœ¨æ ‡é¢˜ã€Œ{prompt_title}ã€",)
        data[target_dict][prompt_title] = prompt_content
        try:
            save_prompt_dicts(data)
        except Exception as e:
            return (f"é”™è¯¯ï¼šä¿å­˜JSONå¤±è´¥ - {str(e)}",)
        return (f"æˆåŠŸï¼šå‘{target_dict}æ°¸ä¹…è¿½åŠ æç¤ºè¯\næ ‡é¢˜ï¼š{prompt_title}\nå†…å®¹ï¼š{prompt_content[:50]}...",)
    

#endregion--------------------------------------------------------------------------



#region-----------------ollamaæ¨¡å‹ç®¡ç†------------

import time
import base64
import json
import os
import subprocess
import threading
from io import BytesIO
from typing import Optional
import requests
import torch
import numpy as np
from PIL import Image
import comfy.sd
import comfy.utils

# ç²¾å‡†å®šä½ComfyUIæ ¹ç›®å½•ï¼Œæ‹¼æ¥æ¨¡å‹è·¯å¾„
try:
    comfy_module_path = os.path.dirname(os.path.abspath(comfy.__file__))
    COMFYUI_ROOT = os.path.abspath(os.path.join(comfy_module_path, ".."))
except:
    current_file_path = os.path.abspath(__file__)
    COMFYUI_ROOT = current_file_path
    for _ in range(5):
        COMFYUI_ROOT = os.path.dirname(COMFYUI_ROOT)
        if os.path.exists(os.path.join(COMFYUI_ROOT, "comfy")):
            break
OLLAMA_MODEL_PATH = os.path.join(COMFYUI_ROOT, "models", "ollama")
os.makedirs(OLLAMA_MODEL_PATH, exist_ok=True)
os.environ["OLLAMA_MODELS"] = OLLAMA_MODEL_PATH
current_dir = os.path.dirname(os.path.abspath(__file__))
json_path = os.path.join(current_dir, "AiPromptPreset.json")

def load_Image_Analysis():
    if not os.path.exists(json_path):
        return {"None": ""}
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        preset_data = data.get("IMAGE_PROMPTS", {})
        if "None" not in preset_data:
            preset_data["None"] = ""
        return preset_data
    except Exception:
        return {"None": ""}

def load_TEXT_PROMPTS():
    if not os.path.exists(json_path):
        return {"None": ""}
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        preset_data = data.get("TEXT_PROMPTS", {})
        if "None" not in preset_data:
            preset_data["None"] = ""
        return preset_data
    except Exception:
        return {"None": ""}

def resize_to_limit(img, max_pixels=262144):
    width, height = img.size
    total_pixels = width * height
    if total_pixels <= max_pixels:
        return img
    scale = (max_pixels / total_pixels) ** 0.5
    new_width = int(width * scale)
    new_height = int(height * scale)
    return img.resize((new_width, new_height), Image.LANCZOS)


IMAGE_PROMPTS = load_Image_Analysis()
TEXT_PROMPTS = load_TEXT_PROMPTS()
OLLMAMA_MODEL_NAME_IMAGE = ["qwen3-vl:latest",]
OLLMAMA_MODEL_NAME_TEXT = []



class AI_Ollama_image:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (OLLMAMA_MODEL_NAME_IMAGE, {"default": "qwen3-vl:latest"}),
                "preset": (list(IMAGE_PROMPTS.keys()), {"default": "None"}),
                "analysis_prompt": ("STRING", {"multiline": True, "default": ""}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 2048, "min": 1, "max": 8192}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 999999999, "step": 1}),
            },
            "optional": {
                "image_1": ("IMAGE",),
                "image_2": ("IMAGE",),
                "image_3": ("IMAGE",),
                "enable_ocr": ("BOOLEAN", {"default": False}),
            },
        }
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("analysis_result", "system_prompt")
    FUNCTION = "run_image_analysis"
    CATEGORY = "Apt_Preset/prompt"
    DESCRIPTION = "CMDè¿è¡Œå‘½ä»¤:  ollama run qwen3-vl:latest"
    def run_image_analysis(
        self,
        model_name,
        preset,
        analysis_prompt,
        temperature,
        max_tokens,
        seed=0,
        image_1=None,
        image_2=None,
        image_3=None,
        enable_ocr=False
    ):
        cleaned_analysis_prompt = analysis_prompt.strip()
        if cleaned_analysis_prompt:
            final_prompt = cleaned_analysis_prompt
        else:
            preset_prompt = IMAGE_PROMPTS.get(preset, "").strip()
            final_prompt = preset_prompt if preset_prompt else "è¯·åŸºäºè¾“å…¥å›¾ç‰‡ï¼Œè¯¦ç»†æè¿°å†…å®¹ã€åˆ†æç‰©ä½“/åœºæ™¯/ç»†èŠ‚ï¼Œè‹¥æœ‰å¤šå¼ å›¾éœ€å¯¹æ¯”å¼‚åŒ"
        img_base64_list = []
        img_inputs = [image_1, image_2, image_3]
        for idx, img_tensor in enumerate(img_inputs, 1):
            if img_tensor is not None:
                try:
                    if len(img_tensor.shape) == 4:
                        img_tensor = img_tensor.squeeze(0)
                    if img_tensor.dtype == torch.float32:
                        img_tensor = (img_tensor * 255).byte()
                    img_np = img_tensor.cpu().numpy().astype(np.uint8)
                    if img_np.shape[-1] == 4:
                        img_np = img_np[..., :3]
                    img_pil = Image.fromarray(img_np)
                    img_pil = resize_to_limit(img_pil)
                    img_buffer = BytesIO()
                    img_pil.save(img_buffer, format="JPEG", quality=95, optimize=True)
                    img_buffer.seek(0)
                    img_base64 = base64.b64encode(img_buffer.getvalue()).decode("utf-8").strip()
                    img_base64_list.append(img_base64)
                except Exception as e:
                    return (f"å›¾ç‰‡{idx}å¤„ç†/ç¼–ç é”™è¯¯ï¼š{str(e)}", final_prompt)
        if not img_base64_list:
            return ("é”™è¯¯ï¼šæœªè¾“å…¥ä»»ä½•å›¾ç‰‡ï¼Œè¯·è‡³å°‘é€‰æ‹©1å¼ å›¾ç‰‡ä¸Šä¼ ", final_prompt)
        if enable_ocr:
            img_count = len(img_base64_list)
            ocr_msg = f"\n\nç‰¹åˆ«æŒ‡ä»¤ï¼šè¯·æå–{img_count}å¼ å›¾ç‰‡ä¸­æ‰€æœ‰å¯è§æ–‡æœ¬å†…å®¹ï¼ŒæŒ‰å›¾ç‰‡åºå·ï¼ˆ1-{img_count}ï¼‰åˆ†ç±»ï¼Œä»¥åˆ—è¡¨å½¢å¼æ•´ç†æ–‡æœ¬å†…å®¹åŠæ–‡å­—ä½ç½®ä¿¡æ¯"
            final_prompt += ocr_msg
        ollama_host = "http://localhost:11434"
        try:
            data = {
                "model": model_name,
                "prompt": final_prompt,
                "images": img_base64_list,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True,
                "options": {
                    "num_ctx": max_tokens,
                    "num_thread": 4
                }
            }
            if seed != 0:
                data["seed"] = seed
            url = f"{ollama_host}/api/generate"
            headers = {"Content-Type": "application/json", "Accept": "application/json"}
            response = requests.post(
                url,
                headers=headers,
                data=json.dumps(data),
                stream=True,
                timeout=300
            )
            response.raise_for_status()
            analysis_result = ""
            for line in response.iter_lines():
                if line:
                    try:
                        line_data = json.loads(line.decode("utf-8"))
                        if "response" in line_data:
                            analysis_result += line_data["response"]
                        if line_data.get("error"):
                            return (f"æ¨¡å‹é”™è¯¯ï¼š{line_data['error']}", final_prompt)
                        if line_data.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
            analysis_result = analysis_result.strip()
            if not analysis_result:
                return ("é”™è¯¯ï¼šæ¨¡å‹æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åˆ†æ", final_prompt)
            return (analysis_result, final_prompt)
        except requests.exceptions.ConnectionError:
            error_msg = "è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·æ£€æŸ¥Ollamaæ˜¯å¦å·²å¯åŠ¨ã€æœåŠ¡åœ°å€æ­£ç¡®ã€ç«¯å£11434æœªè¢«å ç”¨"
            return (error_msg, final_prompt)
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                error_msg = f"æ¨¡å‹æœªæ‰¾åˆ°ï¼šè¯·å…ˆæ‰§è¡Œ `ollama pull {model_name}` ä¸‹è½½æ¨¡å‹åˆ°{OLLAMA_MODEL_PATH}ï¼ˆç¡®ä¿æ¨¡å‹æ”¯æŒè§†è§‰åˆ†æï¼Œå¦‚qwen3-vlã€llavaç­‰ï¼‰"
            elif e.response.status_code == 500:
                error_msg = f"OllamaæœåŠ¡å†…éƒ¨é”™è¯¯ï¼š1. è¯·æ›´æ–°Ollamaåˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆâ‰¥0.12.7ï¼‰ï¼›2. é‡æ–°æ‹‰å–æ¨¡å‹ `ollama pull {model_name}` åˆ°{OLLAMA_MODEL_PATH}ï¼›3. æ£€æŸ¥å›¾ç‰‡æ˜¯å¦æŸå"
            else:
                error_msg = f"HTTPé”™è¯¯ï¼š{str(e)}"
            return (error_msg, final_prompt)
        except requests.exceptions.Timeout:
            return ("é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢å¤§è¶…æ—¶æ—¶é—´", final_prompt)
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯ï¼š{str(e)}"
            return (error_msg, final_prompt)



class AI_Ollama_text:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                # ç§»é™¤ model_name åˆ—è¡¨ä¸­çš„ llama3:8b
                "model_name": (["qwen2.5-coder:7b","qwen3-coder:30b","qwen3-vl:latest",], {"default": "qwen3-vl:latest"}),
                "preset": (list(TEXT_PROMPTS.keys()), {"default": "None"}),

            },
            "optional": {
                "custom_system_prompt": ("STRING", {"default": "", "multiline": False, "description": "è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå¡«å†™åå°†æ›¿ä»£é¢„è®¾çš„preset"}),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 4096}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 999999999, "step": 1}),
            },
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("analysis_result", "system_prompt")

    FUNCTION = "run"
    CATEGORY = "Apt_Preset/prompt"
    
    def run(self, model_name, preset, prompt, temperature, max_tokens, seed=0, custom_system_prompt=""):
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œæ— å†…å®¹æ—¶ä½¿ç”¨é¢„è®¾
        if custom_system_prompt.strip():
            system_prompt = custom_system_prompt.strip()
        else:
            system_prompt = TEXT_PROMPTS.get(preset, "") if preset != "None" else ""
        
        ollama_host = "http://localhost:11434"
        
        try:
            url = f"{ollama_host}/api/generate"
            headers = {"Content-Type": "application/json"}
            data = {
                "model": model_name,
                "prompt": prompt,
                "system": system_prompt,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True
            }
            
            if seed != 0:
                data["seed"] = seed
            
            response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)
            response.raise_for_status()
            
            result = ""
            for line in response.iter_lines():
                if line:
                    line_data = json.loads(line.decode('utf-8'))
                    if 'response' in line_data:
                        result += line_data['response']
                    if line_data.get('done', False):
                        break
            
            return (result,system_prompt)
        
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                return (f"æ¨¡å‹æœªæ‰¾åˆ°ï¼šè¯·å…ˆæ‰§è¡Œ `ollama pull {model_name}` ä¸‹è½½æ¨¡å‹åˆ°{OLLAMA_MODEL_PATH}",)
            else:
                return (f"HTTPé”™è¯¯ï¼š{str(e)}",)
        except Exception as e:
            return (f"é”™è¯¯: {str(e)}",)







class Ai_Ollama_RunModel:
    def __init__(self):
        self.process: Optional[subprocess.Popen] = None
        self.is_running = False

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},  # æ— ä»»ä½•å¿…å¡«å‚æ•°ï¼Œç›´æ¥å¯åŠ¨
            "optional": {
                "timeout": ("INT", {
                    "default": 20,
                    "min": 5,
                    "max": 30,
                    "step": 1,
                })
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("status",)
    FUNCTION = "run_ollama_service"
    CATEGORY = "Apt_Preset/prompt"


    def run_ollama_service(self, timeout: int = 10):
        # æ­¥éª¤1ï¼šæ£€æŸ¥Ollamaæ˜¯å¦å®‰è£…
        try:
            subprocess.run(["ollama", "--version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)
        except FileNotFoundError:
            return ("é”™è¯¯ï¼šæœªæ‰¾åˆ°ollamaå‘½ä»¤ï¼\nè¯·ç¡®ä¿ï¼š1. å·²å®‰è£…Ollamaï¼ˆhttps://ollama.com/downloadï¼‰\n       2. Ollamaå·²æ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡",)

        # æ­¥éª¤2ï¼šåœæ­¢å·²å­˜åœ¨çš„OllamaæœåŠ¡ï¼ˆé¿å…ç«¯å£å ç”¨ï¼‰
        self._stop_existing_ollama()

        # æ­¥éª¤3ï¼šè½»é‡å¯åŠ¨Ollama APIæœåŠ¡
        return self._lightweight_start_service(timeout)

    def _stop_existing_ollama(self):
        """åœæ­¢å·²è¿è¡Œçš„Ollamaè¿›ç¨‹"""
        try:
            if os.name == "nt":  # Windows
                subprocess.run(["taskkill", "/f", "/im", "ollama.exe"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=3)
            else:  # Linux/macOS
                subprocess.run(["pkill", "ollama"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=3)
            time.sleep(1)
        except Exception:
            pass  # å¿½ç•¥åœæ­¢å¤±è´¥ï¼ˆå¯èƒ½åŸæœ¬å°±æ²¡æœ‰è¿è¡Œçš„æœåŠ¡ï¼‰

    def _lightweight_start_service(self, timeout: int) -> tuple:
        """çº¯è½»é‡å¯åŠ¨Ollama APIæœåŠ¡ï¼ˆä»…å¯åŠ¨æœåŠ¡ï¼Œä¸åŠ è½½ä»»ä½•æ¨¡å‹ï¼‰"""
        cmd = ["ollama", "serve"]
        try:
            # éšè—å¯åŠ¨çª—å£ï¼ˆWindowsï¼‰ï¼Œåå°è¿è¡Œï¼ˆLinux/macOSï¼‰
            creationflags = subprocess.CREATE_NO_WINDOW if os.name == "nt" else 0
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                creationflags=creationflags
            )

            # ç­‰å¾…æœåŠ¡å¯åŠ¨å¹¶éªŒè¯è¿é€šæ€§
            start_time = time.time()
            service_alive = False
            while time.time() - start_time < timeout:
                if self.process.poll() is not None:
                    # æœåŠ¡å¼‚å¸¸é€€å‡º
                    stderr_data = self.process.stdout.read() if self.process.stdout else ""
                    return (f"å¯åŠ¨å¤±è´¥ï¼šOllamaæœåŠ¡å¼‚å¸¸é€€å‡º\né”™è¯¯ä¿¡æ¯ï¼š{stderr_data}",)
                # æ£€æŸ¥APIæ˜¯å¦å¯è®¿é—®
                if self._check_service_connected():
                    service_alive = True
                    break
                time.sleep(0.5)

            if not service_alive:
                self.process.kill()
                return (f"å¯åŠ¨å¤±è´¥ï¼šOllamaæœåŠ¡å¯åŠ¨è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼ŒAPIæœªè¿é€š",)

            self.is_running = True
            # å¯åŠ¨æ—¥å¿—ç›‘å¬çº¿ç¨‹ï¼ˆä»…æ‰“å°å…³é”®æ—¥å¿—ï¼‰
            threading.Thread(target=self._print_service_log, args=(self.process,), daemon=True).start()

            # å¯åŠ¨æˆåŠŸä¿¡æ¯
            success_msg = (
                f"âœ… OllamaæœåŠ¡è½»é‡å¯åŠ¨æˆåŠŸï¼\n"
                f"ğŸ“ æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼š{OLLAMA_MODEL_PATH}\n"
                f"ğŸ†” æœåŠ¡PIDï¼š{self.process.pid}\n"
                f"ğŸŒ APIåœ°å€ï¼šhttp://localhost:11434\n"
                "ğŸ’¡ æç¤ºï¼š\n"
                "  1. æœåŠ¡å·²åœ¨åå°è¿è¡Œï¼Œå¯é€šè¿‡APIè°ƒç”¨ä»»æ„å·²ä¸‹è½½çš„æ¨¡å‹\n"
                "  2. åœæ­¢æœåŠ¡éœ€æ‰‹åŠ¨ç»“æŸPIDï¼ˆWindowsä»»åŠ¡ç®¡ç†å™¨/Linux/macOS pkill ollamaï¼‰\n"
                "  3. æ¨¡å‹éœ€æå‰ä¸‹è½½åˆ°ä¸Šè¿°è·¯å¾„ï¼ˆå‘½ä»¤ï¼šollama pull æ¨¡å‹åï¼‰"
            )
            return (success_msg,)

        except Exception as e:
            return (f"å¯åŠ¨å¤±è´¥ï¼š{str(e)}",)

    def _check_service_connected(self) -> bool:
        """æ£€æŸ¥Ollama APIæœåŠ¡æ˜¯å¦è¿é€š"""
        try:
            # è°ƒç”¨Ollamaçš„tagsæ¥å£ï¼ˆæ— æ¨¡å‹ä¾èµ–ï¼Œä»…éªŒè¯æœåŠ¡å­˜æ´»ï¼‰
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            return response.status_code == 200
        except Exception:
            return False

    def _print_service_log(self, process: subprocess.Popen):
        """è½»é‡æ—¥å¿—è¾“å‡ºï¼ˆä»…æ‰“å°é”™è¯¯å’Œå…³é”®ä¿¡æ¯ï¼‰"""
        while self.is_running and process.poll() is None:
            line = process.stdout.readline()
            if line:
                line = line.strip()
                # ä»…æ‰“å°åŒ…å«é”™è¯¯æˆ–å…³é”®çŠ¶æ€çš„æ—¥å¿—
                if "error" in line.lower() or "listening" in line.lower() or "started" in line.lower():
                    print(f"[OllamaæœåŠ¡æ—¥å¿—] {line}")
        # æœåŠ¡é€€å‡ºæ—¶æ‰“å°æ—¥å¿—
        if process.poll() is not None:
            print(f"[OllamaæœåŠ¡æ—¥å¿—] æœåŠ¡å·²é€€å‡ºï¼ˆé€€å‡ºç ï¼š{process.returncode}ï¼‰")
        self.is_running = False





#endregion--------------------------------------------------------------------------
















