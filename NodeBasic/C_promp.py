import os
import re
import numpy as np
from typing import Optional, List, Tuple
import os
import glob
import json
import toml 

import unicodedata
from typing import Dict, List
import datetime
import random

import unicodedata

try:
    import docx  # å¯¼å…¥æ•´ä¸ªdocxåº“ï¼ˆå¤‡ç”¨ï¼‰
    from docx import Document  
    from docx import Document as docx_Document  
    REMOVER_AVAILABLE = True  
except ImportError:
    docx = None
    Document = None
    docx_Document = None
    REMOVER_AVAILABLE = False


from ..main_unit import *




#region----------------------------------------------------------------------#




#region é•œå¤´è§†è§’
LENS_MAP = {
    "None": "",
    "å¹¿è§’é•œå¤´": "å¹¿è§’é•œå¤´è§†è§’ï¼ˆç­‰æ•ˆç„¦è·16-35mmï¼‰ï¼Œè§†é‡å¼€é˜”å®å¤§ï¼Œæ™¯æ·±æ·±é‚ƒï¼Œè¾¹ç¼˜ç•¸å˜è‡ªç„¶ï¼Œé€‚åˆå±•ç°å…¨æ™¯åœºæ™¯æˆ–ç©ºé—´çºµæ·±æ„Ÿ",
    "è¶…å¹¿è§’é•œå¤´": "è¶…å¹¿è§’é•œå¤´è§†è§’ï¼ˆç­‰æ•ˆç„¦è·8-15mmï¼‰ï¼Œè§†é‡æåº¦å®½å¹¿ï¼Œç©ºé—´æ‹‰ä¼¸æ„Ÿå¼ºï¼Œè¿‘å¤§è¿œå°æ•ˆæœæ˜æ˜¾ï¼Œé€‚åˆç‹­å°ç©ºé—´æˆ–éœ‡æ’¼å…¨æ™¯",
    "ä¿¯è§†é•œå¤´": "é«˜ç©ºä¿¯è§†è§’åº¦æ‹æ‘„ï¼Œè‡ªä¸Šè€Œä¸‹å‚ç›´/æ–œå‘è§†è§’ï¼Œå®Œæ•´å±•ç°ä¸»ä½“æ•´ä½“å¸ƒå±€ä¸ç¯å¢ƒå…³ç³»ï¼Œå…¨å±€è§†è§’æ¸…æ™°",
    "ä»°è§†é•œå¤´": "ä½è§’åº¦ä»°è§†æ‹æ‘„ï¼Œè‡ªä¸‹è€Œä¸Šä»°æœ›è§†è§’ï¼Œçªå‡ºä¸»ä½“é«˜è€¸æ„Ÿä¸å‹è¿«æ„Ÿï¼Œå¼ºåŒ–å‚ç›´ç»´åº¦è§†è§‰å†²å‡»",
    "ç‰¹å†™é•œå¤´": "ç‰¹å†™é•œå¤´èšç„¦ä¸»ä½“å±€éƒ¨ï¼ˆå¦‚é¢éƒ¨ã€ç»†èŠ‚ï¼‰ï¼Œç»†èŠ‚æ”¾å¤§çªå‡ºï¼Œä¸»ä½“å æ®ç”»é¢80%ä»¥ä¸Šï¼ŒèƒŒæ™¯è½»å¾®è™šåŒ–ï¼Œå‡¸æ˜¾çº¹ç†ä¸è´¨æ„Ÿ",
    "å¤§ç‰¹å†™é•œå¤´": "å¤§ç‰¹å†™é•œå¤´æè‡´èšç„¦å¾®å°ç»†èŠ‚ï¼ˆå¦‚çœ¼ç›ã€çº¹ç†ï¼‰ï¼Œä¸»ä½“å æ®ç”»é¢90%ä»¥ä¸Šï¼Œç»†èŠ‚çº¤æ¯«æ¯•ç°ï¼ŒèƒŒæ™¯å®Œå…¨è™šåŒ–",
    "å¾®è·é•œå¤´": "è¶…è¿‘è·ç¦»å¾®è·æ‘„å½±ï¼ˆæ”¾å¤§å€ç‡1:1ä»¥ä¸Šï¼‰ï¼Œæè‡´æ”¾å¤§å¾®è§‚ç»†èŠ‚ï¼Œçº¹ç†æ¸…æ™°é”åˆ©ï¼Œè‰²å½©è¿˜åŸçœŸå®ï¼Œçªå‡ºæè´¨è‚Œç†ä¸å¾®å°ç»“æ„",
    "è¿‘æ™¯é•œå¤´": "è¿‘æ™¯æ‹æ‘„èšç„¦ä¸»ä½“ä¸ŠåŠèº«/æ ¸å¿ƒåŒºåŸŸï¼Œä¸»ä½“çªå‡ºé²œæ˜ï¼ŒèƒŒæ™¯é€‚åº¦è™šåŒ–ï¼ˆæµ…æ™¯æ·±ï¼‰ï¼Œå…¼é¡¾ä¸»ä½“ç»†èŠ‚ä¸ç¯å¢ƒæ°›å›´",
    "ä¸­æ™¯é•œå¤´": "ä¸­æ™¯æ‹æ‘„å±•ç°ä¸»ä½“å®Œæ•´å½¢æ€ä¸å‘¨è¾¹ç¯å¢ƒï¼Œä¸»ä½“ä¸èƒŒæ™¯æ¯”ä¾‹åè°ƒï¼Œæ—¢èƒ½çœ‹æ¸…ä¸»ä½“åŠ¨ä½œï¼Œåˆèƒ½ä½“ç°ç¯å¢ƒå…³ç³»",
    "è¿œæ™¯é•œå¤´": "è¿œæ™¯å…¨æ™¯æ‹æ‘„ï¼Œä¸»ä½“ä¸èƒŒæ™¯åè°ƒç»Ÿä¸€ï¼Œå±•ç°å®Œæ•´åœºæ™¯æ ¼å±€ï¼Œç©ºé—´å…³ç³»æ˜ç¡®ï¼Œæ°›å›´æ„Ÿå¼ºçƒˆ",
    "å…¨æ™¯é•œå¤´": "å…¨æ™¯é•œå¤´360åº¦/å®½å¹…è¦†ç›–ï¼Œåœºæ™¯å®Œæ•´æ— é—æ¼ï¼Œç©ºé—´çºµæ·±æ„Ÿä¸å¹¿åº¦å…¼å…·ï¼Œé€‚åˆå®å¤§åœºæ™¯å±•ç°"
}

VIEW_MAP = {
    "None": "",
    "å®Œæ•´å››è§†å›¾": "å·¥ç¨‹åˆ¶å›¾æ ‡å‡†å››è§†å›¾æ­£äº¤æŠ•å½±ï¼ŒåŒ…å«å‰è§†å›¾ã€ä¾§è§†å›¾ã€åè§†å›¾ã€é¡¶è§†å›¾ï¼Œæ¯”ä¾‹ç²¾ç¡®ï¼Œçº¿æ¡æ¸…æ™°æ— ç•¸å˜ï¼Œå°ºå¯¸æ ‡æ³¨è§„èŒƒ",
    "å®Œæ•´å…­è§†å›¾": "å·¥ç¨‹åˆ¶å›¾æ ‡å‡†å…­è§†å›¾æ­£äº¤æŠ•å½±ï¼ŒåŒ…å«å‰/å/å·¦/å³/é¡¶/åº•è§†å›¾ï¼Œå…¨æ–¹ä½æ— æ­»è§’å±•ç¤ºï¼Œæœºæ¢°è®¾è®¡æ ‡å‡†è§„èŒƒ",
    "æ­£é¢è§†å›¾": "æ­£å°„æŠ•å½±æ­£é¢è§†å›¾ï¼Œä¸»ä½“æ­£é¢å®Œæ•´å¯¹ç§°å±•ç°ï¼Œä¸­å¿ƒæ„å›¾å‡è¡¡ï¼Œç»“æ„ç»†èŠ‚æ— é®æŒ¡ï¼Œè½®å»“çº¿æ¡è§„æ•´",
    "ä¾§é¢è§†å›¾": "æ­£å°„æŠ•å½±ä¾§é¢è§†å›¾ï¼Œä¸»ä½“ä¾§é¢è½®å»“æ¸…æ™°åˆ†æ˜ï¼Œæ·±åº¦ç»´åº¦ä¸åšåº¦å…³ç³»æ˜ç¡®ï¼Œä¾§è§†è§’åº¦æ— é€è§†å˜å½¢",
    "èƒŒé¢è§†å›¾": "æ­£å°„æŠ•å½±èƒŒé¢è§†å›¾ï¼Œä¸»ä½“èƒŒéƒ¨ç»“æ„å®Œæ•´å‘ˆç°ï¼Œåéƒ¨ç»†èŠ‚æ— é—æ¼ï¼Œè½®å»“ä¸æ¥å£å…³ç³»æ¸…æ™°",
    "é¡¶éƒ¨è§†å›¾": "æ­£å°„æŠ•å½±é¡¶éƒ¨è§†å›¾ï¼Œä¸»ä½“ä¿¯è§†ç»“æ„å®Œæ•´å±•ç°ï¼Œé¡¶éƒ¨å¸ƒå±€ä¸å°ºå¯¸å…³ç³»æ˜ç¡®ï¼Œæ— é®æŒ¡è§†è§’",
    "åº•éƒ¨è§†å›¾": "æ­£å°„æŠ•å½±åº•éƒ¨è§†å›¾ï¼Œä¸»ä½“ä»°è§†ç»“æ„å®Œæ•´å±•ç°ï¼Œåº•éƒ¨ç»†èŠ‚ä¸æ¥å£å…³ç³»æ¸…æ™°ï¼Œè¡¥å……é¡¶éƒ¨è§†è§’ç›²åŒº",
    "åŠä¾§é¢è§†å›¾": "45åº¦åŠä¾§æ­£äº¤è§†å›¾ï¼Œç«‹ä½“æ„Ÿä¸ç©ºé—´æ„Ÿå…¼å…·ï¼Œå‰åå±‚æ¬¡å…³ç³»æ˜ç¡®ï¼Œé€è§†è‡ªç„¶æ— ç•¸å˜ï¼Œå…¼é¡¾æ­£é¢ä¸ä¾§é¢ç»†èŠ‚",
    "30åº¦ä¾§è§†å›¾": "30åº¦ä¾§è§†æ­£äº¤è§†å›¾ï¼Œä¾§é¢ç»†èŠ‚æ›´çªå‡ºï¼Œç©ºé—´å…³ç³»æ¯”45åº¦æ›´èšç„¦ï¼Œé€‚åˆå±•ç¤ºå•ä¾§ç»“æ„"
}

MOVE_CMD = {
    "å‘å‰å¹³ç§»": "é•œå¤´ç¼“æ…¢å‘å‰å¹³ç§»ï¼Œä¸»ä½“é€æ¸æ”¾å¤§ï¼Œç”»é¢çºµæ·±æ„Ÿå¢å¼ºï¼Œå‰æ™¯ç»†èŠ‚æ¸…æ™°åŒ–",
    "å‘åå¹³ç§»": "é•œå¤´ç¼“æ…¢å‘åå¹³ç§»ï¼Œä¸»ä½“é€æ¸ç¼©å°ï¼Œåœºæ™¯èŒƒå›´æ‰©å¤§ï¼ŒèƒŒæ™¯å…ƒç´ æ›´å¤šçº³å…¥ç”»é¢",
    "å‘å·¦å¹³ç§»": "é•œå¤´å¹³ç¨³å‘å·¦å¹³ç§»ï¼Œä¸»ä½“ä½ç½®å³ç§»ï¼Œå±•ç°å·¦ä¾§ç¯å¢ƒå»¶ä¼¸ï¼Œæ„å›¾å¹³è¡¡è°ƒæ•´",
    "å‘å³å¹³ç§»": "é•œå¤´å¹³ç¨³å‘å³å¹³ç§»ï¼Œä¸»ä½“ä½ç½®å·¦ç§»ï¼Œå±•ç°å³ä¾§ç¯å¢ƒå»¶ä¼¸ï¼Œæ„å›¾é‡å¿ƒåç§»",
    "å‘ä¸Šå¹³ç§»": "é•œå¤´ç¼“æ…¢å‘ä¸Šå¹³ç§»ï¼Œè§†è§’å‡é«˜ï¼Œçªå‡ºä¸»ä½“ä¸‹éƒ¨ç»†èŠ‚ä¸ä¸Šæ–¹ç¯å¢ƒè¡”æ¥",
    "å‘ä¸‹å¹³ç§»": "é•œå¤´ç¼“æ…¢å‘ä¸‹å¹³ç§»ï¼Œè§†è§’è½»å¾®é™ä½ï¼Œçªå‡ºä¸»ä½“ä¸Šéƒ¨ç»†èŠ‚ä¸ä¸‹æ–¹ç¯å¢ƒè¡”æ¥",
    "å‘å·¦ä¸Šæ–¹å¹³ç§»": "é•œå¤´å‘å·¦ä¸Šæ–¹æ–œå‘å¹³ç§»ï¼Œè§†è§’åŒæ—¶å·¦ç§»å‡é«˜ï¼Œå±•ç°å·¦ä¸Šæ–¹åœºæ™¯å»¶ä¼¸",
    "å‘å³ä¸Šæ–¹å¹³ç§»": "é•œå¤´å‘å³ä¸Šæ–¹æ–œå‘å¹³ç§»ï¼Œè§†è§’åŒæ—¶å³ç§»å‡é«˜ï¼Œå±•ç°å³ä¸Šæ–¹åœºæ™¯å»¶ä¼¸",
    "å‘å·¦ä¸‹æ–¹å¹³ç§»": "é•œå¤´å‘å·¦ä¸‹æ–¹æ–œå‘å¹³ç§»ï¼Œè§†è§’åŒæ—¶å·¦ç§»é™ä½ï¼Œå±•ç°å·¦ä¸‹æ–¹åœºæ™¯ç»†èŠ‚",
    "å‘å³ä¸‹æ–¹å¹³ç§»": "é•œå¤´å‘å³ä¸‹æ–¹æ–œå‘å¹³ç§»ï¼Œè§†è§’åŒæ—¶å³ç§»é™ä½ï¼Œå±•ç°å³ä¸‹æ–¹åœºæ™¯ç»†èŠ‚"
}

ANGLE_CMD = {
    "æ°´å¹³å‘å·¦è½¬åŠ¨": "é•œå¤´å‘å·¦æ°´å¹³è½¬åŠ¨{}åº¦ï¼Œè§†è§’æ¨ªå‘æ‰©å±•ï¼Œå·¦ä¾§åœºæ™¯çº³å…¥ç”»é¢ï¼Œæ„å›¾å·¦ä¾§å¡«å……",
    "æ°´å¹³å‘å³è½¬åŠ¨": "é•œå¤´å‘å³æ°´å¹³è½¬åŠ¨{}åº¦ï¼Œè§†è§’æ¨ªå‘æ‰©å±•ï¼Œå³ä¾§åœºæ™¯çº³å…¥ç”»é¢ï¼Œæ„å›¾å³ä¾§å¡«å……",
    "å‘å·¦å€¾æ–œæ—‹è½¬": "é•œå¤´å‘å·¦æ—‹è½¬{}åº¦ï¼Œä¸»ä½“å‘ˆç°å·¦ä¾§å€¾æ–œè§†è§’ï¼Œå¢å¼ºåŠ¨æ€å¼ åŠ›ï¼Œè§†è§‰é‡å¿ƒå·¦ç§»",
    "å‘å³å€¾æ–œæ—‹è½¬": "é•œå¤´å‘å³æ—‹è½¬{}åº¦ï¼Œä¸»ä½“å‘ˆç°å³ä¾§å€¾æ–œè§†è§’ï¼Œå¢å¼ºåŠ¨æ€å¼ åŠ›ï¼Œè§†è§‰é‡å¿ƒå³ç§»",
    "å‘ä¸‹ä¿¯è§†": "é•œå¤´å‘ä¸‹ä¿¯è§†{}åº¦ï¼Œè§†è§’é™ä½ï¼Œçªå‡ºä¸»ä½“é¡¶éƒ¨ç»“æ„ä¸åœ°é¢/æ¡Œé¢ç¯å¢ƒçš„ä½ç½®å…³ç³»",
    "å‘ä¸Šä»°è§†": "é•œå¤´å‘ä¸Šä»°è§†{}åº¦ï¼Œè§†è§’å‡é«˜ï¼Œçªå‡ºä¸»ä½“åº•éƒ¨ç»“æ„ä¸å¤©ç©º/ä¸Šæ–¹ç¯å¢ƒçš„ä½ç½®å…³ç³»",
    "å‘å‰å€¾æ–œæ—‹è½¬": "é•œå¤´å‘å‰æ—‹è½¬{}åº¦ï¼Œè§†è§’å‰å€¾ï¼Œå¢å¼ºç”»é¢å‹è¿«æ„Ÿï¼Œä¸»ä½“è¿‘å¤§è¿œå°æ•ˆæœå¼ºåŒ–",
    "å‘åå€¾æ–œæ—‹è½¬": "é•œå¤´å‘åæ—‹è½¬{}åº¦ï¼Œè§†è§’åä»°ï¼Œå±•ç°ä¸»ä½“ä¸Šéƒ¨ä¸å¤©ç©º/ä¸Šæ–¹ç¯å¢ƒï¼Œç”»é¢å¼€é˜”åº¦æå‡",
    "é¡ºæ—¶é’ˆæ—‹è½¬": "é•œå¤´é¡ºæ—¶é’ˆæ—‹è½¬{}åº¦ï¼Œç”»é¢å‘ˆç°æ—‹è½¬åŠ¨æ€æ•ˆæœï¼Œå¢å¼ºåŠ¨æ„Ÿä¸è§†è§‰å†²å‡»",
    "é€†æ—¶é’ˆæ—‹è½¬": "é•œå¤´é€†æ—¶é’ˆæ—‹è½¬{}åº¦ï¼Œç”»é¢å‘ˆç°åå‘æ—‹è½¬åŠ¨æ€æ•ˆæœï¼Œè¥é€ ç‹¬ç‰¹è§†è§‰ä½“éªŒ"
}





class excel_Qwen_camera:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {

                "é•œå¤´å¹³ç§»æ–¹å‘": (
                    [
                        "None", 
                        "å‘å‰å¹³ç§»", "å‘åå¹³ç§»", 
                        "å‘å·¦å¹³ç§»", "å‘å³å¹³ç§»", 
                        "å‘ä¸Šå¹³ç§»", "å‘ä¸‹å¹³ç§»",
                        "å‘å·¦ä¸Šæ–¹å¹³ç§»", "å‘å³ä¸Šæ–¹å¹³ç§»",
                        "å‘å·¦ä¸‹æ–¹å¹³ç§»", "å‘å³ä¸‹æ–¹å¹³ç§»"
                    ],
                    {"default": "None", "label": "é•œå¤´å¹³ç§»ï¼ˆNone=ä¸å¯ç”¨ï¼‰"}
                ),
                
                "è°ƒæ•´è§’åº¦": (
                    [
                        "None",
                        "æ°´å¹³å‘å·¦è½¬åŠ¨", "æ°´å¹³å‘å³è½¬åŠ¨",
                        "å‘å·¦å€¾æ–œæ—‹è½¬", "å‘å³å€¾æ–œæ—‹è½¬",
                        "å‘ä¸‹ä¿¯è§†", "å‘ä¸Šä»°è§†",
                        "å‘å‰å€¾æ–œæ—‹è½¬", "å‘åå€¾æ–œæ—‹è½¬",
                        "é¡ºæ—¶é’ˆæ—‹è½¬", "é€†æ—¶é’ˆæ—‹è½¬"
                    ],
                    {"default": "None", "label": "è§’åº¦è°ƒæ•´ç±»å‹ï¼ˆNone=ä¸å¯ç”¨ï¼‰"}
                ),
                "è§’åº¦æ•°å€¼": ("INT", {
                    "default": 0, 
                    "min": 0, 
                    "max": 180, 
                    "step": 5, 
                    "display": "slider",
                }),
                
                "é•œå¤´ç±»å‹": ([
                    "None", 
                    "å¹¿è§’é•œå¤´", "è¶…å¹¿è§’é•œå¤´",
                    "ä¿¯è§†é•œå¤´", "ä»°è§†é•œå¤´",
                    "ç‰¹å†™é•œå¤´", "å¤§ç‰¹å†™é•œå¤´",
                    "å¾®è·é•œå¤´",
                    "è¿‘æ™¯é•œå¤´", "ä¸­æ™¯é•œå¤´", "è¿œæ™¯é•œå¤´", "å…¨æ™¯é•œå¤´"
                ], {"default": "None", "label": "ä¸“ä¸šé•œå¤´é€‰æ‹©ï¼ˆNone=ä¸å¯ç”¨ï¼‰"}),
                
                "è§†å›¾ç±»å‹": ([
                    "None", 
                    "å®Œæ•´å››è§†å›¾", "å®Œæ•´å…­è§†å›¾",
                    "æ­£é¢è§†å›¾", "ä¾§é¢è§†å›¾", "èƒŒé¢è§†å›¾",
                    "é¡¶éƒ¨è§†å›¾", "åº•éƒ¨è§†å›¾",
                    "åŠä¾§é¢è§†å›¾ï¼ˆ45åº¦ï¼‰", "30åº¦ä¾§è§†å›¾"
                ], {"default": "None", "label": "æ­£äº¤è§†å›¾é€‰æ‹©ï¼ˆNone=ä¸å¯ç”¨ï¼‰"}),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("æç¤ºè¯",)
    FUNCTION = "generate_prompt"
    CATEGORY = "Apt_Preset/prompt"

    def generate_prompt(self, é•œå¤´å¹³ç§»æ–¹å‘, è°ƒæ•´è§’åº¦, è§’åº¦æ•°å€¼, 
                       é•œå¤´ç±»å‹, è§†å›¾ç±»å‹):
        prompt_parts = []
        
        # å¤„ç†é•œå¤´å¹³ç§»ï¼šç›´æ¥ç”¨ä¸‹æ‹‰é€‰é¡¹ä½œä¸ºMOVE_CMDçš„é”®ï¼ˆæ— éœ€é¢å¤–æ˜ å°„ï¼‰
        if é•œå¤´å¹³ç§»æ–¹å‘ != "None":
            prompt_parts.append(MOVE_CMD.get(é•œå¤´å¹³ç§»æ–¹å‘, ""))
        
        # å¤„ç†è§’åº¦è°ƒæ•´ï¼šç›´æ¥ç”¨ä¸‹æ‹‰é€‰é¡¹ä½œä¸ºANGLE_CMDçš„é”®
        if è°ƒæ•´è§’åº¦ != "None" and è§’åº¦æ•°å€¼ > 0:
            prompt_parts.append(ANGLE_CMD.get(è°ƒæ•´è§’åº¦, "").format(è§’åº¦æ•°å€¼))
        
        # å¤„ç†ä¸“ä¸šé•œå¤´
        if é•œå¤´ç±»å‹ != "None":
            prompt_parts.append(LENS_MAP.get(é•œå¤´ç±»å‹, ""))
        
        # å¤„ç†æ­£äº¤è§†å›¾
        view_key = è§†å›¾ç±»å‹.replace("ï¼ˆ45åº¦ï¼‰", "").replace("30åº¦", "30åº¦")
        if è§†å›¾ç±»å‹ != "None":
            prompt_parts.append(VIEW_MAP.get(view_key, ""))
        
        # è¿‡æ»¤ç©ºå€¼å¹¶ä¼˜åŒ–æç¤ºè¯æµç•…åº¦
        valid_parts = list(filter(None, prompt_parts))
        if valid_parts:
            if len(valid_parts) == 1:
                final_prompt = valid_parts[0] + "ï¼Œç”»é¢æ„å›¾åè°ƒï¼Œè§†è§‰æ•ˆæœè‡ªç„¶"
            elif len(valid_parts) == 2:
                final_prompt = f"{valid_parts[0]}ï¼ŒåŒæ—¶{valid_parts[1]}ï¼Œæ•´ä½“ç”»é¢ç»Ÿä¸€å’Œè°"
            else:
                final_prompt = "ï¼Œ".join(valid_parts[:-1]) + f"ï¼Œå¹¶{valid_parts[-1]}ï¼Œç”»é¢å±‚æ¬¡ä¸°å¯Œä¸”åè°ƒ"
            final_prompt += "ï¼Œå…‰å½±è¿‡æ¸¡è‡ªç„¶ï¼Œç»†èŠ‚æ¸…æ™°å¯è¾¨"
        else:
            final_prompt = "æ ‡å‡†é•œå¤´è§†è§’ï¼ˆç­‰æ•ˆç„¦è·50mmï¼‰ï¼Œè§†è§’è‡ªç„¶æ— ç•¸å˜ï¼Œä¸»ä½“å±…ä¸­æ„å›¾ï¼Œæ™¯æ·±é€‚ä¸­ï¼Œç»†èŠ‚ä¸ç¯å¢ƒå…¼é¡¾ï¼Œå…‰å½±åè°ƒ"
        
        return (final_prompt + "ã€‚",)







#endregion--------------------------------------



#region-----------------é”™å¼€-------------


import os
import json
import numpy as np
import torch
import folder_paths
from PIL import Image, ImageDraw



class Coordinate_loadImage:
    @classmethod
    def INPUT_TYPES(cls):
        input_dir = folder_paths.get_input_directory()
        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.bmp', '.gif'))]
        return {
            "required": {
                "image": (sorted(files), {"image_upload": True}),
                "points_data": ("STRING", {"default": "[]", "multiline": False}),
            },
        }
    
    NAME = "Coordinate_loadImage"
    RETURN_TYPES = ("IMAGE","IMAGE", "STRING",)
    RETURN_NAMES = ("ORIGINAL_IMAGE","img_tensor", "coordinates")
    FUNCTION = "process"
    CATEGORY = "Apt_Preset/image/ImageCoordinate"
    OUTPUT_NODE = False
    
    def process(self, image, points_data="[]"):
        image_path = folder_paths.get_annotated_filepath(image)
        pil_image = Image.open(image_path).convert("RGB")
        original_image = pil_image.copy()
        img_width, img_height = pil_image.size
        
        try:
            points = json.loads(points_data)
        except json.JSONDecodeError:
            points = []
        
        if points and len(points) > 0:
            pil_image = self._draw_markers(pil_image, points, img_width, img_height)
        
        # å¤„ç†å¸¦æ ‡è®°çš„å›¾ç‰‡
        img_array = np.array(pil_image).astype(np.float32) / 255.0
        img_tensor = torch.from_numpy(img_array).unsqueeze(0)
        
        # å¤„ç†åŸå›¾
        original_array = np.array(original_image).astype(np.float32) / 255.0
        original_tensor = torch.from_numpy(original_array).unsqueeze(0)
        
        return (original_tensor,img_tensor,points_data)
    
    def _draw_markers(self, image, points, img_width, img_height):
        marker_color = (255, 69, 0, 230)
        text_rgb = (255, 255, 255)
        
        if image.mode != 'RGBA':
            result_image = image.convert('RGBA')
        else:
            result_image = image.copy()
            
        overlay = Image.new('RGBA', result_image.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        
        marker_radius = max(14, min(img_width, img_height) // 40)
        outline_width = max(2, marker_radius // 10)
        outline_color = (255, 255, 255, 230)
        
        for point in points:
            rel_x = point.get("x", 0)
            rel_y = point.get("y", 0)
            index = point.get("index", 1)
            abs_x = int(rel_x * img_width)
            abs_y = int(rel_y * img_height)
            
            draw.ellipse(
                [
                    abs_x - marker_radius - outline_width,
                    abs_y - marker_radius - outline_width,
                    abs_x + marker_radius + outline_width,
                    abs_y + marker_radius + outline_width
                ],
                fill=outline_color
            )
            
            draw.ellipse(
                [
                    abs_x - marker_radius,
                    abs_y - marker_radius,
                    abs_x + marker_radius,
                    abs_y + marker_radius
                ],
                fill=marker_color
            )
            
            self._draw_number_geometric(draw, abs_x, abs_y, index, marker_radius, text_rgb)
        
        result_image = Image.alpha_composite(result_image, overlay)
        return result_image.convert('RGB')
    
    def _draw_number_geometric(self, draw, cx, cy, number, radius, color):
        num_str = str(number)
        digit_height = int(radius * 1.0)
        digit_width = int(radius * 0.55)
        spacing = int(radius * 0.15)
        line_width = max(2, radius // 5)
        total_width = len(num_str) * digit_width + (len(num_str) - 1) * spacing
        start_x = cx - total_width // 2
        
        for i, digit_char in enumerate(num_str):
            digit = int(digit_char)
            digit_cx = start_x + i * (digit_width + spacing) + digit_width // 2
            self._draw_single_digit(draw, digit_cx, cy, digit, digit_width, digit_height, line_width, color)
    
    def _draw_single_digit(self, draw, cx, cy, digit, width, height, line_width, color):
        hw = width // 2
        hh = height // 2
        gap = line_width // 2
        
        segments = {
            'a': [(cx - hw + gap, cy - hh), (cx + hw - gap, cy - hh)],
            'b': [(cx + hw, cy - hh + gap), (cx + hw, cy - gap)],
            'c': [(cx + hw, cy + gap), (cx + hw, cy + hh - gap)],
            'd': [(cx - hw + gap, cy + hh), (cx + hw - gap, cy + hh)],
            'e': [(cx - hw, cy + gap), (cx - hw, cy + hh - gap)],
            'f': [(cx - hw, cy - hh + gap), (cx - hw, cy - gap)],
            'g': [(cx - hw + gap, cy), (cx + hw - gap, cy)],
        }
        
        digit_segments = {
            0: ['a', 'b', 'c', 'd', 'e', 'f'],
            1: ['b', 'c'],
            2: ['a', 'b', 'g', 'e', 'd'],
            3: ['a', 'b', 'g', 'c', 'd'],
            4: ['f', 'g', 'b', 'c'],
            5: ['a', 'f', 'g', 'c', 'd'],
            6: ['a', 'f', 'e', 'd', 'c', 'g'],
            7: ['a', 'b', 'c'],
            8: ['a', 'b', 'c', 'd', 'e', 'f', 'g'],
            9: ['a', 'b', 'c', 'd', 'f', 'g'],
        }
        
        for seg_name in digit_segments.get(digit, []):
            if seg_name in segments:
                start, end = segments[seg_name]
                draw.line([start, end], fill=color, width=line_width)
                r = line_width // 2
                draw.ellipse([start[0]-r, start[1]-r, start[0]+r, start[1]+r], fill=color)
                draw.ellipse([end[0]-r, end[1]-r, end[0]+r, end[1]+r], fill=color)
    
    @classmethod
    def IS_CHANGED(cls, image, **kwargs):
        points_data = kwargs.get("points_data", "[]")
        return f"{image}_{points_data}"
    
    @classmethod
    def VALIDATE_INPUTS(cls, image,** kwargs):
        if not folder_paths.exists_annotated_filepath(image):
            return f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image}"
        return True



#endregion-----------------------------








class text_mul_Split:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {"multiline": True}),
                "delimiter": ("STRING", {
                    "default": "\\n",
                    "multiline": False,
                    "tooltip": "Use \\n for newline, \\t for tab, \\s for space"
                }),
            },
        }

    RETURN_TYPES = ("LIST", "STRING", "STRING", "STRING", "STRING", 
                    "STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("list_output", "item1", "item2", "item3", "item4", 
                    "item5", "item6", "item7", "item8")
    #OUTPUT_IS_LIST = (True, False, False, False, False, False, False, False, False)
    FUNCTION = "split_text"
    CATEGORY = "Apt_Preset/ğŸš«Deprecated/ğŸš«"

    def split_text(self, text, delimiter):
        # å¤„ç†ç‰¹æ®Šè½¬ä¹‰å­—ç¬¦
        if delimiter == "\\n":
            actual_delimiter = "\n"
        elif delimiter == "\\t":
            actual_delimiter = "\t"
        elif delimiter == "\\s":
            actual_delimiter = " "
        else:
            actual_delimiter = delimiter.strip()

        # ä½¿ç”¨å®é™…åˆ†éš”ç¬¦è¿›è¡Œåˆ†å‰²
        parts = [part.strip() for part in text.split(actual_delimiter)]

        # ç”Ÿæˆ8ä¸ªå›ºå®šè¾“å‡ºï¼Œä¸è¶³è¡¥ç©ºå­—ç¬¦ä¸²
        output_items = parts[:8]
        while len(output_items) < 8:
            output_items.append("")


        list_out = []
        for text_item in parts:
            list_out.append(text_item)
        

        return (list_out, *output_items)




class text_batch_combine :
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
                    "text_list": (any_type,),
                    "delimiter":(["newline","comma","backslash","space"],),
                            },
                }
    RETURN_TYPES = ("STRING",) 
    RETURN_NAMES = ("text",) 
    FUNCTION = "run"
    CATEGORY = "Apt_Preset/ğŸš«Deprecated/ğŸš«"

    INPUT_IS_LIST = True
    OUTPUT_IS_LIST = (False,)

    def run(self,text_list,delimiter):
        delimiter=delimiter[0]
        if delimiter =='newline':
            delimiter='\n'
        elif delimiter=='comma':
            delimiter=','
        elif delimiter=='backslash':
            delimiter='\\'
        elif delimiter=='space':
            delimiter=' '
        t=''
        if isinstance(text_list, list):
            t=delimiter.join(text_list)
        return (t,)




class text_mul_Join:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "join_rule": (
                    ["None", "Custom", "Line", "Space", "Comma", "Period", "Semicolon", "Tab", "Pipe"],
                    {"default": "Comma"}
                ),
                "custom_pattern": ("STRING", {
                    "multiline": True,
                    "default": "{text1},{text2},{text3},{text4},{text5},{text6},{text7},{text8}",
                }),
                **{f"text{i+1}": ("STRING", {"default": "", "multiline": False}) for i in range(8)},
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("joined_text",)
    FUNCTION = "smart_join"
    CATEGORY = "Apt_Preset/prompt/text_tool"
    DESCRIPTION = """
    æ–‡æœ¬åˆå¹¶é¢„è®¾è¯´æ˜
    Noneï¼šç›´æ¥æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬ï¼ˆæ— åˆ†éš”ç¬¦ï¼‰ã€‚
    Lineï¼šæŒ‰è¡Œåˆ†éš”ï¼ˆ\\nï¼‰ã€‚
    Spaceï¼šæŒ‰ç©ºæ ¼åˆ†éš”ã€‚
    Commaï¼šæŒ‰é€—å·åˆ†éš”ï¼ˆæ”¯æŒä¸­è‹±æ–‡é€—å·ï¼‰ã€‚
    Periodï¼šæŒ‰å¥å·åˆ†éš”ï¼ˆæ”¯æŒä¸­è‹±æ–‡å¥å·ï¼‰ã€‚
    Semicolonï¼šæŒ‰åˆ†å·åˆ†éš”ï¼ˆæ”¯æŒä¸­è‹±æ–‡åˆ†å·ï¼‰ã€‚
    Tabï¼šæŒ‰åˆ¶è¡¨ç¬¦åˆ†éš”ï¼ˆ\\tï¼‰ã€‚
    Pipeï¼šæŒ‰ç«–çº¿åˆ†éš”ï¼ˆ|ï¼‰ã€‚
    Customï¼šè‡ªå®šä¹‰æ’ç‰ˆæ›¿æ¢ï¼ˆæ”¯æŒ{text1}-{text8}å ä½ç¬¦è‡ªç”±ç»„åˆï¼Œä¿ç•™æ’ç‰ˆæ ¼å¼ï¼‰
    """ 

    def _normalize_text(self, text: str) -> str:
        text = unicodedata.normalize('NFKC', text)
        text = ''.join([c for c in text if c.isprintable() or c in ['\n', ' ']])
        return text.strip()

    def _get_separator_template(self, join_rule: str) -> str:
        rule_template_map = {
            "None": "",
            "Line": "\n",
            "Space": " ",
            "Comma": "{sep}",
            "Period": "{sep}",
            "Semicolon": "{sep}",
            "Tab": "\t",
            "Pipe": "|",
            "Custom": None
        }
        
        symbol_map = {
            "Comma": (",", "ï¼Œ"),
            "Period": (".", "ã€‚"),
            "Semicolon": (";", "ï¼›")
        }
        
        template = rule_template_map.get(join_rule, "\n")
        
        if join_rule in symbol_map and template == "{sep}":
            template = symbol_map[join_rule][1]
        
        return template

    def _replace_custom_placeholders(self, custom_pattern: str, text_dict: dict) -> str:
        normalized_pattern = self._normalize_text(custom_pattern)
        result = normalized_pattern
        
        for i in range(8):
            placeholder = f"{{text{i+1}}}"
            text_content = text_dict.get(f"text{i+1}", "")
            result = result.replace(placeholder, text_content)
        
        return result

    def smart_join(self, join_rule: str, custom_pattern: str, **kwargs):
        text_dict: Dict[str, str] = {}
        non_empty_texts: List[str] = []
        for i in range(8):
            text_key = f"text{i+1}"
            text_content = self._normalize_text(kwargs.get(text_key, ""))
            text_dict[text_key] = text_content
            if text_content:
                non_empty_texts.append(text_content)

        if join_rule == "Custom":
            joined_text = self._replace_custom_placeholders(custom_pattern, text_dict)
        else:
            separator = self._get_separator_template(join_rule)
            joined_text = separator.join(non_empty_texts) if non_empty_texts else ""

        joined_text = self._normalize_text(joined_text)
        if not joined_text:
            joined_text = "âŒ No valid content to join"

        return (joined_text,)



class text_Splitter:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text_input": ("STRING", {"multiline": True, "default": "", "placeholder": "Enter text to split"}),
                "split_rule": (["None", "Custom", "Line", "Space", "Comma", "Period", "Semicolon", "Tab", "Pipe"], {"default": "Line"}),
                "custom_separator": ("STRING", {"multiline": False, "default": ""}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
            }
        }
    RETURN_TYPES = ("STRING", "LIST")
    RETURN_NAMES = ("single_text", "split_list")
    FUNCTION = "smart_process"
    CATEGORY = "Apt_Preset/prompt/text_tool"
    DESCRIPTION = """
    æ–‡æœ¬æ‹†åˆ†é¢„è®¾è¯´æ˜
    Noneï¼šä¸åˆ†å‰²ã€‚
    Lineï¼šæŒ‰è¡Œæ‹†åˆ†ã€‚
    Spaceï¼šæŒ‰ç©ºæ ¼æ‹†åˆ†ã€‚
    Commaï¼šæŒ‰é€—å·æ‹†åˆ†ã€‚
    Periodï¼šæŒ‰å¥å·æ‹†åˆ†ã€‚
    Semicolonï¼šæŒ‰åˆ†å·æ‹†åˆ†ã€‚
    Tabï¼šç”¨\tï¼ˆåˆ¶è¡¨ç¬¦ï¼‰æ‹†åˆ†ã€‚
    Pipeï¼šä»¥|ï¼ˆç«–çº¿ï¼‰æ‹†åˆ†ã€‚
    Customï¼šä½¿ç”¨è‡ªå®šä¹‰åˆ†éš”ç¬¦
    """ 
    def _normalize_text(self, text: str) -> str:
        text = unicodedata.normalize('NFKC', text)
        text = ''.join([c for c in text if c.isprintable() or c in ['\n', ' ']])
        return text.strip()
    def _get_smart_separator(self, rule: str, custom_sep: str) -> List[str]:
        smart_sep_map = {
            "None": [],
            "Line": ["\n", "\r\n"],
            "Space": [" ", "ã€€"],
            "Comma": [",", "ï¼Œ"],
            "Period": [".", "ã€‚"],
            "Semicolon": [";", "ï¼›"],
            "Tab": ["\t"],
            "Pipe": ["|"],
            "Custom": [custom_sep.replace("\\n", "\n").replace("\\t", "\t")] if custom_sep else []
        }
        return smart_sep_map.get(rule, ["\n"])
    def _smart_split(self, text: str, separators: List[str]) -> List[str]:
        if not separators or not text:
            return [self._normalize_text(text)]
        escaped_seps = [re.escape(sep) for sep in separators if sep]
        if not escaped_seps:
            return [self._normalize_text(text)]
        sep_pattern = '|'.join(escaped_seps)
        split_result = re.split(f'(?:{sep_pattern})', text)
        result = []
        for item in split_result:
            cleaned = self._normalize_text(item)
            if cleaned:
                result.append(cleaned)
        return result if result else [""]
    def smart_process(self, text_input: str, split_rule: str, custom_separator: str, seed: int):
        text_content = self._normalize_text(text_input)
        if not text_content:
            return ("âŒ Text input is empty", [])
        separators = self._get_smart_separator(split_rule, custom_separator)
        split_list = self._smart_split(text_content, separators)
        if not split_list or split_list == [""]:
            return ("âŒ No valid content after splitting", [])
        random.seed(seed)
        single_text = split_list[random.randint(0, len(split_list)-1)] if split_list else ""
        return (single_text, split_list)



class text_filter:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text_input": ("STRING", {"multiline": True, "default": "", "placeholder": "Enter text to filter"}),
                "filter_rule": (["None", "custom", "@text@", "@text", "text @", '"text"', "'text'", "{text}", "(text)"], {"default": "None"}),
                "custom_rule": ("STRING", {"multiline": False, "default": "", "placeholder": "Custom filter rule, e.g., [text], [text, text]"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
            },
            "optional": {
                "match_all": ("BOOLEAN", {"default": False, "label_on": "Match All", "label_off": "Single Match"}),
            }
        }
    RETURN_TYPES = ("STRING", "LIST")
    RETURN_NAMES = ("single_text", "Matched_list")
    FUNCTION = "smart_process"
    CATEGORY = "Apt_Preset/prompt/text_tool"
    DESCRIPTION = """
    æ–‡æœ¬è¿‡æ»¤é¢„è®¾è¯´æ˜
    Noneï¼šä¸è¿‡æ»¤ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ã€‚
    @text@ï¼šæå–@ç¬¦å·åŒ…è£¹çš„æ–‡æœ¬ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰ã€‚
    @textï¼šæå–@ç¬¦å·åçš„æ‰€æœ‰æ–‡æœ¬ã€‚
    text @ï¼šæå–@ç¬¦å·å‰çš„æ‰€æœ‰æ–‡æœ¬ã€‚
    "text"ï¼šæå–åŒå¼•å·åŒ…è£¹çš„æ–‡æœ¬ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰ã€‚
    'text'ï¼šæå–å•å¼•å·åŒ…è£¹çš„æ–‡æœ¬ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰ã€‚
    {text}ï¼šæå–å¤§æ‹¬å·åŒ…è£¹çš„æ–‡æœ¬ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰ã€‚
    (text)ï¼šæå–å°æ‹¬å·åŒ…è£¹çš„æ–‡æœ¬ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰ã€‚
    Customï¼šä½¿ç”¨è‡ªå®šä¹‰è¿‡æ»¤è§„åˆ™ï¼Œä¾‹å¦‚:
         [text] ï¼šæ‹¬å·å†…çš„æ–‡æœ¬éƒ½ä¼šè¢«æå–å¹¶è¿”å›ã€‚
         [text ï¼šæ‹¬å·åé¢çš„æ–‡æœ¬éƒ½ä¼šè¢«æå–å¹¶è¿”å›ã€‚
         text]ï¼šæ‹¬å·å‰é¢çš„æ–‡æœ¬éƒ½ä¼šè¢«æå–å¹¶è¿”å›ã€‚
    """
    def _normalize_text(self, text: str) -> str:
        text = unicodedata.normalize('NFKC', text)
        text = ''.join([c for c in text if c.isprintable() or c in ['\n', ' ']])
        return text.strip()
    def _get_filter_pattern(self, filter_rule: str, custom_rule: str) -> Optional[str]:
        # å¤„ç†è‡ªå®šä¹‰è§„åˆ™
        if filter_rule == "custom" and custom_rule.strip():
            target_rule = custom_rule.strip()
            if "text" in target_rule:
                if len(target_rule) == len("text") + 2 and target_rule.count("text") == 1:
                    prefix = target_rule.replace("text", "", 1)[0]
                    suffix = target_rule.replace("text", "", 1)[-1]
                    return re.escape(prefix) + r"(.*?)" + re.escape(suffix)
                elif target_rule.endswith("text") and len(target_rule) == len("text") + 1:
                    prefix = target_rule.replace("text", "")
                    return re.escape(prefix) + r"(.*)"
                elif target_rule.startswith("text") and len(target_rule) == len("text") + 1:
                    suffix = target_rule.replace("text", "")
                    return r"(.*?)" + re.escape(suffix)
        
        rule_pattern_map = {
            "None": None,
            "@text@": re.escape("@") + r"(.*?)" + re.escape("@"),
            "@text": re.escape("@") + r"(.*)",
            "text @": r"(.*?)" + re.escape("@"),
            '"text"': re.escape('"') + r"(.*?)" + re.escape('"'),
            "'text'": re.escape("'") + r"(.*?)" + re.escape("'"),
            "{text}": re.escape("{") + r"(.*?)" + re.escape("}"),
            "(text)": re.escape("(") + r"(.*?)" + re.escape(")"),
            "custom": None,  # å¦‚æœé€‰æ‹©customä½†æ²¡æœ‰æä¾›è§„åˆ™ï¼Œè¿”å›None
        }
        return rule_pattern_map.get(filter_rule, None)
    def _smart_filter(self, text: str, pattern: Optional[str], match_all: bool) -> Tuple[str, List[str]]:
        if pattern is None:
            normalized_text = self._normalize_text(text)
            return (normalized_text, [normalized_text] if normalized_text else [])
        match_results = re.findall(pattern, text, re.DOTALL)
        match_results = [res.strip() for res in match_results if res.strip()]
        if not match_results:
            return ("âŒ No valid content after filtering", [])
        if match_all:
            main_result = "\n".join(match_results)
        else:
            main_result = match_results[0]
        return (main_result, match_results)
    def smart_process(self, text_input: str, filter_rule: str, custom_rule: str, seed: int, match_all: bool = False):
        text_content = self._normalize_text(text_input)
        if not text_content:
            return ("âŒ Text input is empty", [])
        pattern = self._get_filter_pattern(filter_rule, custom_rule)
        main_result, match_list = self._smart_filter(text_content, pattern, match_all)
        if not match_all and len(match_list) > 1:
            random.seed(seed)
            main_result = match_list[random.randint(0, len(match_list)-1)]
        return (main_result, match_list)




class text_modifier:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {"multiline": True, "default": ""}),
                "repair_type": (
                    [
                        "None", "å–æ•°å­—", 
                        "å–å­—æ¯", "è½¬å¤§å†™", "è½¬å°å†™", "å–ä¸­æ–‡", 
                        "å»æ ‡ç‚¹", "å»æ¢è¡Œ", "å»ç©ºè¡Œ", "å»ç©ºæ ¼", 
                        "å»æ ¼å¼", "ç»Ÿè®¡å­—æ•°", "å»ç‰¹æ®Šå­—ç¬¦", 
                        "å»é‡å¤è¡Œ", "æ¯è¡Œé¦–å­—æ¯å¤§å†™"
                    ], 
                    {"default": "None"}
                ),
                "replace_targets": ("STRING", {"multiline": False, "default": "{text1},{text2}", "placeholder": "ç›®æ ‡æ–‡æœ¬æ ¼å¼: {text1},{text2}"}),
                "replace_content": ("STRING", {"multiline": False, "default": "{text3},{text4}", "placeholder": "æ›¿æ¢å†…å®¹æ ¼å¼: {text3},{text4}"}),
                "remove_targets": ("STRING", {"multiline": False, "default": "{text1},{text2}", "placeholder": "ç§»é™¤å†…å®¹æ ¼å¼: {text1},{text2}"})
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("processed_text",)
    FUNCTION = "process_text"
    CATEGORY = "Apt_Preset/prompt/text_tool"
    DESCRIPTION = """
    å¤šæ–‡æœ¬æ›¿æ¢æˆ–ç§»é™¤ï¼Œä½¿ç”¨é€—å·åˆ†éš”ï¼Œæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ã€‚
    ä¾‹å¦‚ï¼š
    targets = "{man}, {dog}"
    replacements = "{girl}, {cat}" åŒæ—¶æ›¿æ¢
    words_to_remove = "{man}, {dog}" åŒæ—¶ç§»é™¤
    """ 



    def process_text(self, text, repair_type, replace_targets, replace_content, remove_targets):
        text = text or ""
        
        # å¤„ç†æ›¿æ¢æ–‡æœ¬
        if replace_targets.strip() and replace_content.strip():
            text = self.replace_text(text, replace_targets, replace_content)[0]
        
        # å¤„ç†ç§»é™¤æ–‡æœ¬
        if remove_targets.strip():
            text = self.remove_text(text, remove_targets)[0]
        
        # å¤„ç†å…¶ä»–ä¿®å¤ç±»å‹
        if repair_type == "None":
            return (text,)
        else:
            return self.repair_text(text, repair_type)
    
    def replace_text(self, text, replace_targets, replace_content):
        """æ›¿æ¢æ–‡æœ¬åŠŸèƒ½ï¼Œä½¿ç”¨ {text1},{text2} æ ¼å¼"""
        if not replace_targets.strip() or not replace_content.strip():
            return (text,)
        
        # è§£æç›®æ ‡æ–‡æœ¬å’Œæ›¿æ¢å†…å®¹
        def parse_bracket_content(input_str):
            # æå–æ‹¬å·å†…çš„å†…å®¹ï¼Œä¸åŒ…å«æ‹¬å·
            pattern = r'\{([^}]+)\}'
            matches = re.findall(pattern, input_str)
            return [match.strip() for match in matches if match.strip()]
        
        targets = parse_bracket_content(replace_targets)
        replacements = parse_bracket_content(replace_content)
        
        if not targets or not replacements:
            return (text,)
        
        # åˆ›å»ºæ›¿æ¢æ˜ å°„
        word_map = {}
        min_len = min(len(targets), len(replacements))
        for i in range(min_len):
            if targets[i]:
                word_map[targets[i]] = replacements[i]
        
        # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆæ›¿æ¢é•¿çš„å†…å®¹
        sorted_targets = sorted(word_map.keys(), key=len, reverse=True)
        
        result = text
        for target in sorted_targets:
            pattern = re.escape(target)
            result = re.sub(pattern, word_map[target], result)
        
        return (result,)
    
    def remove_text(self, text, remove_targets):
        """ç§»é™¤æ–‡æœ¬åŠŸèƒ½ï¼Œä½¿ç”¨ {text1},{text2} æ ¼å¼"""
        if not remove_targets.strip():
            return (text,)
        
        # è§£æè¦ç§»é™¤çš„å†…å®¹
        def parse_bracket_content(input_str):
            # æå–æ‹¬å·å†…çš„å†…å®¹ï¼Œä¸åŒ…å«æ‹¬å·
            pattern = r'\{([^}]+)\}'
            matches = re.findall(pattern, input_str)
            return [match.strip() for match in matches if match.strip()]
        
        remove_words = parse_bracket_content(remove_targets)
        
        if not remove_words:
            return (text,)
        
        # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆç§»é™¤é•¿çš„å†…å®¹
        remove_words_sorted = sorted(remove_words, key=len, reverse=True)
        pattern = '|'.join(re.escape(word) for word in remove_words_sorted)
        cleaned_text = re.sub(pattern, '', text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        return (cleaned_text,)
    

    
    def repair_text(self, input_string, option):
        input_string = input_string or ""
        
        if option == "å–æ•°å­—":
            result = ''.join(re.findall(r'\d', input_string))
        elif option == "å–å­—æ¯":
            processed = ''.join([self.full2half(c) for c in input_string])
            result = ''.join(filter(lambda char: char.isalpha() and not self.is_chinese(char), processed))
        elif option == "è½¬å¤§å†™":
            result = input_string.upper()
        elif option == "è½¬å°å†™":
            result = input_string.lower()
        elif option == "å–ä¸­æ–‡":
            result = ''.join(filter(self.is_chinese, input_string))
        elif option == "å»æ ‡ç‚¹":
            result = re.sub(r'[^\d\w\s\u4e00-\u9fff]', '', input_string)
        elif option == "å»æ¢è¡Œ":
            result = input_string.replace('\n', '').replace('\r', '')
        elif option == "å»ç©ºè¡Œ":
            result = '\n'.join(filter(lambda line: line.strip(), input_string.splitlines()))
        elif option == "å»ç©ºæ ¼":
            result = input_string.replace(' ', '').replace('\t', '')
        elif option == "å»æ ¼å¼":
            result = re.sub(r'\s+', '', input_string)
        elif option == "ç»Ÿè®¡å­—æ•°":
            clean_str = re.sub(r'\s+', '', input_string)
            result = str(len(clean_str))
        elif option == "å»ç‰¹æ®Šå­—ç¬¦":
            result = re.sub(r'[^\u4e00-\u9fffa-zA-Z0-9\s]', '', input_string)
        elif option == "å»é‡å¤è¡Œ":
            lines = input_string.splitlines()
            unique_lines = []
            seen = set()
            for line in lines:
                stripped_line = line.strip()
                if stripped_line not in seen:
                    seen.add(stripped_line)
                    unique_lines.append(stripped_line)
            result = '\n'.join(unique_lines)
        elif option == "æ¯è¡Œé¦–å­—æ¯å¤§å†™":
            def capitalize_line(line):
                if not line.strip():
                    return line
                stripped = line.lstrip()
                return line[:len(line)-len(stripped)] + stripped[0].upper() + stripped[1:]
            lines = input_string.splitlines()
            processed_lines = [capitalize_line(line) for line in lines]
            result = '\n'.join(processed_lines)
        else:
            result = input_string

        return (result,)
    
    @staticmethod
    def is_chinese(char):
        return '\u4e00' <= char <= '\u9fff'
    
    @staticmethod
    def full2half(char):
        if ord(char) == 0x3000:
            return ' '
        if '\uff01' <= char <= '\uff5e':
            return chr(ord(char) - 0xfee0)
        return char
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return True




class text_loadText:
    
    @classmethod
    def INPUT_TYPES(cls):
        file_types = ["text", "md", "json", "js", "py", "toml"]
        if REMOVER_AVAILABLE:
            file_types.append("docx")
        
        return {
            "required": {
                "path": ("STRING", {
                    "default": "",
                    "placeholder": "è¾“å…¥æ–‡ä»¶è·¯å¾„"
                }),
                "file_type": (file_types,),
                "char_limit": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100000,
                    "step": 10,
                }),
                "batch_mode": ("BOOLEAN", {
                    "default": False,
                    "label_on": "æ–‡ä»¶å¤¹æ‰¹é‡è¯»å–",
                    "label_off": "å•æ–‡ä»¶è¯»å–"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")  # æ–°å¢æ–‡ä»¶åå­—æ®µè¾“å‡º
    RETURN_NAMES = ("text", "file_paths", "file_names")  # æ–°å¢æ–‡ä»¶åå­—æ®µåç§°
    FUNCTION = "read_content"
    CATEGORY = "Apt_Preset/prompt/text_tool"

    def read_content(self, path: str, file_type: str, char_limit: int, batch_mode: bool) -> Tuple[str, str, str]:
        path = path.strip('\'"')
        
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        ext_mapping = {
            "text": ".txt",
            "md": ".md",
            "json": ".json",
            "js": ".js",
            "py": ".py",
            "toml": ".toml",
            "docx": ".docx"
        }
        target_ext = ext_mapping.get(file_type, "")
        
        if file_type == "docx" and not REMOVER_AVAILABLE:
            raise ValueError("ç¼ºå°‘python-docxåº“ï¼Œè¯·å®‰è£…åé‡è¯•ï¼ˆå¯ä½¿ç”¨å‘½ä»¤ï¼špip install python-docxï¼‰")
        
        try:
            if batch_mode:
                if not os.path.isdir(path):
                    raise ValueError(f"æ‰¹é‡æ¨¡å¼ä¸‹è·¯å¾„å¿…é¡»æ˜¯æ–‡ä»¶å¤¹ - {path}")
                
                search_pattern = os.path.join(path, f"*{target_ext}")
                file_paths = glob.glob(search_pattern)
                
                if not file_paths:
                    raise ValueError(f"è­¦å‘Šï¼šåœ¨ {path} ä¸­æœªæ‰¾åˆ°{target_ext}ç±»å‹æ–‡ä»¶")
                
                file_paths.sort(key=lambda x: os.path.basename(x))
                read_paths = []
                read_names = []  # å­˜å‚¨è¯»å–çš„æ–‡ä»¶ååˆ—è¡¨
                
                merged_content = []
                total_char_count = 0
                for file_path in file_paths:
                    file_name = os.path.basename(file_path)
                    merged_content.append(f"\n\n===== å¼€å§‹ï¼š{file_name} =====")
                    
                    content = self._read_single_file(file_path, file_type)
                    merged_content.append(content)
                    merged_content.append(f"===== ç»“æŸï¼š{file_name} =====")
                    
                    read_paths.append(file_path)
                    read_names.append(file_name)  # æ”¶é›†æ–‡ä»¶å
                    total_char_count += len(content)
                    
                    if char_limit > 0 and total_char_count > char_limit:
                        merged_content.append(f"\n\n...ï¼ˆå·²è¾¾å­—ç¬¦é™åˆ¶ {char_limit}ï¼Œåç»­æ–‡ä»¶æœªè¯»å–ï¼‰")
                        break
                
                final_content = ''.join(merged_content)
                paths_str = "\n".join(read_paths)
                names_str = "\n".join(read_names)  # æ–‡ä»¶åç”¨æ¢è¡Œåˆ†éš”æ‹¼æ¥
                return (final_content, paths_str, names_str)
            
            else:
                if not os.path.isfile(path):
                    raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨ - {path}")
                
                if not path.lower().endswith(target_ext):
                    raise ValueError(f"è­¦å‘Šï¼šæ–‡ä»¶æ‰©å±•åä¸æ‰€é€‰ç±»å‹ä¸åŒ¹é…ï¼ˆé¢„æœŸ{target_ext}ï¼‰")
                
                content = self._read_single_file(path, file_type)
                file_name = os.path.basename(path)  # è·å–å•ä¸ªæ–‡ä»¶çš„æ–‡ä»¶å
                
                if char_limit > 0 and len(content) > char_limit:
                    content = content[:char_limit] + f"\n\n...ï¼ˆå†…å®¹å·²æˆªæ–­ï¼ŒåŸé•¿åº¦{len(content)}å­—ç¬¦ï¼‰"
                
                return (content, path, file_name)  # è¿”å›å•ä¸ªæ–‡ä»¶å
                
        except Exception as e:
            raise ValueError(f"è¯»å–å¤±è´¥ï¼š{str(e)}")
    
    def _read_single_file(self, file_path: str, file_type: str) -> str:
        if file_type == "json":
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                return json.dumps(json_data, ensure_ascii=False, indent=2)
        elif file_type == "toml":
            with open(file_path, 'r', encoding='utf-8') as f:
                toml_data = toml.load(f)
                return toml.dumps(toml_data)
        elif file_type == "docx":
            if not REMOVER_AVAILABLE or docx is None:
                raise ValueError("python-docxåº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å–docxæ–‡ä»¶")
            
            doc = docx_Document(file_path)
            full_text = []
            for para in doc.paragraphs:
                full_text.append(para.text)
            return '\n'.join(full_text)
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()




class text_saveText:
    @classmethod
    def INPUT_TYPES(cls):
        file_types = ["text", "md", "json", "js", "py", "toml"]
        if REMOVER_AVAILABLE:
            file_types.append("docx")
        
        return {
            "required": {
                "content": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è¾“å…¥è¦ä¿å­˜çš„å†…å®¹"
                }),

                "file_path": ("STRING", {
                    "default": "",
                    "placeholder": "è¾“å…¥æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆåŒ…å«æ–‡ä»¶åï¼‰"
                }),
                 "file_type": (file_types,),
            },
            "optional": {
                "custom_file_name": ("STRING", {
                    "default": "",
                    "placeholder": "è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼Œç•™ç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("status",)
    FUNCTION = "write_content"
    CATEGORY = "Apt_Preset/prompt/text_tool"

    def write_content(self, content: str, file_path: str, file_type: str, custom_file_name: str = "") -> Tuple[str]:
        file_path = file_path.strip('\'"')
        
        if not file_path:
            raise ValueError("æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        ext_mapping = {
            "text": ".txt",
            "md": ".md",
            "json": ".json",
            "js": ".js",
            "py": ".py",
            "toml": ".toml",
            "docx": ".docx"
        }
        target_ext = ext_mapping.get(file_type, "")
        if not target_ext:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
        
        try:
            parent_dir = os.path.dirname(file_path)
            original_file_name = os.path.basename(file_path)
            
            if custom_file_name.strip():
                full_path = os.path.join(parent_dir, f"{custom_file_name.strip()}{target_ext}")
            elif not os.path.exists(file_path):
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                default_file_name = f"output_{timestamp}"
                full_path = os.path.join(parent_dir, f"{default_file_name}{target_ext}")
            else:
                full_path = file_path if file_path.lower().endswith(target_ext) else f"{file_path}{target_ext}"
            
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)
            
            if file_type == "json":
                try:
                    json_data = json.loads(content)
                    with open(full_path, 'w', encoding='utf-8') as f:
                        json.dump(json_data, f, ensure_ascii=False, indent=2)
                except (json.JSONDecodeError, TypeError):
                    with open(full_path, 'w', encoding='utf-8') as f:
                        f.write(content)
            
            elif file_type == "toml":
                try:
                    toml_data = toml.loads(content)
                    with open(full_path, 'w', encoding='utf-8') as f:
                        toml.dump(toml_data, f)
                except toml.TomlDecodeError:
                    with open(full_path, 'w', encoding='utf-8') as f:
                        f.write(content)
            
            elif file_type == "docx":
                if not REMOVER_AVAILABLE:
                    raise ValueError("ç¼ºå°‘python-docxåº“ï¼Œè¯·å®‰è£…åé‡è¯•")
                
                doc = Document()
                for para_text in content.split('\n'):
                    doc.add_paragraph(para_text)
                doc.save(full_path)
            
            else:
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            return (f"æˆåŠŸï¼šæ–‡ä»¶å·²ä¿å­˜è‡³ {full_path}",)
        
        except Exception as e:
            raise ValueError(f"å†™å…¥å¤±è´¥ï¼š{str(e)}")



class text_loadText:
    @classmethod
    def INPUT_TYPES(cls):
        file_types = ["text", "md", "json", "js", "py", "toml"]
        if REMOVER_AVAILABLE:
            file_types.append("docx")
        
        return {
            "required": {
                "path": ("STRING", {
                    "default": "",
                    "placeholder": "è¾“å…¥æ–‡ä»¶è·¯å¾„"
                }),
                "file_type": (file_types,),
                "char_limit": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100000,
                    "step": 10,
                }),
                "batch_mode": ("BOOLEAN", {
                    "default": False,
                    "label_on": "æ–‡ä»¶å¤¹æ‰¹é‡è¯»å–",
                    "label_off": "å•æ–‡ä»¶è¯»å–"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("text", "file_paths", "file_names")
    FUNCTION = "read_content"
    CATEGORY = "Apt_Preset/prompt/text_tool"

    def read_content(self, path: str, file_type: str, char_limit: int, batch_mode: bool) -> Tuple[str, str, str]:
        path = path.strip('\'"')
        
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        ext_mapping = {
            "text": ".txt",
            "md": ".md",
            "json": ".json",
            "js": ".js",
            "py": ".py",
            "toml": ".toml",
            "docx": ".docx"
        }
        target_ext = ext_mapping.get(file_type, "")
        
        # æ ¡éªŒdocxä¾èµ–
        if file_type == "docx" and not REMOVER_AVAILABLE:
            raise ValueError("ç¼ºå°‘python-docxåº“ï¼Œè¯·å®‰è£…åé‡è¯•ï¼ˆå¯ä½¿ç”¨å‘½ä»¤ï¼špip install python-docxï¼‰")
        
        try:
            if batch_mode:
                # æ‰¹é‡æ¨¡å¼ï¼šè¯»å–æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æŒ‡å®šæ‰©å±•åçš„æ–‡ä»¶
                if not os.path.isdir(path):
                    raise ValueError(f"æ‰¹é‡æ¨¡å¼ä¸‹è·¯å¾„å¿…é¡»æ˜¯æ–‡ä»¶å¤¹ - {path}")
                
                # å¤„ç†ç©ºæ‰©å±•åï¼ˆé˜²æ­¢globåŒ¹é…é”™è¯¯ï¼‰
                if not target_ext:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{file_type}")
                
                search_pattern = os.path.join(path, f"*{target_ext}")
                file_paths = glob.glob(search_pattern)
                
                if not file_paths:
                    raise ValueError(f"è­¦å‘Šï¼šåœ¨ {path} ä¸­æœªæ‰¾åˆ°{target_ext}ç±»å‹æ–‡ä»¶")
                
                file_paths.sort(key=lambda x: os.path.basename(x))
                read_paths = []
                read_names = []
                merged_content = []
                total_char_count = 0
                
                for file_path in file_paths:
                    file_name = os.path.basename(file_path)
                    merged_content.append(f"\n\n===== å¼€å§‹ï¼š{file_name} =====")
                    
                    content = self._read_single_file(file_path, file_type)
                    merged_content.append(content)
                    merged_content.append(f"===== ç»“æŸï¼š{file_name} =====")
                    
                    read_paths.append(file_path)
                    read_names.append(file_name)
                    total_char_count += len(content)
                    
                    # å­—ç¬¦é™åˆ¶æ ¡éªŒ
                    if char_limit > 0 and total_char_count > char_limit:
                        merged_content.append(f"\n\n...ï¼ˆå·²è¾¾å­—ç¬¦é™åˆ¶ {char_limit}ï¼Œåç»­æ–‡ä»¶æœªè¯»å–ï¼‰")
                        break
                
                final_content = ''.join(merged_content)
                paths_str = "\n".join(read_paths)
                names_str = "\n".join(read_names)
                return (final_content, paths_str, names_str)
            
            else:
                # å•æ–‡ä»¶æ¨¡å¼ï¼šè¯»å–æŒ‡å®šæ–‡ä»¶
                if not os.path.isfile(path):
                    raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨ - {path}")
                
                # æ‰©å±•åæ ¡éªŒï¼ˆå®¹é”™ï¼šå…è®¸ç”¨æˆ·è¾“å…¥è·¯å¾„ä¸å¸¦æ‰©å±•åï¼‰
                if not path.lower().endswith(target_ext):
                    # è‡ªåŠ¨è¡¥å…¨æ‰©å±•å
                    new_path = path + target_ext
                    if os.path.isfile(new_path):
                        path = new_path
                    else:
                        raise ValueError(f"æ–‡ä»¶æ‰©å±•åä¸æ‰€é€‰ç±»å‹ä¸åŒ¹é…ï¼ˆé¢„æœŸ{target_ext}ï¼Œå½“å‰è·¯å¾„ï¼š{path}ï¼‰")
                
                content = self._read_single_file(path, file_type)
                file_name = os.path.basename(path)
                
                # å­—ç¬¦æˆªæ–­
                if char_limit > 0 and len(content) > char_limit:
                    content = content[:char_limit] + f"\n\n...ï¼ˆå†…å®¹å·²æˆªæ–­ï¼ŒåŸé•¿åº¦{len(content)}å­—ç¬¦ï¼‰"
                
                return (content, path, file_name)
                
        except Exception as e:
            raise ValueError(f"è¯»å–å¤±è´¥ï¼š{str(e)}")
    
    def _read_single_file(self, file_path: str, file_type: str) -> str:
        """è¯»å–å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ï¼ˆæŒ‰ç±»å‹é€‚é…ï¼‰"""
        try:
            if file_type == "json":
                with open(file_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                    return json.dumps(json_data, ensure_ascii=False, indent=2)
            
            elif file_type == "toml":
                with open(file_path, 'r', encoding='utf-8') as f:
                    toml_data = toml.load(f)
                    return toml.dumps(toml_data)
            
            elif file_type == "docx":
                # åŒé‡æ ¡éªŒdocxä¾èµ–
                if not REMOVER_AVAILABLE or docx_Document is None:
                    raise ValueError("python-docxåº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å–docxæ–‡ä»¶")
                
                doc = docx_Document(file_path)
                full_text = []
                for para in doc.paragraphs:
                    full_text.append(para.text)
                return '\n'.join(full_text)
            
            else:
                # é€šç”¨æ–‡æœ¬æ–‡ä»¶è¯»å–
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
        
        except UnicodeDecodeError:
            # å®¹é”™ï¼šUTF-8è¯»å–å¤±è´¥æ—¶å°è¯•GBK
            with open(file_path, 'r', encoding='gbk') as f:
                return f.read()
        except Exception as e:
            raise ValueError(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥ï¼š{str(e)}")



#endregion--------------------------------------





# region--------text_wildcards----------
import os
import random
import re
from pathlib import Path
import folder_paths
import unicodedata
import sys

if sys.platform == "win32":
    os.environ["PYTHONIOENCODING"] = "utf-8"

MODEL_ROOT_DIR = os.path.join(folder_paths.models_dir, "Apt_File")
wildcards_dir1 = Path(MODEL_ROOT_DIR) / "wildcards"
os.makedirs(wildcards_dir1, exist_ok=True)

wildcards_dir2 = Path(__file__).parent.parent / "wildcards"
os.makedirs(wildcards_dir2, exist_ok=True)

full_dirs = [wildcards_dir1, wildcards_dir2]

wildcard_map = {}
for wildcard in wildcards_dir1.rglob("*.txt"):
    if wildcard.is_file():
        rel_path = str(wildcard.relative_to(wildcards_dir1))[:-4]
        wildcard_map[rel_path] = f"dir1 | {rel_path}"

for wildcard in wildcards_dir2.rglob("*.txt"):
    if wildcard.is_file():
        rel_path = str(wildcard.relative_to(wildcards_dir2))[:-4]
        if rel_path not in wildcard_map:
            wildcard_map[rel_path] = f"base_path | {rel_path}"

WILDCARDS_LIST = ["None"] + list(wildcard_map.values())

total_files = len(list(wildcards_dir1.rglob("*.txt"))) + len(list(wildcards_dir2.rglob("*.txt")))
unique_files = len(wildcard_map)
if total_files > unique_files:
    duplicate_count = total_files - unique_files
    print(f"â„¹ï¸ æ£€æµ‹åˆ° {duplicate_count} ä¸ªåŒåé€šé…ç¬¦æ–‡ä»¶ï¼Œå·²ä¼˜å…ˆä½¿ç”¨ dir1 ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œbase_path ä¸­çš„åŒåæ–‡ä»¶å·²è¢«è¿‡æ»¤")

class text_wildcards:
    @classmethod
    def INPUT_TYPES(cls):
        inputs = {
            "required": {
                "output_join_rule": (
                    ["Line", "Custom", "Space", "Comma", "Period", "Semicolon", "Tab", "Pipe"],
                    {"default": "Comma"}
                ),
                "custom_join": ("STRING", {
                    "multiline": True,
                    "default": "{text0}ï¼Œ{text1}ï¼Œ{text2}ï¼Œ{text3}ï¼Œ{text4}ï¼Œ{text5}ï¼Œ{text6}ï¼Œ{text7}ï¼Œ{text8}",
                    "placeholder": "åœ¨Customæ¨¡å¼ä¸‹ï¼Œæ‰æœ‰æ•ˆ It only works in Custom mode"
                }),
                "input_split_rule": (
                    ["Line", "Custom", "Space", "Comma", "Period", "Semicolon", "Tab", "Pipe"],
                    {"default": "Line"}
                ),
                "custom_split": ("STRING", {
                    "multiline": False,
                    "default": "",
                    "placeholder": "åœ¨Customæ¨¡å¼ä¸‹ï¼Œæ‰æœ‰æ•ˆ It only works in Custom mode."
                }),
                "text0": ("STRING", {
                    "default": "",
                    "multiline": True,
                }),
                **{f"text{i}": (WILDCARDS_LIST, {"default": WILDCARDS_LIST[0]}) for i in range(1, 9)},
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xFFFFFFFFFFFFFFFF}),
            }
        }
        return inputs

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "stack_Wildcards"
    CATEGORY = "Apt_Preset/prompt/text_tool"
    DESCRIPTION = """
    1. æ”¯æŒtext0ï¼ˆè‡ªå®šä¹‰æ–‡æœ¬ï¼‰ï¼Œtext1-text8é€šé…ç¬¦æ–‡ä»¶é€‰Noneåˆ™è·³è¿‡
    2. text0ä¸ºè‡ªå®šä¹‰æ–‡æœ¬å‰ç¼€ï¼Œå¯ç›´æ¥è¾“å…¥ä»»æ„å†…å®¹ï¼ˆå®Œç¾æ”¯æŒä¸­æ–‡ï¼‰
    Lineï¼šæŒ‰è¡Œåˆ†éš”ï¼ˆ\\nï¼‰ã€‚
    Spaceï¼šæŒ‰ç©ºæ ¼åˆ†éš”ï¼ˆæ”¯æŒå…¨è§’/åŠè§’ç©ºæ ¼ï¼‰ã€‚
    Commaï¼šæŒ‰é€—å·åˆ†éš”ï¼ˆæ”¯æŒä¸­è‹±æ–‡é€—å·ï¼‰ã€‚
    Periodï¼šæŒ‰å¥å·åˆ†éš”ï¼ˆæ”¯æŒä¸­è‹±æ–‡å¥å·ï¼‰ã€‚
    Semicolonï¼šæŒ‰åˆ†å·åˆ†éš”ï¼ˆæ”¯æŒä¸­è‹±æ–‡åˆ†å·ï¼‰ã€‚
    Tabï¼šæŒ‰åˆ¶è¡¨ç¬¦åˆ†éš”ï¼ˆ\\tï¼‰ã€‚
    Pipeï¼šæŒ‰ç«–çº¿åˆ†éš”ï¼ˆ|ï¼‰ã€‚
    Customï¼šè‡ªå®šä¹‰æ’ç‰ˆæ›¿æ¢ï¼ˆæ”¯æŒ{text1}-{text8}å ä½ç¬¦è‡ªç”±æ’ç‰ˆï¼‰
    ğŸŒŸ å¢å¼ºç‰¹æ€§ï¼šå®Œç¾æ”¯æŒä¸­æ–‡æ–‡ä»¶åã€ä¸­æ–‡æ–‡æœ¬å¤„ç†ã€å…¨è§’/åŠè§’ç¬¦å·ç»Ÿä¸€ï¼›åŒåæ–‡ä»¶ä¼˜å…ˆä½¿ç”¨ dir1 ç›®å½•ä¸‹çš„ç‰ˆæœ¬
    """

    def _normalize_text(self, text: str) -> str:
        if not text:
            return ""
        text = unicodedata.normalize('NFKC', text)
        allowed_chars = set(['\n', '\t', ' ', 'ã€€', 'ï¼Œ', 'ã€‚', 'ï¼›', 'ï¼š', 'ï¼', 'ï¼Ÿ', 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€', 'â€¦', 'â€”'])
        text = ''.join([c for c in text if c.isprintable() or c in allowed_chars])
        text = text.strip(' \t\n\rã€€')
        return text

    def _get_split_separator(self, rule: str, custom_sep: str) -> list:
        split_sep_map = {
            "Line": ["\n", "\r\n"],
            "Space": [" ", "ã€€"],
            "Comma": [",", "ï¼Œ"],
            "Period": [".", "ã€‚"],
            "Semicolon": [";", "ï¼›"],
            "Tab": ["\t"],
            "Pipe": ["|"],
            "Custom": [custom_sep.replace("\\n", "\n").replace("\\t", "\t").replace("\\s", " ").replace("\\u3000", "ã€€")] if custom_sep else []
        }
        return split_sep_map.get(rule, ["\n"])

    def _get_join_separator(self, rule: str) -> str:
        join_sep_map = {
            "Line": "\n",
            "Space": " ",
            "Comma": "ï¼Œ",
            "Period": "ã€‚",
            "Semicolon": "ï¼›",
            "Tab": "\t",
            "Pipe": "|"
        }
        return join_sep_map.get(rule, "ï¼Œ")

    def _smart_split(self, text: str, separators: list) -> list:
        if not text:
            return []
        if not separators or all(not sep for sep in separators):
            cleaned = self._normalize_text(text)
            return [cleaned] if cleaned else []
        escaped_seps = []
        for sep in separators:
            if sep:
                escaped_seps.append(re.escape(sep))
        if not escaped_seps:
            cleaned = self._normalize_text(text)
            return [cleaned] if cleaned else []
        sep_pattern = '|'.join(escaped_seps)
        split_result = re.split(f'(?:{sep_pattern})', text)
        result = []
        for item in split_result:
            cleaned = self._normalize_text(item)
            if cleaned:
                result.append(cleaned)
        return result

    def _get_wildcard_content(self, wildcard_key: str, seed: int, input_split_rule: str, custom_split: str) -> str:
        if wildcard_key == "None":
            return ""
        target_dir = None
        wildcard_filename = ""
        if wildcard_key.startswith("dir1 | "):
            wildcard_filename = wildcard_key[len("dir1 | "):]
            target_dir = wildcards_dir1
        elif wildcard_key.startswith("base_path | "):
            wildcard_filename = wildcard_key[len("base_path | "):]
            target_dir = wildcards_dir2
        if not target_dir or not wildcard_filename:
            return ""
        wildcard_file = target_dir / f"{wildcard_filename}.txt"
        if not wildcard_file.exists():
            print(f"âš ï¸ é€šé…ç¬¦æ–‡ä»¶æœªæ‰¾åˆ°: {wildcard_file} (ä¸­æ–‡è·¯å¾„/æ–‡ä»¶åè¯·ç¡®ä¿ç¼–ç æ­£ç¡®)")
            return ""
        try:
            with open(wildcard_file, "r", encoding="utf-8", errors="replace") as f:
                file_content = f.read()
            separators = self._get_split_separator(input_split_rule, custom_split)
            split_lines = self._smart_split(file_content, separators)
            if not split_lines:
                print(f"âš ï¸ é€šé…ç¬¦æ–‡ä»¶æ— æœ‰æ•ˆå†…å®¹: {wildcard_file}")
                return ""
            random.seed(seed)
            selected_content = random.choice(split_lines)
            return self._normalize_text(selected_content)
        except UnicodeDecodeError as e:
            print(f"âš ï¸ é€šé…ç¬¦æ–‡ä»¶ç¼–ç é”™è¯¯: {wildcard_file}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ä¸ºUTF-8ç¼–ç ã€‚é”™è¯¯è¯¦æƒ…: {e}")
            return ""
        except Exception as e:
            print(f"âš ï¸ è¯»å–é€šé…ç¬¦æ–‡ä»¶å¤±è´¥: {wildcard_file}ï¼Œé”™è¯¯: {str(e)}")
            return ""

    def _replace_custom_placeholders(self, custom_join: str, text_dict: dict) -> str:
        if not custom_join:
            return ""
        result = self._normalize_text(custom_join)
        for i in range(0, 9):
            placeholder = f"{{text{i}}}"
            text_content = text_dict.get(f"text{i}", "")
            result = result.replace(placeholder, text_content)
        return self._normalize_text(result)

    def stack_Wildcards(self, output_join_rule: str, input_split_rule: str, custom_split: str, custom_join: str, seed: int, text0="", **kwargs):
        text_dict = {}
        non_empty_texts = []
        text0_content = self._normalize_text(text0)
        text_dict["text0"] = text0_content
        if text0_content:
            non_empty_texts.append(text0_content)
        for i in range(1, 9):
            text_key = f"text{i}"
            wildcard_selector = kwargs.get(text_key, "None")
            content = self._get_wildcard_content(wildcard_selector, seed, input_split_rule, custom_split)
            text_dict[text_key] = content
            if content:
                non_empty_texts.append(content)
        if output_join_rule == "Custom":
            joined_content = self._replace_custom_placeholders(custom_join, text_dict)
        else:
            separator = self._get_join_separator(output_join_rule)
            joined_content = separator.join(non_empty_texts) if non_empty_texts else ""
        final_result = self._normalize_text(joined_content)
        if not final_result:
            final_result = "âŒ æ— æœ‰æ•ˆé€šé…ç¬¦å†…å®¹"
        return (final_result,)

#endregion--------------------






import itertools



class text_StrMatrix:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "fstring": ("STRING", {
                    "multiline": True,
                    "default": "{a}_{b}_{c}_{d}",
                }),
                "max_count": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 999,
                    "step": 1,
                }),
            },
            "optional": {
                "a": (ANY_TYPE, {"default": None}),
                "b": (ANY_TYPE, {"default": None}),
                "c": (ANY_TYPE, {"default": None}),
                "d": (ANY_TYPE, {"default": None}),
            }
        }

    INPUT_IS_LIST = True
    RETURN_NAMES = ("string_Matrix","string_lsit",)
    RETURN_TYPES = (ANY_TYPE,"LIST",)
    OUTPUT_IS_LIST = (True,False,)

    FUNCTION = "execute"
    CATEGORY = "Apt_Preset/prompt/text_tool"
    
    def execute(self, fstring, max_count, a=[], b=[], c=[], d=[]):
        fstring_template = fstring[0] if isinstance(fstring, list) and len(fstring) > 0 else "{a}_{b}_{c}_{d}"
        
        normalized = []
        for lst in [a, b, c, d]:
            if isinstance(lst, list) and len(lst) > 0:
                normalized.append(lst)
            else:
                normalized.append([None])
        
        formatted_strings = []
        current_count = 0
        max_count = max_count[0] if isinstance(max_count, list) else max_count
        
        try:
            for combination in itertools.product(*normalized):
                if current_count >= max_count:
                    break
                
                fmt_dict = {
                    'a': combination[0],
                    'b': combination[1],
                    'c': combination[2],
                    'd': combination[3]
                }
                
                fmt_dict = {k: v if v is not None else "" for k, v in fmt_dict.items()}
                formatted = fstring_template.format(**fmt_dict)
                formatted_strings.append(formatted)
                
                current_count += 1
        except Exception as e:
            print(f"[text_StrMatrix] æ‰§è¡Œé”™è¯¯: {e}")
            formatted_strings = []

        return (formatted_strings,formatted_strings,)

